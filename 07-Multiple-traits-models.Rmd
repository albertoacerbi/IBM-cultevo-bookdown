
# Multiple traits models

In all the scenarios we considered so far, individuals could posses one of two cultural traits, $A$ or $B$. This is a useful simplification, and it represents cases in which cultural traits can be modeled as binary choices, such as being in favour or against a particular policy, choosing between Beatles and Rolling Stones (and no one else!), eating meat or not, and similar. In other cases, however, there are many options: there are many books to read, movies to watch, and, despite our Beatles and Rolling Stones example, many musical bands one can choose to listen to. What does it happen when we copy others' choices? To simplify, we are again assuming unbiased copying as in the [first chapter][Unbiased transmission]: all traits are equivalent and we do not copy preferentially from any individual, but just pick them at random.

The first modification we need to do in the code concerns how traits are represented. Since we have an undetermined number of possible traits we can not use the two letters $A$ and $B$, but we will use instead numbers, so that traits will be now referred as trait "1", trait "2", trait "2", etc.  To start with, we can initliase each individual with a trait randomly chosen between "1" and "100".

```{r 7.1}
library(tidyverse)
set.seed(111)
N <- 100
population <- tibble(trait = sample(1:N, N, replace = TRUE))
```

As usual, you can inspect the `population` tibble by writing its name.

```{r 7.2}
population
```

The basic code of the simulation is similar to the code in the [first chapter][Unbiased transmission], but what the **output** should be? Until now, we just needed to save the frequency of one of the two trait, but now we need the frequencies of all *N* traits to have an idea of what happens in the simulation. 

Another modification in the code above concerns how we measure the frequency of the traits in each generation. The function `hist()`, as the name suggests, is generally used to plot an histogram of the data. However, it can be used, adding the argument `plot = FALSE`, to only calculate what we would need for an histogram, without producing the graph. Among the outputs produced, `density` gives the relative frequencies of the binned data, which is what we are interested in.

```{r 7.3}
multiple_traits <- function(N, t_max) {
  
  output <- tibble(trait = as.factor(rep(1:N, each = t_max)), generation = rep(1:t_max, N), p = rep(NA, t_max * N))

  population <- tibble(trait = sample(1:N, N, replace = TRUE))
  # create first generation
  
  output[output$generation == 1, ]$p <- hist(population$trait, 0:N, plot = FALSE)$density
  # add first generation's p for all traits

  for (t in 2:t_max) {
    previous_population <- population # copy individuals to previous_population tibble

    population <- tibble(trait = sample(previous_population$trait, N, replace = TRUE))    # randomly copy from previous generation

    output[output$generation == t, ]$p <- hist(population$trait, 0:N, plot = FALSE)$density # get p for all traits and put it into output slot for this generation t
  }
  output # export data from function
}
```

Finally, the function to plot the output is similar to what we have already done when plotting multiple runs, with the difference that now the  colored lines do not represent different runs, but different traits, as indicated below by `aes(colour = trait)`. We also need to specify that traits should not be interpreted The new line `theme(legend.position = "none")` simply tells to not include the legends in the graph, as it is not informative, and it would show 100 colors, one for each trait.

```{r 7.4}
plot_multiple_traits <- function(data_model) {
  ggplot(data = data_model, aes(y = p, x = generation)) +
    geom_line(aes(colour = trait)) +
    ylim(c(0, 1)) +
    theme_bw() +
    theme(legend.position = "none")
}
```

As usual, we can call the function and see what happens:

```{r 7.5}
data_model <- multiple_traits(N = 100, t_max = 200)
plot_multiple_traits(data_model)
```

Only one trait is still present at the end of the simulation. In general, only one or two traits are still present in the population after 200 generations, and, if we increase $t_\text{max}$ for example to 1000, virtually all runs end up with only a single trait reaching fixation:

```{r 7.6}
data_model <- multiple_traits(N = 100, t_max = 1000)
plot_multiple_traits(data_model)
```

This is similar to what we saw when considering the analogous situation with only two traits, $A$ and $B$: with unbiased copying and relative small populations, drift is a powerful force, that quickly erodes cultural diversity. 

As we already discussed, increasing $N$ limits the effect of drift. You can experiment with various values for $N$ and $t_\text{max}$. However, the general point is that variation is gradually lost in all cases. How can we counterbalance the homogenizing effect that drift has in small and isolated population, such as the one we are simulating? 

## Introducing innovations

An option is to introduce new traits with individual innovations. We can imagine that, at each time step, a proportion of individuals, $\mu$ (we use the same notation that we used for mutation in [chapter 2][Unbiased and biased mutation]), introduces a new trait in the population. The remaining proportion of individuals, $1-\mu$ copy at random from others, as before. We can start with a small value, such as $\mu=0.01$. Since $N=100$, this means that at each generation, on average, one new trait will be introduced in the population.

Let's see how we can write what happens at each generation: 

```{r 7.7}
mu <- 0.01

last_trait <- max(population) # record what is the last trait introduced in the population

previous_population <- population # copy the population tibble to previous_population tibble

population <- tibble(trait = sample(previous_population$trait, N, replace = TRUE))  # randomly copy from previous generation's individuals

innovators <- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators

population[innovators,]$trait <- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators' traits with new traits

```

There are two modifications here. First, we need to select who are the innovators. For that, we use again the function `sample()`, biased by $\mu$, picking $TRUE$ (corresponding to be an innovator) or $FALSE$ (keeping the copied cultural trait) for $N$ times.

Second, we need to keep track of the new introduced traits. In order to do so, we record at the beginning of each generation what is the "name" of the last trait introduced (at the beginning, with $N=100$, it will be "100", as we initialise each individual of the population with a different trait). When new traits are introduced, we call them with consecutive numbers: the first new traits will be called "101", the second "102" and so on.  

We can now, as usual, wrap everything in a function. 

```{r 7.8}
multiple_traits_2 <- function(N, t_max, mu) {
  max_traits <- N + N * mu * t_max

  output <- tibble(trait = as.factor(rep(1:max_traits, each = t_max)), generation = rep(1:t_max, max_traits), p = rep(NA, t_max * max_traits))

  population <- tibble(trait = sample(1:N, N, replace = TRUE))
  # create first generation
  
  output[output$generation == 1, ]$p <- hist(population$trait, 0:N, plot = FALSE)$density
  # add first generation's p for all traits

  for (t in 2:t_max) {
    last_trait <- max(population) # record what is the last trait introduced in the population

    previous_population <- population # copy individuals to previous_population tibble

    population <- tibble(trait = sample(previous_population$trait, N, replace = TRUE))  # randomly copy from previous generation's individuals
    # randomly copy from previous generation

    innovators <- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators
    if ((last_trait + sum(innovators)) < max_traits) {  
      if( sum(innovators) > 0){
        population[innovators,]$trait <- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators' traits with new traits
      }
    }
    output[output$generation == t, ]$p <- hist(population$trait, 0:max_traits, plot = FALSE)$density # get p for all traits and put it into output slot for this generation t
  }
  output # export data
}
```

You should now be familiar with more or less everything within this function, with one exception: the introduction of the new quantity *max_traits*. This is a trick we are using to avoid making the code too heavy to run. Our `output` tibble, as you remember, record all the frequencies of all traits. When programming, a good rule-of-thumb is to avoid to modify dynamically the size of your data structures, such as, for example, adding new rows to a pre-existing tibble, as in our case. In our simulation, at every generation, there is some probability that a new trait will be introduced so that, as a consequence, we would need to add new rows in the `output` tibble to record its frequency. To avoid this, we are creating a bigger tibble from the beginning, with rows for many possible new traits. How many is 'many'? We do not know, but an estimate is that we will need space for the initial traits ($N$), plus around $N\mu$ traits for each generation. To be sure to not exceed this number, we wrapped the innovation instruction in the `if ((last_trait + sum(innovators)) < max_traits)` control. As a consequence, it is possible that in some runs, in the very last generations, innovations will not be permitted. For many purposes, this does not change the outcome of the simulation, and for the time being is better than modify dynamically our `output`.        

Let's now run the function with an innovation rate $\mu=0.01$, again with a population of 100 individuals, and for 200 generations.

```{r 7.9}
data_model <- multiple_traits_2(N = 100, t_max = 200, mu = 0.01)
plot_multiple_traits(data_model)
```

There should be now more traits at non-zero frequency at the end of the simulation that what happened when innovations were not possible. We can actually check the exact number, by inspecting how many frequencies higher than 0 are in the last row of our matrix:

```{r 7.10}
sum(filter(data_model, generation==200)$p > 0)
```

What happens if we increase the number of generations, or time steps, to 1000, as we did before?

```{r 7.11}
data_model <- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01)
plot_multiple_traits(data_model)
```

As you can see in the plot, there should still be various traits that have frequencies higher than 0, even after 1000 generations. Again, we can check it

```{r 7.12}
sum(filter(data_model, generation==1000)$p > 0)
```

Innovation, in sum, allows the maintenance of variation even in small populations. 

## Optimising the code

It is time for a short technical digression. You may have noticed that running the function `multiple_traits_2()` it is quite time consuming with a population of 1000 individuals. There is a quick way to check the exact time needed, using the function `Sys.time()`, which return your system current time. Let's run the function again and calculate how long it takes.

```{r 7.13}
start_time <- Sys.time()
data_model <- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01)
end_time <- Sys.time()
end_time - start_time
```

On a common laptop, it may take more than 30 seconds. To store the output, we are using a tibble with $1100000$ observations, as *max_traits* is equal to $1100$, which needs to be updated, in the right position, at each generation. A possibility to speed up the simulation is to record our output in a different data structure.

So far, we have been using tibbles to store the output. R, as all programming languages, can store data in different structures. Depending on what the data are and on what one wants to do with them, different structures can be more or less suitable. So far, we used tibbles for the important data of our simulations, such as `population` or `output`. Tibbles can have heterogeneous data, depending on what we need to store: for example, in our `output` tibble, the $trait$ column was specified as a factor, whereas the others two columns, $generation$ and $p$, were numeric.

An alternative is to use vectors---and matrices, which can be thought as 2-dimensional vectors--- that are basic structures in R. Differently from tibbles, in a matrix all data should be of the same type, in our case numeric (so we need to be careful to remember what numbers represent), but they may be convenient to use as the calculations performed on them are efficient, so that when we run models that are more complicated, or when we will need to produce bigger outputs, our simulations will still be relatively fast. 

We can rewrite a function that runs exactly the same simulation, but using matrices instead of tibbles. The output is now a matrix with $t_\text{max}$ rows and *max_traits* columns, intialised with NAs at the beginning, and the population is a vector of integers, representing the trait for each individual.  

```{r 7.14}
multiple_traits_matrix <- function(N, t_max, mu) {
  
  max_traits <- N + N * mu * t_max
  
  output <- matrix(data = NA, nrow = t_max, ncol = max_traits)
  
  # create first generation
  population <- sample(1:N, N, replace = TRUE)
  output[1, ] <- hist(population, 0:max_traits, plot = FALSE)$density
  
  # add first generation's p for all traits
  for (t in 2:t_max) {
    last_trait <- max(population) # record what is the last trait introduced in the population
  
    previous_population <- population # copy individuals to previous_population tibble
    
    population <- sample(previous_population, N, replace = TRUE)
    # randomly copy from previous generation
    
    innovators <- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators
    if ((last_trait + sum(innovators)) < max_traits) {
      population[innovators] <- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators' traits with new traits
    }
    
    output[t, ] <- hist(population, 0:max_traits, plot = FALSE)$density # get p for all traits and put it into output slot for this generation t
  }
  output # export data
}
```

To plot the output, we re-transform it in a tibble, so that can be handled by `ggplot()`. In order to do this, we first create a column that explicitly indicates the number of generations, and then we use the function `gather()` to reassemble the columns of the matrix in key-value pairs.  

```{r 7.15}
plot_multiple_traits_matrix <- function(data_model) {
  generation <- rep(1:dim(data_model)[1], dim(data_model)[2])
  data_to_plot <- as_tibble(data_model) %>%
    gather( key = "trait", value = "p") %>%
    add_column(generation)
  ggplot(data = data_to_plot, aes(y = p, x = generation)) +
    geom_line(aes(colour = trait)) +
    ylim(c(0, 1)) +
    theme_bw() +
    theme(legend.position = "none")
}
```

We can now run the function calculating the time needed, and then plot the results, to see if they look the same.

```{r 7.16}
start_time <- Sys.time()
data_model <- multiple_traits_matrix(N = 100, t_max = 1000, mu = 0.01)
end_time <- Sys.time()
plot_multiple_traits_matrix(data_model)
end_time - start_time
```

The results are equivalent, and the simulations is almost 100 times faster! This shows that implementation details are very important when building individual based models. When one needs to run several times the same simulation, or testing many different parameter values, implementation choices can make drastic differences. 

## The distribution of popularity

An interesting aspect of these simulations is that, even if all traits are equal and individuals are not biased, few traits, for random reasons, are more successful than the majority of the others. A way to visualise this is to plot their cumulative popularity, that is the sums of their  quantities over all generations. Given our matrix, it is easy to calculate them by summing each column and multiplying by *N* (remember they are frequencies, whereas we want now to visualise their actual quantities).   

```{r 7.17}
cumulative <- colSums(data_model) * N 
```

Let's sort them from the more to the least popular and plot the results.

```{r 7.18}
data_to_plot <- tibble(cumulative = sort(cumulative, decreasing = TRUE))

ggplot(data = data_to_plot, aes(x = seq_along(cumulative), y = cumulative)) +
  geom_point() +
  theme_bw() +
  labs(x = "trait label", y = "cumulative popularity")
```

This is an example of a long-tailed distribution. The great majority of traits did not spread in the population, and their cumulative popularity is very close to one. Very few of them---the ones on the left side of the plot---were instead successful. Long-tailed distributions like the one we just produced are very common for cultural traits: very few movies, books, first names are very popular, while the great majority is not. In addition, in these domains, the popular traits are *much* more popular than the unpopular ones. The average cumulative popularity is `mean(cumulative)`, but the most successful trait has a popularity of `max(cumulative)`.

It is common to plot these distributions by binning the data in intervals of exponentially increasing size. In other words, we want to know how many traits have a cumulative popularity between 1 and 2, than between 2 and 4, than between 4 and 8, and so on, until we reach the maximum value of cumulative popularity. The code below does that, using a `for` cycle find how many traits fall in each bins and further normalising according to bin size. The size is increased 50 times, until an arbitrary maximum bin size of $2^{50}$, to be sure to include all cumulative popularities. (We will get rid of the empty bins before plotting.)  

```{r 7.19}
bin <- rep(NA, 50)
x <- rep(NA, 50)
for( i in 1:50 ){
  bin[i] <- sum( cumulative >= 2^(i-1) & cumulative < 2^i)
  bin[i] <- ( bin[i] / length( cumulative ) ) / 2^(i-1);
  x[i] <- 2^i
}
```

We can now visualise the data on a log-log plot (after filtering out the empty bins). A log-log plot is a graph that uses logarithmic scales on both axes. Using logarithmic axes is useful when, as in this case, the data are skewed towards large values. In the previous plot, we were not able to appreciate visually any difference in the great majority of data points, for example points that had cumulative popularity between 1 and 10, as they were all tangled close to the x-axis. 

```{r 7.20}
data_to_plot <- tibble(bin = bin, x = x) 
data_to_plot <- filter(data_to_plot, bin > 0)

ggplot(data = data_to_plot, aes(x = x, y = bin)) +
  geom_point() +
  labs(x = "cumulative popularity", y = "proportion of traits") +
  scale_x_log10() +
  scale_y_log10() +
  stat_smooth(method = "lm") +
  theme_bw()
```

In addition, on a log-log scale, the distribution of cumulative popularity produced by unbiased copying lies approximately on a straight line (we plotted a linear fitting of the data with the command `stat_smooth(method = "lm")`). The goodness of fit and the slope of the line can be used to compare different models of cultural transmission (what would happen with conformity? What with a model bias? And so on...) and to match them with empirical data to develop hypotheses on the processes that generated these distributions in the real world.    

## Summary of the model

In this chapter we simulated situations where more than two traits can be taken into account, exploring the simplest case, i.e. unbiased copying. We also implemented the possibility of innovations, where individuals introduce, with some probability, new traits in the cultural pool of the population. Individual innovations counterbalance the homogenizing effect of drift, and replace the traits that are gradually lost. To simulate multiple traits and individual innovations we also needed to deal with a few technical details such as how to keep track of an initially unknown number of new traits. Mostly, from the technical point of view, we showed how important is to use appropriate data structures when simulations start to become slightly more complex. Replacing tibbles with matrices, we were able to make our simulation 100 times faster. 

Our results showed that unbiased copying produces long-tailed distributions where very few traits are very popular and the great majority are not.
An interesting insight from this model is that these `extreme` distributions do not necessarily result from extreme tendencies at individual level. Some traits become way more popular than others without individuals being biased, for example, towards popular traits. Cultural transmission generates these distributions without biases, but simply because popular traits have the intrinsic advantage of being more likely to be stumbled upon. We also introduced a new technique, the log-log plot of binned popularity distributions, to visualise this outcome.   

***

## Analytical appendix

ANYHTING TO DO HERE?

***

## Further readings

@neiman_stylistic_1995 first introduced a model of unbiased copying with multiple traits to explain popularity distributions in assemblages of Neolithic pottery. @bentley_random_2004 elaborates on this idea, presenting a 'random copying' model (equivalent to the one developed in this chapter) and comparing the popularity distributions produced with real datasets, including first names in US and citations of patents. @mesoudi_random_2009 added biases to a multiple traits model to compare the popularity distributions produced with the distribution produced by unbiased copying. 






