# (PART\*) Advanced topics - Culture and populations {-} 


# Demography

*In the previous chapters, we have looked at the transmission of information between individuals. We have seen that relatively simple mechanisms at the individual level can lead to population-level outcomes (e.g. the fixation of a rare cultural trait). We have also seen the importance of the characteristics of individuals (e.g. for success and prestige bias) in cultural processes. What we have not yet looked at is how the characteristics of the population may affect the outcome of cultural dynamics. In the following three chapters we will have a closer look at how: population size (demography), population structure (social networks), and group structured populations (with migration) can influence cultural evolution.*

## Background on demography-mediated cultural loss

Why would demography matter to cultural evolution? As long as information is transmitted among individuals and between generations, the size of the population should not play a role. In theory, this statement is true but it relies on a crucial assumption: information transfer is not only complete (all information from the previous generation is transmitted to the next generation) but also error-free. However, from many lab experiments, we know that copying information is an error-prone process. In this chapter, we will look at how those errors affect information accumulation and how population size is augmenting this process. 

Several studies have looked at population effects. @shennan_demography_2015 provide a good overview of a variety of approaches and questions. For example, in their model, @ghirlanda_sustainability_2010, investigate the interplay between cultural innovations and cultural loss. While it would be trivial to say that culture accumulates where the rate of cultural innovation is higher than the rate of cultural loss, this model allows for two additional complicating mechanisms: (1) it lets innovations affect carrying capacity (and so the number of innovators), and (2) it allows trait corruption (i.e. a trait that was adaptive before can become maladaptive later, e.g. because it allows the over-exploitation of a resource). 

Another well-known study is that by Joseph @henrich_demography_2004. His model takes inspiration from the archaeological record of Tasmania, which shows a deterioration of some and the persistence of other cultural traits after Tasmania was cut-off from Australia at the end of the last ice age. Henrich develops a compelling analytical model to show that the same adaptive processes in cultural evolution can result in the improvement and retention of simple skills but also the deterioration and even loss of complex skills. In the following section we will take a closer look at this model.

## The Tasmania Case model

The principle idea of Henrich's model is the following: information transmission from one generation to another (or from one individual to another, here it does not make a difference) has a random component (error rate) that will lead to most individuals failing to achieve the same skill level (denoted with $z$) as their cultural model, whereas a few will match and - even fewer - exceed that skill level. Imagine a group of students who try to acquire the skills to manufacture a spear. As imitation is imperfect, and memorising and recalling action sequences is error-prone some students will end up with a spear that is inferior to the one of their cultural model. A few individuals might achieve a similar or even higher skill level than their cultural model. Fig. \@ref(fig:henrichGumbel) is showing this principle. 

To simulate imperfect imitation, Henrich's model uses random values from a Gumbel distribution. This distribution is commonly used to model the distribution of extreme (here maximum proficiency) values. Its shape is controlled by two parameters: $\mu$ (location) and $\sigma>0$ (scale, sometimes also denoted as $\beta$). Varying $\mu$ affects how tricky it is to acquire a given skill. If we subtract an amount $\alpha$ from $\mu$ we move the distribution to the left, and so fewer individuals will acquire a skill level that is larger than that of the cultural model. The larger $\alpha$ the harder it is to acquire a given skill. Varying $\sigma$ on the other hand affects the width of the distribution, and so whether imitators make very similar or systematic mistakes (small $\sigma$, narrow distribution) or whether errors are very different from each other (large $\sigma$, wide distribution). By using different values for $\alpha$ and $\sigma$, we can simulate different skill complexity and imperfect imitation. Intuitively, whether the average skill level of a population increases, persists, or decreases depends on how likely it is that some imitators will achieve a skill that exceeds the current cultural model. Additional to the skill complexity, this also depends on how many individuals try to imitate the skill (how many values are drawn from the distribution). The smaller the pool of imitators, the fewer individuals will achieve a higher skill level and so, over time the skill level will decrease. Henrich provides an analytical model to explain how societies below a critical size (of cultural learners) might lose complex (or even simple) cultural skills over time. We will attempt to re-create his results using an individual-based model. 

```{r henrichGumbel, fig.cap="Shown are the probability distributions to acquire a specific skill level (z, x-axis) for two different skills (a simple one that is easy to learn, and a compelx one that is harder to learn). Given that learning is error-prone more individuals will acquire a skill level that is lower than that of a cultural model (its level is indicated by the vertical dashed line) through imitation (left of the dashed line). A few individauls will achieve higher skill levels (right of the dashed line). For the complex skill the probability to be above the skill level of the cultural model is lower (smaller area under the curve) than for simple skills. For simple skill $\\alpha=-5$ and $\\sigma=3$ and for complex skill $\\alpha=-9$ and $\\sigma=2$.", message=FALSE, warning=FALSE, echo=FALSE}
library(extraDistr)
library(tidyverse)
data <- tibble(skill = rep(c("simple","complex"), each = 6000),
                   z = c(rgumbel(n = 6000, mu = -5, sigma = 3),
                         rgumbel(n = 6000, mu = -9, sigma = 2)))
ggplot(data, aes(x = z, col = skill)) +
  geom_density() + 
  geom_vline(xintercept = 0, col = "grey", linetype = 2) + 
  theme_bw() +
  xlab("imitator value z") + 
  ylab("probability imitatior acquires z") 
```


### Modelling the Tasmania Case

Our model looks like this: we simulate a population with $N$ individuals. Each individual has a skill level $z$. In each round, we determine the highest skill level in the population, $z_{\text{max}}$. We will then draw new values of $z$ for each individual in the population. We draw these values from Gumbel distribution where the new mean is the same as the skill level of the most skilled individual minus $\alpha$, i.e. $\mu = z_{\text{max}} - \alpha$. To keep tack of the simulation we will store the average proficiency $\bar z$ and the change in average proficiency $\Delta \bar z$. 

We begin by loading the packages we will need. We will load the `extraDistr` package that gives us access to the `rgumbel()` function, which draws random values from a Gumbel distribution. We will have to define the shape of the distribution by providing two values, $\mu$ (location) and $\sigma$ (scale). 

Next, we set the variables that we need to run the simulation, that is, population size `N`, a vector to store the skill level `z` for each individual, the number of simulation turns `t_max`, and the reporting variables `z_bar` and `z_delta_bar` for average skill level and the change of the average skill level respectively. 

Finally, we write down a very basic learning loop. The first step in this `for()` loop is to draw new values of `z` and store them in `z_new`. We then calculate the mean of the new skill levels and the change compared to the previous time step, and finally update all values stored in `z`.


```{r 10.2, cache = TRUE}
library(tidyverse)
library(extraDistr)

# set population size
N <- 1000
# draw random values from a uniform distribution to initialise z
z <- rep(1, N)
# set number of simulation rounds
t_max <- 5000
# set up variable to store average z
z_bar <- rep(NA, t_max)
# set up variable to store change in average z
z_delta_bar <- rep(NA, t_max)

# set parameters for Gumbel distribution
sigma <- 3
alpha <- -5

for(r in 1:t_max){
  # calculate new z
  z_new <- rgumbel(n = N, mu = max(z) + alpha, sigma = sigma)
  # record average skill level
  z_bar[r] <- mean(z_new)
  # record average change in z
  z_delta_bar[r] <- mean(z_new - z)
  # update z
  z <- z_new
}
```

Let us now plot the result of this simulation run: 

```{r 10.3, fig.cap="While $\\bar z$ is sometimes above and sometimes below $0$, it is on average postive (dashed line), which indicated that the average skill level of the population increases."}
z_delta_bar_val <- tibble(x = 1:length(z_delta_bar), y = z_delta_bar)
ggplot(z_delta_bar_val) + 
  geom_line(aes(x = x, y = y)) +
  xlab("time") +
  ylab("change in z") +
  geom_hline(yintercept = mean(z_delta_bar_val$y), col = "grey", linetype = 2) +
  theme_bw()
```

We find that $\Delta \bar z$ quickly plateaus at about `r round(mean(z_delta_bar),1)` (grey dashed line). As this is $>0$, on average the population will improve its skill over time. We can see that this is the case when we plot the average skill level over time:

```{r 10.4, fig.cap="For the given parameter ($\\alpha=7$, $\\sigma=1$) the average skill-level increases continously."}
z_bar_val <- tibble(x = 1:length(z_bar), y = z_bar)
ggplot(z_bar_val) + 
  geom_line(aes(x = x, y = y)) +
  xlab("time") +
  ylab("average skill-level") +
  theme_bw()
```

As in the previous chapters, we can now write a wrapper function that allows us to execute this model repeatedly and for different parameters. In the following, we will use a new function: `lapply()`. There is a series of apply functions in the R programming language that 'apply' a function to the elements of a given data object. Generally, these functions take an argument `X` (a vector, matrix, list, etc.) and then apply the function `FUN` to each element. We use `lapply` here on a vector `1:R_MAX`, that is, a vector of the length of the number of repetitions that we want. What will happen is that `lapply()` will execute the function that we will provide exactly R_MAX times, and then return the result of each calculation in a list at the end. We could also use a `for()` loop just as we have done it in the previous chapters. However, the advantage of using the apply function over the loop is that each simulation can run independently from each other. That is because the second simulation does not have to wait for the first to be finished. In contrast, we could not use the apply function for the individual turns. Here the second simulation step _does_ rely on the results of the first step. In this case all simulation steps have to be calculated in sequence. 

Have a look at our `demography_model()` wrapper function: 

```{r 10.5}
demography_model <- function(T_MAX, N, ALPHA, SIGMA, R_MAX){
  res <- lapply(1:R_MAX, function(repetition){
                  z <- rep(1, N)
                  z_delta_bar <- rep(NA, T_MAX)
                  for(turn in 1:T_MAX){
                    z_new <- rgumbel(n = N, mu = max(z) + ALPHA, sigma = SIGMA)
                    z_delta_bar[turn] <- mean(z_new - z)
                    z <- z_new
                  }
                  return(mean(z_delta_bar))
                })
  mean(unlist(res))
}
```

We begin by initiating a function called `demography_model` that is taking a set of parameters (note, it can be useful to capitalise arguments of a function to differentiate between those values that are calculated within a function (not capitalized) and those that have been provided with the function call). When we execute `demography_model()` it will first run an `lapply()` function for `R_MAX` number of rounds. The `lapply()` function will now run independent simulations which we have discussed above (i.e. setting up a population of individuals with skill level `z`, updating these values, and calculating the change in average skill level). The last step is to calculate the mean of `z_delta_bar`, i.e. the average of the change of the mean skill level. This is value is calculated for each repetition. `lapply()` returns all of these values in a list called `res`. As we are interested in the average change of the skill level across all repetitions, we first turn this list into a vector (using the `unlist()` function) and then calculate the mean. 

Let us now use the `demography_model()` function, to run repeated simulations for different population sizes and different skill complexity. Here, we use the following parameters for the skill complexities: $\alpha=-7, \sigma=1$ (simple) and $\alpha=-9, \sigma=1$ (complex). 

We first define a variable, `sizes`, for the different population sizes. We are then again relying on the magic of the `lapply()` function. As above, the reason is that we can let simulations with different population sizes run independently from each other. Note that we provide `sizes` as our `X` argument, and `demography_model()` as the `FUN` function argument. Our `demography_model()` itself requires further arguments to run. In the `lapply()` function we can simply add them at the end. They will be directly handed over to `demography_model()` when we execute the `lapply()` function.

In the last line of this chunk, we create a `tibble` that will hold the results of the simulations for each skill and the different population sizes. 

```{r 10.6, cache = TRUE}
sizes <- c(2, seq(from = 100, to = 6100, by = 500))
sizes

simple_skill <- lapply(X=sizes, FUN=demography_model, 
                       T_MAX = 200, ALPHA = -7, SIGMA = 1, R_MAX = 20)

complex_skill <- lapply(X=sizes, FUN=demography_model, 
                        T_MAX = 200, ALPHA = -9, SIGMA = 1, R_MAX = 20)

data <- tibble(N = rep(sizes, 2), 
               z_delta_bar = c(unlist(simple_skill), 
                               unlist(complex_skill)), 
               skill = rep(c("simple","complex"), each = length(sizes)))
```

Let us now plot the results:

```{r effectivePopSize, fig.cap="For a simple skill effective populaton size (at which the skill can be just maintained in a population) is much smaller than the population that is required to maintain a complext skill."}
ggplot(data) + 
  geom_line(aes(x = N, y = z_delta_bar, color = skill)) +
  xlab("Effective population size") + 
  ylab("Change in average skill level, delta z bar") + 
  geom_hline(yintercept = 0) + 
  theme_bw()
```

In Fig. \@ref(fig:effectivePopSize) we can see that the simple skill (blue) intercepts the x-axis at much smaller population sizes than the complex skill. That means, a simple skill can be maintained by much smaller populations, whereas larger populations of imitators are required for complex skills. 

### Calculating critical population sizes based on skill complexity

Henrich calls the minimum population size required to maintain a skill the critical population size, $N^\star$. How can we calculate $N^\star$ for different skill complexities? We could run simulations for many more population sizes and find the one where $\Delta \bar z$ is closest to zero. Alternatively, here is a more elegant and less computationally intensive method: when we plot our results over logarithmic population size the resulting graphs are almost linear (see \@ref(fig:logEffectivePopSize)). 

```{r logEffectivePopSize, fig.cap="The same as in Fig. \\@ref(fig:effectivePopSize) but using log on population sizes."}
ggplot(data) + 
  geom_line(aes(x = log(N), y = z_delta_bar, color=skill)) +
  xlab("log(Effective population size)") +
  ylab("Change in average skill level, delta z bar") + 
  geom_hline(yintercept = 0) + 
  theme_bw()
```

And so, we could use a linear fit and then solve for $y = 0$ to calculate $N^\star$:

```{r 10.9}
fit <- lm(formula = z_delta_bar ~ log(N), 
          data = data[data$skill=="simple",])
print(fit)
N_star_simple <- exp(solve(coef(fit)[-1], -coef(fit)[1]))
N_star_simple

fit <- lm(formula = z_delta_bar ~ log(N), 
          data = data[data$skill=="complex",])
N_star_complex <- exp(solve(coef(fit)[-1], -coef(fit)[1]))
N_star_complex
```

Note that we need to take the exponent of the resulting value to revert the log function. We see that a simple skill with a low alpha to sigma ratio requires a minimum population size of about `r round(N_star_simple)`, whereas a much large population size is required to maintain a complex trait (here `r round(N_star_complex`). 

Let us now calculate the $N^\star$ values for different skill complexities and different population sizes. We first set up the parameter space (all possible combinations of population sizes $N$ and skill complexity, which we will vary using different values for $\alpha$):

```{r 10.10}
# run simulation for the following population sizes
sizes <- seq(from = 100, to = 6100, by = 500)

# run simulation for the following values of alpha
alphas <- seq(from = -9, to = -4, by = .5)

simulations <- expand.grid(N = sizes, alpha = alphas)
simulations <- as_tibble(simulations)
simulations
```

Now we can run simulations for all combinations of population sizes and skill compexities:

```{r 10.11}
z_delta_bar <- lapply(X=1:nrow(simulations), FUN=function(s){
                  demography_model(T_MAX = 200, 
                                   N = simulations[s, "N"], 
                                   ALPHA = simulations[s, "alpha"], 
                                   SIGMA = 1, 
                                   R_MAX = 5)
  })
# add results to population size and skill complexity
data <- add_column(simulations, z_delta_bar=unlist(z_delta_bar))
data
```

Finally, 

```{r 10.11, cache = TRUE}
res <- do.call("rbind", 
               lapply(X = alphas, 
                      FUN = function(alpha){
                            tmp_z <- unlist(lapply(X = sizes, 
                                                   FUN = demography_model, 
                                                   T_MAX = 200, ALPHA = alpha, SIGMA = 1, R_MAX = 5)
                             )
                            fit <- lm(tmp_z ~ log(sizes))
                            n_star <- exp(solve(coef(fit)[-1], -coef(fit)[1]))
                            tibble(n_star = n_star, alpha = alpha)
                        }
                      )
               )
```

This code chunk is a combination of everything we have done in this chapter. At its core, we run the function `demography_model()` (have a look at the line that creates `tmp_z`). As we have done before, we run this for To analyse what is happening here, first take a look 

```{r 10.11}
res
```

And finally, we can print the critical population sizes as a function of the skill complexity $\alpha$ over $\sigma$. 

```{r 10.12, fig.cap="Critical population size, $N^\\star$, for different skill complexities."}
ggplot(res, aes(x = alpha, y = n_star)) + 
  geom_line() + 
  xlab(expression(alpha/sigma)) +
  ylab("Critical populaton size, N*") + 
  theme_bw()
```

It is interesting to observe that the critical population size increases exponentially with skill complexity. This also suggests that all being equal, very high skill levels will never be reached by finite population sizes. However, different ways of learning (e.g. teaching) could considerably decrease $\alpha$ and $\sigma$ over time and so allow high skill levels. 



## Summary of the model
Similar to the model in the chapter on Rogers' paradox, the present model is very simple and is making many simplifications. Nevertheless, it provides an intuitive understanding of how changes (up and down) in population size can affect the cultural repertoire of a population, and how it can be that simple skills thrive, while complex ones disappear. In the next chapter, we will discuss the importance of social networks, i.e. who can interact with whom. We will see that this will also have an effect (additional to the population size).


## Further readings
@henrich_demography_2004 provides a detailed analytical model. @powell_late_2009 provide an extension to Henrich's model by incorporating subpopulations with varying density. @shennan_demography_2001 is another modelling paper that suggests that innovations are far more successful in larger compared to smaller populations. There is also an increasing number of empirical studies reporting population size effects, for example, on the number folk tale types @acerbi_cultural_2017.




