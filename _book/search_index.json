[
["index.html", "Individual-based models of cultural evolution A step-by-step guide using R Introduction", " Individual-based models of cultural evolution A step-by-step guide using R Alberto Acerbi Alex Mesoudi Marco Smolla 2019-09-04 Introduction TO DO "],
["unbiased-transmission.html", "1 Unbiased transmission 1.1 Initialising the simulation 1.2 Execute generation turn-over many times 1.3 Plotting the model results 1.4 Write a function to wrap the model code 1.5 Run several independent simulations and plot their results 1.6 Varying initial conditions 1.7 Summary of the model 1.8 Analytical appendix", " 1 Unbiased transmission We start by simulating a simple case of unbiasbed cultural transmission, and we will go through the details of each step of the implementation and explainig the code line-by-line. In the next chapters, we will reuse most of what we did here, building up the complexity of our simulations. 1.1 Initialising the simulation Here we will simulate a case where \\(N\\) individuals possess each one of two mutually exclusive cultural traits, for example, preferring a vegetarian diet, which we will denote as trait \\(A\\), or eating products derived from animals, denoted as trait \\(B\\). In each generation, all \\(N\\) individuals are replaced with \\(N\\) new individuals. Each new individual picks a member of the previous generation at random and copies their cultural trait. This is known as unbiased oblique cultural transmission. ‘Unbiased’ refers to the fact that traits are copied entirely at random. The term ‘oblique’ means that members of one generation learns from those of the previous, non-overlapping, generation. This is different from, e.g. horizontal cultural transmission, where individuals copy members of the same generation, and vertical cultural transmission, where offspring copy their parents. If we assume that the two cultural traits are transmitted in an unbiased way, what does that mean for the average trait frequency in the population? To answer this question, we are interested in tracking the proportion of individuals who possess trait \\(A\\) over successive generations. We will call this proportion \\(p\\). We could also track the proportion who possess trait \\(B\\), but this will always be \\(1 - p\\) given that the two traits are mutually exclusive. For example, if \\(70\\%\\) of the population have trait \\(A\\) \\((p=0.7)\\), then the remaining \\(30\\%\\) must have trait \\(B\\) (i.e. \\(1-p=1-0.7=0.3\\)). The output of the model will be a plot showing \\(p\\) over all generations up to the last generation. Generations (or time steps) are denoted by \\(t\\), where generation one is \\(t=1\\), generation two is \\(t=2\\), up to the last generation \\(t=t_{\\text{max}}\\). Generations could correspond to biological generations, but could equally be ‘cultural generations’ (or learning episodes), which might be much shorter. First, we need to specify the fixed parameters of the model. These are \\(N\\) (the number of individuals) and \\(t_{\\text{max}}\\) (the number of generations). Let’s start with \\(N=100\\) and \\(t_{\\text{max}}=200\\): N &lt;- 100 t_max &lt;- 200 Now we need to create our individuals. The only information we need to keep about our individuals is their cultural trait (\\(A\\) or \\(B\\)). We’ll call population the data structure containing the individuals. Initially, we will give each individual either an \\(A\\) or \\(B\\) at random, using the sample() command. The first part of the sample() command lists the elements to pick at random, in our case, the traits \\(A\\) and \\(B\\). The second part gives the number of times to pick, in our case \\(N\\) times, once for each individual. The final part says to replace or reuse the elements after they’ve been picked (otherwise there would only be one copy of \\(A\\) and one copy of \\(B\\), so we could only give two individuals traits before running out). There are two other lines before. First, we need to call the tidyverse library, that we will use throughout the chapter. Second, we use the function set.seed() to set the seed of the random number generator, so to make sure that the results shown in the book are the same you obtain in your session. The number \\(111\\) has nothing special, you can replace it with any other number and you will see how the traits of the individuals will be created differently. library(tidyverse) set.seed(111) population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) We can see the cultural traits of our population by simply entering its name in the R console: population ## # A tibble: 100 x 1 ## trait ## &lt;chr&gt; ## 1 B ## 2 B ## 3 A ## 4 B ## 5 A ## 6 A ## 7 A ## 8 B ## 9 A ## 10 A ## # … with 90 more rows As expected, there is a single column called \\(trait\\) containing \\(A\\)s and \\(B\\)s. The type of the column, in this case ‘chr’ (i.e. character) is reported below the name. A specific individual’s trait can be retrieved using the square bracket notation in R. For example, individual 4’s trait can be retrieved by typing: population$trait[4] ## [1] &quot;B&quot; This should match the fourth row in the table above. We also need a data structure to record the output of our simulation, that is, to track the trait frequency \\(p\\) in each generation. This will have two columns with \\(t_{\\text{max}}\\) rows, one row for each generation. The first column is simply a counter of the generations, from 1 to \\(t_{\\text{max}}\\). As we will see shortly, this is useful to plot the output. The other contains the values of \\(p\\) for each generation. At this stage we don’t know what \\(p\\) will be in each generation, so for now let’s fill the output tibble with lots of NAs, which is R’s symbol for Not Available, or missing value. We can use the rep() (repeat) command to repeat NA \\(t_{\\text{max}}\\) times. We’re using NA rather than, say, zero, because zero could be misinterpreted as \\(p = 0\\), which would mean that all individuals have trait \\(B\\). This would be misleading, because at the moment we haven’t yet calculated \\(p\\), so it’s nonexistent, rather than zero. output &lt;- tibble(generation = 1:t_max, p = rep(NA, t_max)) We can, however, fill in the first value of \\(p\\) for our already-created first generation of individuals, held in population. The command below sums the number of \\(A\\)s in population and divides by \\(N\\) to get a proportion out of 1 rather than an absolute number. It then puts this proportion in the first slot of \\(p\\) in output, the one for the first generation, \\(t=1\\). We can again write the name of the tibble, output, to see that it worked. output$p[1] &lt;- sum(population$trait == &quot;A&quot;) / N output ## # A tibble: 200 x 2 ## generation p ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.51 ## 2 2 NA ## 3 3 NA ## 4 4 NA ## 5 5 NA ## 6 6 NA ## 7 7 NA ## 8 8 NA ## 9 9 NA ## 10 10 NA ## # … with 190 more rows This first value of \\(p\\) is 0.51, meaning that 51 individuals have trait \\(A\\), and 49 have the trait \\(B\\). As the traits are assigned at random, we do not expect that always exactly half of the population will have trait \\(A\\) and the other half \\(B\\). You can try by varying the seed of the random number generator. In the same way that flipping a coin 100 times will not always give exactly 50 heads and 50 tails, sometimes we will have 51 \\(A\\)s, sometimes 49, etc. 1.2 Execute generation turn-over many times Now that we built the population, we can start working on what individuals will do in each generation. We iterate the process over \\(t_{\\text{max}}\\) generations. In each generation, we need to: copy the current individuals to a separate tibble called previous_population to use as demonstrators for the new individuals; this allows us to implement oblique transmission with its non-overlapping generations, rather than mixing up the generations create a new generation of individuals, each of whose trait is picked at random from the previous_population tibble calculate \\(p\\) for this new generation and store it in the appropriate slot in output To iterate, we’ll use a for-loop, using \\(t\\) to track the generation. We’ve already done generation 1 so we’ll start at generation 2. The random picking of models is done with sample() again, but this time picking from the traits held in previous_population. Note that we have added comments briefly explaining what each line does. This is perhaps superfluous when the code is that simple, but it’s always good practice. Code often gets cut-and-pasted into other places and loses its context. Explaining what each line does lets other people - and a future, forgetful you - know what’s going on. for (t in 2:t_max) { previous_population &lt;- population # copy the population tibble to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals output$p[t] &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into the output slot for this generation t } Now we should have 200 values of \\(p\\) stored in output, one for each generation. Let’s plot them. 1.3 Plotting the model results We use ggplot() to plot our data. The syntax of ggplot may be slightly obscure at the beginning, but it forces us to have a clear picture of the data before plotting them, and you will get more and more comfortable with it with time. In the first line in the code below, we are telling ggplot that the data we want to plot are in the tibble output. Then, with the command aes() we declare the ‘aesthetics’ of the plot, that is, how we want our data mapped in our plot. In this case, we want the values of \\(p\\) on the y-axis, and the values of \\(generation\\) on the x-axis (this is why we created before, in the tibble output, a column to keep the count of generations). We then use geom_line(). In ggplot, ‘geoms’ are used to declare how our plot should look. The visual representation is independent from the mapping that we decided before. The same data, with the same mapping, can be graphically represented in many different ways. In this case, we are asking ggplot to represent the data as a line. You can change geom_line() in the code below with geom_point(), and see what happens (other geoms have less obvious effects, and we will see some of them in the later chapters). The other commands are mainly to make the plot look nicer. We want the y-axis to span all the possible values of \\(p\\), from 0 to 1, and we use a particular ‘theme’ for our plot, in this case a standard theme with white background. With the command labs() we give a more informative label to the y-axis (ggplot automatically labels the axis with the name of the tibble columns that are plotted: this is good for \\(generation\\), but less so for \\(p\\)). ggplot(data = output, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) The proportion of individuals with trait \\(A\\) starts off hovering around 0.5, and after around 120 generations drop to 0. Unbiased transmission, or random copying, is by definition random, so different runs of this simulation will generate different plots. If you rerun all the code, trying different seeds of the random number generator, you will ll get something different. In all likelihood, \\(p\\) might go to 0 or 1 at some point. At \\(p = 0\\) there are no \\(A\\)s and every individual possesses \\(B\\). At \\(p = 1\\) there are no \\(B\\)s and every individual possesses \\(A\\). This is a typical feature of cultural drift, analogous to genetic drift: in small populations, with no selection or other directional processes operating, traits can be lost purely by chance after some generations. 1.4 Write a function to wrap the model code Ideally we would like to repeat the simulation to explore this idea in more detail, perhaps changing some of the parameters. For example, if we increase \\(N\\), are we more or less likely to lose one of the traits? With individual based models like this one are stochastic, thus it is essential to run simulations many times to understand what happens. With our code scattered about in chunks, it is hard to quickly repeat the simulation. Instead we can wrap it all up in a function: unbiased_transmission_1 &lt;- function(N, t_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) output &lt;- tibble(generation = 1:t_max, p = rep(NA, t_max)) output$p[1] &lt;- sum(population$trait == &quot;A&quot;) / N for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output$p[t] &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t } output } This is just all of the code snippets that we already ran above, but all within a function with parameters \\(N\\) and \\(t_{\\text{max}}\\) as arguments to the function. In addition, unbiased_transmission_1() ends with the line output. This means that this tibble will be exported from the function when it is run. This can be useful for storing data from simulations wrapped in functions, otherwise that data is lost after the function is executed. Nothing will happen when you run the above code, because all you’ve done is define the function, not actually run it. The point is that we can now call the function in one go, easily changing the values of \\(N\\) and \\(t_{\\text{max}}\\). Let’s try first with the same values of \\(N\\) and \\(t_{\\text{max}}\\) as before, and save the output from the simulation into data_model, as a record of what happened. data_model &lt;- unbiased_transmission_1(N = 100, t_max = 200) We need to also write another function to plot the data, so we do not need to rewrite all the plotting instruction each time. Whereas it may look unpractical now, it is convenient to keep separate the function that runs the simulation and the function that plots the data for various reasons. First, with more complicated models, we do not want to rerun a simulation only because we want to change some detail in the plot. Second, it makes conceptually sense to keep separated the raw output of the model from the various ways we can visualise it, or the further analysis we want to perform on it. As above, the code is identical to what we already wrote: plot_single_run &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) } At this point, we can visualise the results: plot_single_run(data_model) As anticipated, the plot is different from the simulation we run before, even if the code is exactly the same. This is due to the stochastic nature of the simualtion. This time, the trait \\(A\\) reach the frequenct of 1, meaning that all individuals have it, and the trait \\(B\\) disappeared. Now let’s try changing the parameters. We can call the simulation and the plotting functions together. The following code reruns and plots the simulation with a much larger \\(N\\). data_model &lt;- unbiased_transmission_1(N = 10000, t_max = 200) plot_single_run(data_model) You should see much less fluctuation. Rarely in a population of \\(N = 10000\\) will either trait go to fixation. 1.5 Run several independent simulations and plot their results Wrapping a simulation in a function like this is good because we can easily re-run it with just a single command. However, it’s a bit laborious to manually re-run it. Say we wanted to re-run the simulation 10 times with the same parameter values to see how many times \\(A\\) goes to fixation, and how many times \\(B\\) goes to fixation. Currently, we’d have to manually run the unbiased_transmission_1() function 10 times and record somewhere else what happened in each run. It would be better to automatically re-run the simulation several times and plot each run as a separate line on the same plot. We could also add a line showing the mean value of \\(p\\) across all runs. Let’s use a new parameter \\(r_{\\text{max}}\\) to specify the number of independent runs, and use another for-loop to cycle over the \\(r_{\\text{max}}\\) runs. Let’s rewrite the unbiased_transmission_1() function to handle multiple runs, we will call the new function unbiased_transmission_2(). unbiased_transmission_2 &lt;- function(N, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { # for each run population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } There are a few changes here. First, we need a different output tibble, because we need to store data for all the runs. For that, we initialise the same \\(generation\\) and \\(p\\) columns as before, but with space for all the runs. \\(generation\\) is now built by repeating the count of each generation for \\(r_{\\text{max}}\\) times, and \\(p\\) is simply NA repeated for all generations, for all runs. We need a new column called \\(run\\), that keeps track of which run are the data in the other two columns. Notice that the definition of \\(run\\) is preceded by as.factor(). This specifies the type of the data, a ‘factor’ in this case, and it serves the purpose of making explicit that, even if different runs are labeled with numbers (run 1, 2, 3, etc.), this should not be intended literally: the numbers, in this case, are just names for the runs, and there is not any special meaning in them. Runs could have been called with letters, or any other label you can think of. Whereas this does not make any difference when running the simulation, it would create problems when plotting the data, as ggplot would not plot the runs as separate entities, but it would presuppose they are ordered according to their number, and plot something that is not what we want (you can try by yourself modifying the definition of output). This is a good example of how it is important to have a clear understanding of your data before trying to plot or analyse them. We then set up our \\(r\\) loop, which executes once for each run. The code is mostly the same as before, except that we now use the [output$generation == t &amp; output$run == r, ] notation to put \\(p\\) into the right place in output. The plotting function is also changed to handle multiple runs: plot_multiple_runs &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line(aes(colour = run)) + stat_summary(fun.y = mean, geom = &quot;line&quot;, size = 1) + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) } To understand how it works, we need to explain the general functioning of ggplot. If you remember from above, aes() specifies the ‘aesthetics’, or how the data are mapped in the plot, and it is independent from all possible visual representations of the same mapping, or ‘geoms’. If we declare a specific aesthetics when we call ggplot(), this aesthetics will be applied to all geoms we call after. Alternatively, we can specify the aesthetics in the geom itself. For example this code: ggplot(data = output, aes(y = p, x = generation)) + geom_line() is equivalent to this one: ggplot(data = output) + geom_line(aes(y = p, x = generation)) However, we can use this property to make more complex plots. The plot in our function has a first geom, geom_line(), that inherits the aesthetics as specified in the call to ggplot() and has, in addition, a new mapping, colour = run, which says to plot separately, with a different colour, the data identified by different runs. After that, there is the command stat_summary(), used to calculate the mean of all runs, that only inherits the mapping specified in the ggplot() call. If in the aesthetic of stat_summary() we had also specified colour = run, it would separate the data by run, and it would calculate the mean of each run, which would be the trend itself. For this reason, we did not put colour = run in the ggplot() call, but only in geom_line(). As above, there are various ways to obtain the same result. This code: ggplot(data = output) + geom_line(aes(y = p, x = generation, colour = run)) + stat_summary(aes(y = p, x = generation), fun.y = mean, geom = &quot;line&quot;, size = 1) is equivalent to the code we wrapped in the function. However, our code is clearer, as it makes evident what is the global mapping, and what is the mapping specific to the various visual representations. stat_summary() is a generic ggplot function, that can be used to plot different statistics to summarise our data. In this case, we say that we want to calculate the mean on the data mapped in \\(y\\), that we want to plot them with a line, and that we want this line to be thicker than the lines for the single runs. The default line size for geom_line is 0.5, so size = 1 makes the line that represents the mean of the runs thicker. Let’s now run the function and plot the results for five runs with the same parameters we used at the beginning (\\(N=100\\) and \\(t_{\\text{max}}=200\\)): data_model &lt;- unbiased_transmission_2(N = 100, t_max = 200, r_max = 5) plot_multiple_runs(data_model) You should be able to see five independent runs of our simulation shown as regular thin lines, along with a thicker line showing the mean of these lines. Some runs have probably gone to 0 or 1, and the mean should be somewhere in between. The data is stored in data_model, which we can inspect by writing its name. data_model ## # A tibble: 1,000 x 3 ## generation p run ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 0.53 1 ## 2 2 0.61 1 ## 3 3 0.55 1 ## 4 4 0.47 1 ## 5 5 0.47 1 ## 6 6 0.37 1 ## 7 7 0.34 1 ## 8 8 0.4 1 ## 9 9 0.44 1 ## 10 10 0.52 1 ## # … with 990 more rows Now let’s run unbiased_transmission_2() model with \\(N = 10000\\), to compare with \\(N = 100\\). data_model &lt;- unbiased_transmission_2(N = 10000, t_max = 200, r_max = 5) plot_multiple_runs(data_model) The mean line should be almost exactly at \\(p = 0.5\\) now, with the five independent runs fairly close to it. 1.6 Varying initial conditions Let’s add one final modification. So far the starting frequencies of \\(A\\) and \\(B\\) have been the same, roughly 0.5 each. But what if we were to start at different initial frequencies of \\(A\\) and \\(B\\)? Say, \\(p = 0.2\\) or \\(p = 0.9\\)? Would unbiased transmission keep \\(p\\) at these initial values, or would it go to \\(p = 0.5\\) as we have found so far? To find out, we can add another parameter, \\(p_0\\), which specifies the initial probability of drawing an \\(A\\) rather than a \\(B\\) in the first generation. Previously this was always \\(p_0 = 0.5\\), but in the new function below we add it to the sample() function to weight the initial allocation of traits in \\(t = 1\\). unbiased_transmission_3 &lt;- function(N, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } unbiased_transmission_3() is mostly equal to the previous function. The only changes are the addition of \\(p_0\\) as an argument for the function, and the \\(prob\\) argument in the sample() command. The \\(prob\\) argument gives the probability of picking each option, in our case \\(A\\) and \\(B\\), in the first generation. The probability of \\(A\\) is now \\(p_0\\), and the probability of \\(B\\) is now \\(1 - p_0\\). We can use the same plotting function as before to visualise the result. Let’s see what happens with a different value of \\(p_0\\), for example \\(p_0 = 0.2\\). data_model &lt;- unbiased_transmission_3(N = 10000, p_0 = 0.2, t_max = 200, r_max = 5) plot_multiple_runs(data_model) With \\(p_0 = 0.2\\), trait frequencies stay at \\(p = 0.2\\). Unbiased transmission is truly non-directional: it maintains trait frequencies at whatever they were in the previous generation, barring random fluctuations caused by small population sizes. 1.7 Summary of the model Even this extremely simple model provides some valuable insights. First, unbiased transmission does not in itself change trait frequencies. As long as populations are large, trait frequencies remain the same. Second, the smaller the population size, the more likely traits are to be lost by chance. This is a basic insight from population genetics, known there as genetic drift, but it can also be applied to cultural evolution. Many studies have tested (and some supported) the idea that population size and other demographic factors can shape cultural diversity. Furthermore, generating expectations about cultural change under simple assumptions like random cultural drift can be useful for detecting non-random patterns like selection: if we don’t have a baseline, we won’t know selection or other directional processes when we see them. We also have introduced several programming techniques that will be useful in later simulations. We have seen how to use tibbles to hold characteristics of individuals and the outputs of simulations, how to use loops to cycle through generations and simulation runs, how to use sample() to pick randomly from sets of elements, how to wrap simulations in functions to easily re-run them with different parameter values, and how to use ggplot() to plot the results of simulations. 1.8 Analytical appendix If \\(p\\) is the frequency of \\(A\\) in one generation, we are interested in calculating \\(p&#39;\\), the frequency of \\(A\\) in the next generation under the assumption of unbiased transmission. Each new individual in the next generation picks a demonstrator at random from among the previous generation. The demonstrator will have \\(A\\) with probability \\(p\\). The frequency of \\(A\\) in the next generation, then, is simply the frequency of \\(A\\) in the previous generation: \\[p&#39; = p \\hspace{30 mm}(1.1)\\] Equation 1.1 simply says that under unbiased transmission there is no change in \\(p\\) over time. If, as we assumed above, the initial value of \\(p\\) in a particular population is \\(p_0\\), then the equilibrium value of \\(p\\), \\(p^*\\), at which there is no change in \\(p\\) over time, is just \\(p_0\\). We can plot this recursion, to recreate the final simulation plot above: p_0 &lt;- 0.2 t_max &lt;- 200 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Don’t worry, it gets more complicated than this in later chapters. The key point here is that analytical (or deterministic) models assume infinite populations (note there is no \\(N\\) in the above recursion) and no stochasticity. Simulations with very large populations should give the same results as analytical models. Basically, the closer we can get in stochastic models to the assumption of infinite populations, the closer the match to infinite-population deterministic models. Deterministic models give the ideal case; stochastic models permit more realistic dynamics based on finite populations. More generally, creating deterministic recursion-based models can be a good way of verifying simulation models, and vice versa: if the same dynamics occur in both individuals-based and recursion-based models, then we can be more confident that those dynamics are genuine and not the result of a programming error or mathematical mistake. "],
["unbiased-and-biased-mutation.html", "2 Unbiased and biased mutation 2.1 Unbiased mutation 2.2 Biased mutation 2.3 Summary of the model 2.4 Analytical appendix", " 2 Unbiased and biased mutation Evolution doesn’t work without a source of variation that introduces new variation upon which selection, drift and other processes can act. In genetic evolution, mutation is almost always blind with respect to function. Beneficial genetic mutations are no more likely to arise when they are needed than when they are not needed - in fact most genetic mutations are neutral or detrimental to an organism. Cultural evolution is more interesting, in that novel variation may sometimes be directed to solve specific problems, or systematically biased due to features of our cognition. In the models below we’ll simulate both unbiased and biased mutation. 2.1 Unbiased mutation First we will simulate unbiased mutation in the same basic model as used in the previous chapter. We’ll remove unbiased transmission to see the effect of unbiased mutation alone. As in the previous model, we assume \\(N\\) individuals each of whom possesses one of two non-overlapping cultural traits, denoted \\(A\\) and \\(B\\). In each generation, from \\(t = 1\\) to \\(t = t_{\\text{max}}\\), the \\(N\\) individuals are replaced with \\(N\\) new individuals. Instead of random copying, each individual now gives rise to a new individual with exactly the same cultural trait as them. (Another way of looking at this is in terms of timesteps, such as years: the same \\(N\\) individual live for \\(t_{\\text{max}}\\) years, and keep their cultural trait from one year to the next.) At each generation, however, there is a probability \\(\\mu\\) that each individual mutates from their current trait to the other trait. Vegetarian individuals can decide to eat animal products, and vice versa. Remember, this is not copied from other individuals, as in the previous model, but can be thought as an individual decision. another way to say it, is that the probability of changing trait applies to each individual independently; whether an individual mutates has no bearing on whether or how many other individuals have mutated. On average, that means that \\(\\mu N\\) individuals mutate each generation. Like in the previous model, we are interested in tracking the proportion \\(p\\) of agents with trait \\(A\\) over time. We’ll wrap this in a function called unbiased_mutation(), using much of the same code as unbiased_transmission_3(). As before, we need to call the tidyverse library, and set a seed for the random number genrator, so the results will be exactly the same each time we rerun the code. library(tidyverse) set.seed(111) unbiased_mutation &lt;- function(N, mu, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mutate &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # find &#39;mutant&#39; individuals if (nrow(population[mutate &amp; previous_population$trait == &quot;A&quot;, ]) &gt; 0) { # if there are &#39;mutant&#39; from A to B population[mutate &amp; previous_population$trait == &quot;A&quot;, ]$trait &lt;- &quot;B&quot; # then flip them to B } if (nrow(population[mutate &amp; previous_population$trait == &quot;B&quot;, ]) &gt; 0) { # if there are &#39;mutant&#39; from B to A population[mutate &amp; previous_population$trait == &quot;B&quot;, ]$trait &lt;- &quot;A&quot; # then flip them to A } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } The only changes from the previous model are the addition of \\(\\mu\\), the parameter that specifies the probability of mutation, in the function definition and new lines of code within the for loop on \\(t\\) which replace the random copying command with unbiased mutation. Let’s examine these lines to see how they work. The most obvious way of implementing unbiased mutation - which is not done above - would have been to set up another for loop. We would cycle through each individual one by one, each time calculating whether it should mutate or not based on \\(mu\\). This would certainly work, but R is notoriously slow at loops. It’s always preferable in R, where possible, to use ‘vectorised’ code. That’s what is done above in our three added lines, starting from mutate &lt;- sample(). First, we pre-specify the probability of mutating for each individual For that, we use again the function sample(), picking \\(TRUE\\) (corresponding to be a mutant) or \\(FALSE\\) (keeping the same cultural trait) for \\(N\\) times. The draw, however, is not random: the probability of drawing \\(TRUE\\) is equal to \\(\\mu\\), and the probability to draw \\(FALSE\\) is \\(1-\\mu\\). You can think about the procedure in this way: each individual in the population flips a biased coin that has \\(\\mu\\) probability to land on, say, head, and \\(1-\\mu\\) to land on tail. If it land on head they change their cultural trait. After that, in the following lines, we actually change the traits for the ‘mutant’ individuals. We need to check whether there are individuals that change their trait, both from \\(A\\) to \\(B\\) and viceversa, using the two if conditionals. This is because if there are not individuals changing their traits, we are trying to assign a new value to an empty tibble, and we get an error. To check, we make sure that the number of rows is higher than 0 (using nrow()&gt;0 within the if). To plot the results, we can use the same function plot_multiple_runs() we wrote in the previous chapter. Let’s now run and plot the model: data_model &lt;- unbiased_mutation(N = 100, mu = 0.05, p_0 = 0.5, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Unbiased mutation produces random fluctuations over time, and does not alter the overall frequency of \\(A\\), which stays around \\(p = 0.5\\). Because mutations from \\(A\\) to \\(B\\) are as equally likely as \\(B\\) to \\(A\\), there is no overall directional trend. If you remember from the previous chapter, with unbiased transmission, instead, when populations where small generally one of the trait was disappearing after some generations. Why this difference? You can think it in this way: when one trait becomes popular, say the frequency of \\(A\\) is equal to \\(0.8\\), with unbiased transmission it is more likely that individuals of the new generation will pick up randomly \\(A\\) when copying. The few individuals will trait \\(B\\) will have 80% of probability to copy \\(A\\). With unbiased mutation, instead, since \\(\\mu\\) is applied independently to each individual, when \\(A\\) is popular there will be more individuals that will flip to \\(B\\) (namely, \\(\\mu p N\\), in our case 4) than individual that will flip to \\(A\\) (equal to \\(\\mu (1-p) N\\), in our case 1) keeping the traits at similar freuqencies. But what if we were to start at different initial frequencies of \\(A\\) and \\(B\\)? Say, \\(p=0.1\\) and \\(p=0.9\\)? Would \\(A\\) disappear? Would unbiased mutation keep \\(p\\) at these initial values, like we saw unbiased transmission does in Model 1? To find out, let’s change \\(p_0\\), which, as you may recall, specifies the initial probability of drawing an \\(A\\) rather than a \\(B\\) in the first generation. data_model &lt;- unbiased_mutation(N = 100, mu = 0.05, p_0 = 0.1, t_max = 200, r_max = 5) plot_multiple_runs(data_model) You should see \\(p\\) go from 0.1 up to 0.5. In fact, whatever the initial starting frequencies of \\(A\\) and \\(B\\), unbiased mutation always leads to \\(p = 0.5\\), for the reason explained above: unbiased mutation always tends to balance the proportion of \\(A\\)s and \\(B\\)s. 2.2 Biased mutation A more interesting case is biased mutation. Let’s assume now that there is a probability \\(\\mu_b\\) that an individual with trait \\(B\\) mutates into \\(A\\), but there is no possibility of trait \\(A\\) mutating into trait \\(B\\). Perhaps trait \\(A\\) is a particularly catchy or memorable version of a story, or an intuitive explanation of a phenomenon, and \\(B\\) is difficult to remember or unintuitive. The function biased_mutation() captures this unidirectional mutation. biased_mutation &lt;- function(N, mu_b, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mutate &lt;- sample(c(TRUE, FALSE), N, prob = c(mu_b, 1 - mu_b), replace = TRUE) # find &#39;mutant&#39; individuals if (nrow(population[mutate &amp; previous_population$trait == &quot;B&quot;, ]) &gt; 0) { population[mutate &amp; previous_population$trait == &quot;B&quot;, ]$trait &lt;- &quot;A&quot; # if individual was B and mutates, flip to A } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } There are just two changes in this code compared to unbiased_mutation(). First, we’ve replaced \\(\\mu\\) with \\(\\mu_b\\) to keep the two parameters distinct and avoid confusion. Second, the line in unbiased_mutation() which caused individuals with \\(A\\) to mutate to \\(B\\) has been deleted. Let’s see what effect this has by running biased_mutation(). We’ll start with the population entirely composed of individuals with \\(B\\), i.e. \\(p_0 = 0\\), to see how quickly and in what manner \\(A\\) spreads via biased mutation. data_model &lt;- biased_mutation(N = 100, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) The plot shows a steep increase that slows and plateaus at \\(p = 1\\) by around generation \\(t = 100\\). There should be a bit of fluctuation in the different runs, but not much. Now let’s try a larger sample size. data_model &lt;- biased_mutation(N = 10000, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) With \\(N = 10000\\) the line should be smooth with little (if any) fluctuation across the runs. But notice that it plateaus at about the same generation, around \\(t = 100\\). Population size has little effect on the rate at which a novel trait spreads via biased mutation. \\(\\mu_b\\), on the other hand, does affect this speed. Let’s double the biased mutation rate to 0.1. data_model &lt;- biased_mutation(N = 10000, mu_b = 0.1, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Now trait \\(A\\) reaches fixation around generation \\(t = 50\\). Play around with \\(N\\) and \\(\\mu_b\\) to confirm that the latter determines the rate of diffusion of trait \\(A\\), and that it takes the same form each time - roughly an ‘r’ shape with an initial steep increase followed by a plateauing at \\(p = 1\\). 2.3 Summary of the model With this simple model we can draw the following insights. Unbiased mutation, which resembles genetic mutation in being non-directional, always leads to an equal mix of the two traits. It introduces and maintains cultural variation in the population. It is interesting to compare unbiased mutation to unbiased transmission from Model 1. While unbiased transmission did not change \\(p\\) over time, unbiased mutation always converges on \\(p^* = 0.5\\), irrespective of the starting frequency. (NB \\(p^* = 0.5\\) assuming there are two traits; more generally, \\(p^* = 1/v\\), where \\(v\\) is the number of traits.) Biased mutation, which is far more common - perhaps even typical - in cultural evolution, shows different dynamics. Novel traits favoured by biased mutation spread in a characteristic fashion - an r-shaped diffusion curve - with a speed characterised by the mutation rate \\(\\mu_b\\). Population size has little effect, whether \\(N = 100\\) or \\(N = 10000\\). Whenever biased mutation is present (\\(\\mu_b &gt; 0\\)), the favoured trait goes to fixation, even if it is not initially present. In terms of programming techniques, the major novelty in Model 2 is the use of sample() to determine which individuals should undergo whatever the fixed probability specifies (in our case, mutation). This could be done with a loop, but vectorising code in the way we did here is much faster in R than loops. 2.4 Analytical appendix If \\(p\\) is the frequency of \\(A\\) in one generation, we are interested in calculating \\(p&#39;\\), the frequency of \\(A\\) in the next generation under the assumption of unbiased mutation. The next generation retains the cultural traits of the previous generation, except that \\(\\mu\\) of them switch to the other trait. There are therefore two sources of \\(A\\) in the next generation: members of the previous generation who had \\(A\\) and didn’t mutate, therefore staying \\(A\\), and members of the previous generation who had \\(B\\) and did mutate, therefore switching to \\(A\\). The frequency of \\(A\\) in the next generation is therefore: \\[p&#39; = p(1-\\mu) + (1-p)\\mu \\hspace{30 mm}(2.1)\\] The first term on the right-hand side of Equation 2.1 represents the first group, the \\((1 - \\mu)\\) proportion of the \\(p\\) \\(A\\)-carriers who didn’t mutate. The second term represents the second group, the \\(\\mu\\) proportion of the \\(1 - p\\) \\(B\\)-carriers who did mutate. To calculate the equilibrium value of \\(p\\), \\(p^*\\), we want to know when \\(p&#39; = p\\), or when the frequency of \\(A\\) in one generation is identical to the frequency of \\(A\\) in the next generation. This can be found by setting \\(p&#39; = p\\) in Equation 2.1, which gives: \\[p = p(1-\\mu) + (1-p)\\mu \\hspace{30 mm}(2.2)\\] Rearranging Equation 2.2 gives: \\[\\mu(1 - 2p) = 0 \\hspace{30 mm}(2.3)\\] The left-hand side of Equation 2.3 equals zero when either \\(\\mu = 0\\), which given our assumption that \\(\\mu &gt; 0\\) cannot be the case, or when \\(1 - 2p = 0\\), which after rearranging gives the single equilibrium \\(p^* = 0.5\\). This matches our simulation results above. As we found in the simulations, this does not depend on \\(\\mu\\) or the starting frequency of \\(p\\). We can also plot the recursion in Equation 2.1 like so: p_0 &lt;- 0 t_max &lt;- 200 mu &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] * (1 - mu) + (1 - pop_analytical$p[i - 1]) * mu } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Again, this should resemble the figure generated by the simulations above, and confirm that \\(p^* = 0.5\\). For biased mutation, assume that only \\(B\\)s are switching to \\(A\\), and with probability \\(\\mu_b\\) instead of \\(\\mu\\). The first term on the right hand side becomes simply \\(p\\), because \\(A\\)s do not switch. The second term remains the same, but with \\(\\mu_b\\). Thus, \\[p&#39; = p + (1-p)\\mu_b \\hspace{30 mm}(2.4)\\] The equilibrium value \\(p^*\\) can be found by again setting \\(p&#39; = p\\) and solving for \\(p\\). Assuming \\(\\mu_b &gt; 0\\), this gives the single equilibrium \\(p^* = 1\\), which again matches the simulation results. We can plot the above recursion like so: p_0 &lt;- 0 t_max &lt;- 200 mu_b &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] + (1 - pop_analytical$p[i - 1]) * mu_b } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Hopefully, this looks identical to the final simulation plot with the same value of \\(\\mu_b\\). Furthermore, we can specify an equation for the change in \\(p\\) from one generation to the next, or \\(\\Delta p\\). We do this by subtracting \\(p\\) from both sides of Equation 2.4, giving: \\[\\Delta p = p&#39; - p = (1-p)\\mu_b \\hspace{30 mm}(2.5)\\] Seeing this helps explain two things. First, the \\(1 - p\\) part explains the r-shape of the curve. It says that the smaller is \\(p\\), the larger \\(\\Delta p\\) will be. This explains why \\(p\\) increases in frequency very quickly at first, when \\(p\\) is near zero, and the increase slows when \\(p\\) gets larger. We have already determined that the increase stops altogether (i.e. \\(\\Delta p\\) = 0) when \\(p = p^* = 1\\). Second, it says that the rate of increase is proportional to \\(\\mu_b\\). This explains our observation in the simulations that larger values of \\(\\mu_b\\) cause \\(p\\) to reach its maximum value faster. "],
["biased-transmission-directcontent-bias.html", "3 Biased transmission (direct/content bias) 3.1 Strenght of selection 3.2 Summary of the model 3.3 Analytical appendix", " 3 Biased transmission (direct/content bias) So far we have looked at unbiased transmission (Chapter 1) and mutation, both unbiased and biased (Chapter 2). Let’s complete the set by looking at biased transmission. This occurs when one trait or one demonstrator is more likely to be copied than another trait or demonstrator. Trait-based copying is often called ‘direct’ or ‘content’ bias, while demonstrator-based copying is often called ‘indirect’ or ‘context’ bias. Both are sometimes also called ‘cultural selection’ because one thing (trait or demonstrator) is selected to be copied over another. In this chapter, we wil look at trait-based (direct, content) bias. (As an aside, there is a confusing array of terminology in the field of cultural evolution, as illustrated by the preceding paragraph. That’s why models are so useful. Words and verbal descriptions can be ambiguous. Often the writer doesn’t realise that there are hidden assumptions or unrecognised ambiguities in their descriptions. They may not realise that what they mean by ‘cultural selection’ is entirely different to how someone else uses it. Models are great because they force us to precisely specify exactly what we mean by a particular term or process. I can use the words in the paragraph above to describe biased transmission, but it’s only really clear when I model it, making all my assumptions explicit.) To simulate biased transmission, following the simulations in Chapter 1,we assume there are two traits \\(A\\) and \\(B\\), and that each individual chooses another individual from the previous generation at random. This time, however, we associate to the traits two different probabilities of being copied: we can call them, \\(s_a\\) and \\(s_b\\) respectively. When an individual encounter an individual with trait \\(A\\), they will copy them with probability \\(s_a\\), and when they encounter an individual with trait \\(B\\), they will copy them with probability \\(s_b\\). With \\(s_a=s_b\\), copying is unbiased, and individuals switch to the encountered alternative with the same probabilty. This reproduces the results of the simulations when transmission is unbiased. If \\(s_a=s_b=1\\), the model is exactly the same of Chapter 1. The relevant situation is when \\(s_a&gt;s_b\\) (or vice versa), so that we have biased transmission. Perhaps \\(A\\) (or \\(B\\)) is a more effective tool, a more memorable story, or a more easily pronounced word. Let’s first write the function, and then explore what happens in this case. Below is a function biased_ransmission_direct() that implements all of these ideas library(tidyverse) set.seed(111) biased_ransmission_direct &lt;- function (N, s_a, s_b, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble demonstrator_trait &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # for each individual, pick a random individual from the previous generation to act as demonstrator and store their trait # biased probabilities to copy: copy_a &lt;- sample(c(TRUE, FALSE), N, prob = c(s_a, 1 - s_a), replace = TRUE) copy_b &lt;- sample(c(TRUE, FALSE), N, prob = c(s_b, 1 - s_b), replace = TRUE) if (nrow(population[copy_a &amp; demonstrator_trait == &quot;A&quot;, ]) &gt; 0) { population[copy_a &amp; demonstrator_trait == &quot;A&quot;, ]$trait &lt;- &quot;A&quot; } if (nrow(population[copy_b &amp; demonstrator_trait == &quot;B&quot;, ]) &gt; 0) { population[copy_b &amp; demonstrator_trait == &quot;B&quot;, ]$trait &lt;- &quot;B&quot; } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } Most of biased_ransmission_direct() is recycled from the previous models. As before, we initalise the data structure output from multiple runs, and in generation \\(t = 1\\), we create a population tibble to hold the trait of each individual. The major change is that we now include biased transmission. We first select at random the demonstrators from the previous generation (using the same code we used in unbiased_transmission()) and we store their trait in demonstrator_trait. Then we get the probabilities to copy \\(A\\) and to copy \\(B\\) for all the population, using the same code used in biased_mutation(), only that this time it produces a probability to copy. Again using the same code as in biased mutation(), we have the individuals copy the trait at hand with the desired probability. Let’s run our function biased_ransmission_direct(). As before, to plot the results, we can use the same function plot_multiple_runs() we wrote in Chapter 1. As we discussed, the interesting case is when one trait is favored with respect to the other. We can assume, for example, \\(s_a=0.1\\) and \\(s_b=0\\). This means that when individuals encounter another individual with trait \\(A\\) they copy them 1 every 10 times, but, when individuals encounter another individual with trait \\(B\\), they never switch. We can also assume that the favoured trait, \\(A\\), is initially rare in the population (\\(p_0=0.01\\)) to see how selection favours this initially-rare trait (\\(p_0\\) needs to be higher than 0. Since we are re not including any mutation in the model, we need to include at least some \\(A\\)s at the beginning of the simulation, otherwise it would never appear). data_model &lt;- biased_ransmission_direct(N = 10000, s_a = 0.1, s_b = 0 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) With a moderate selection strength we can see that \\(A\\) gradually replaces \\(B\\) and goes to fixation. It does this in a characteristic manner: the increase is slow at first, then picks up speed, then plateaus. Note the difference to biased mutation. Where biased mutation was r-shaped, with a steep initial increase, biased transmission is s-shaped, with an initial slow uptake. This is because the strength of biased transmission (like selection in general) is proportional to the variation in the population. When \\(A\\) is rare initially, there is only a small chance of picking another individual with \\(A\\). As \\(A\\) spreads, the chances of picking an \\(A\\) individual increases. As \\(A\\) becomes very common, there are few \\(B\\) individuals left to switch. In the case of biased mutation, instead, the probability to switch is independent from the variation in the population. 3.1 Strenght of selection From what does the strength of the selection depend? First, the strength is independent from the specific values of \\(s_a\\) and \\(s_b\\). What counts is their relative difference, in this case \\(s_a-s_b = 0.1\\). We can try and run a simulation whith, say, \\(s_a=0.6\\) and \\(s_b=0.5\\). We observe the same pattern, with slighlty more noise - the single runs are more different one another with respect to the previois simulation - due to the fact that switches from \\(A\\) to \\(B\\) are also possible. data_model &lt;- biased_ransmission_direct(N = 10000, s_a = 0.6, s_b = 0.5 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) To change the selection strength, we need to modify the difference between \\(s_a\\) and \\(s_b\\). We can double the strenght by setting \\(s_a = 0.2\\), and keeping \\(s_b=0\\). data_model &lt;- biased_ransmission_direct(N = 10000, s_a = 0.2, s_b = 0 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) As we might expect, increasing the strength of selection increases the speed with which \\(A\\) goes to fixation. Note, though, that it retains the s-shape. 3.2 Summary of the model We saw how biased transmission causes a trait favoured by cultural selection to spread and go to fixation in a population, even when it is initially very rare. Biased transmission differs in its dynamics from biased mutation. Its action is proportional to the variation in the population at the time at which it acts. It is strongest when there is lots of variation (in our model, when there are equal numbers of \\(A\\) and \\(B\\) at \\(p = 0.5\\)), and weakest when there is little variation (when \\(p\\) is close to 0 or 1). 3.3 Analytical appendix As before, we have \\(p\\) individuals with trait \\(A\\), and \\(1 - p\\) individuals with trait \\(B\\). As we saw that what is important is the relative difference between the two probabilites of being copied associated to the two traits and not their absolute value, we consider always \\(s_b=0\\), and vary \\(s_a\\), which we can call simply \\(s\\). Thus, the \\(p\\) individuals with trait \\(A\\) always keep their \\(A\\)s. The \\(1 - p\\) individuals with trait \\(B\\) pick another individual at random, hence with probability \\(p\\), and with probability \\(s\\) they switch to trait \\(A\\). We can therefore write the recursion for \\(p\\) under biased transmission as: \\[p&#39; = p + p(1-p)s \\hspace{30 mm}(3.1)\\] The first term on the right-hand side is the unchanged \\(A\\) bearers, and the second term is the \\(1-p\\) \\(B\\)-bearers who find one of the \\(p\\) \\(A\\)-bearers and switch with probability \\(s\\). Here is some code to plot this biased transmission recursion: t_max &lt;- 150 s &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- 0.01 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] + pop_analytical$p[i - 1] * (1 - pop_analytical$p[i - 1]) * s } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) The curve above should be identical to the simulation curve, given that the simulation had the same biased transmission strength \\(s\\) and a large enough \\(N\\) to minimise stochasticity. From the equation above, we can see how the strength of biased transmission depends on variation in the population, given that \\(p(1 - p)\\) is the formula for variation. This determines the shape of the curve, while \\(s\\) determines the speed with which the equilibrium \\(p^*\\) is reached. But what is the equilibrium \\(p^*\\) here? In fact there are two. As before, the equilibrium can be found by setting the change in \\(p\\) to zero, or when: \\[p(1-p)s = 0 \\hspace{30 mm}(3.2)\\] There are three ways in which the left-hand side can equal zero: when \\(p = 0\\), when \\(p = 1\\) and when \\(s = 0\\). The last case is uninteresting: it would mean that biased transmission is not occurring. The first two cases simply say that if either trait reaches fixation, then it will stay at fixation. This is to be expected, given that we have no mutation in our model. It contrasts with unbiased and biased mutation, where there is only one equlibrium value of \\(p\\). We can also say that \\(p = 0\\) is an unstable equilibrium, meaning that any slight perturbation away from \\(p = 0\\) moves \\(p\\) away from that value. This is essentially what we simulated above: a slight perturbation up to \\(p = 0.01\\) went all the way up to \\(p = 1\\). In contrast, \\(p = 1\\) is a stable equilibrium: any slight perturbation from \\(p = 1\\) immediately goes back to \\(p = 1\\). "],
["biased-transmission-conformist-bias.html", "4 Biased transmission (conformist bias) 4.1 Summary of Model 4 4.2 Analytic appendix", " 4 Biased transmission (conformist bias) Model 3 looked at the case where one cultural trait is intrinsically more likely to be copied than another trait. Here we will look at another kind of biased transmission: conformity (or ‘positive frequency dependent bias’). Here, individuals are disproportionately more likely to adopt the most common trait in the population, irrespective of its intrinsic characteristics. For example, imagine trait \\(A\\) has a frequency of 0.7 in the population, with the rest possessing trait \\(B\\). An unbiased learner would adopt trait \\(A\\) with a probability exactly equal to 0.7. This is unbiased transmission, and is what happens in Model 1: by picking a member of the previous generation at random, the probability of adoption in Model 1 is equal to the frequency of that trait amongst the previous generation. A conformist learner, on the other hand, would adopt trait \\(A\\) with a probability greater than 0.7. In other words, common traits get an ‘adoption boost’ relative to unbiased transmission. Uncommon traits get an equivalent ‘adoption penalty’. The magnitude of this boost or penalty can be controlled by a parameter, which we will call \\(D\\). Let’s keep things simple in our model. Rather than assuming that individuals sample across the entire population, which in any case might be implausible in large populations, let’s assume they pick only three demonstrators at random. Why three? This is the minimum number of demonstrators that can yield a majority (i.e. 2 vs 1), which we need in order to implement conformity. When two demonstrators have one trait and the other demonstrator has a different trait, we want to boost the probability of adoption for the majority trait, and reduce it for the minority trait. Following Boyd and Richerson (1985), we can specify the probability of adoption as follows: Table 1: Probability of adopting trait \\(A\\) for each possible combination of traits amongst three demonstrators Demonstrator 1 Demonstrator 2 Demonstrator 3 Probability of adopting trait \\(A\\) \\(A\\) \\(A\\) \\(A\\) 1 \\(A\\) \\(A\\) \\(B\\) \\(A\\) \\(B\\) \\(A\\) \\(2/3 + D/3\\) \\(B\\) \\(A\\) \\(A\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(1/3 - D/3\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) 0 The first row says that when all demonstrators have trait \\(A\\), then trait \\(A\\) is definitely adopted. Similarly, the bottom row says that when all demonstrators have trait \\(B\\), then trait \\(A\\) is never adopted, and by implication trait \\(B\\) is always adopted. For the three combinations where there are two \\(A\\)s and one \\(B\\), the probability of adopting trait \\(A\\) is \\(2/3\\), which it would be under unbiased transmission (because two out of three demonstrators have \\(A\\)), plus the conformist adoption boost specified by \\(D\\). \\(D\\) is divided by three so that it varies from 0 to 1. Similarly, for the three combinations where there are two \\(B\\)s and one \\(A\\), the probability of adopting \\(A\\) is 1/3 minus the conformist adoption penalty specified by \\(D\\). Let’s implement these assumptions in the kind of agent-based model we’ve been building so far. As before, assume \\(N\\) agents each of whom possess one of two traits \\(A\\) or \\(B\\). The frequency of \\(A\\) is denoted by \\(p\\). The initial frequency of \\(A\\) in generation \\(t = 1\\) is \\(p_0\\). Rather than going straight to a function, let’s go step by step. First we’ll specify our parameters, \\(N\\) and \\(p_0\\) as before, plus the new conformity parameter \\(D\\). We can also create an agent dataframe and fill it with \\(A\\)s and \\(B\\)s in the proportion specified by \\(p_0\\), again exactly as before. To remind ourselves what agent looks like, we use the head command. N &lt;- 100 p_0 &lt;- 0.5 D &lt;- 1 agent &lt;- data.frame(trait = sample(c(&quot;A&quot;,&quot;B&quot;), N, replace = TRUE, prob = c(p_0,1-p_0)), stringsAsFactors = FALSE) # create first generation head(agent) ## trait ## 1 A ## 2 A ## 3 A ## 4 A ## 5 A ## 6 A Now we’ll create a dataframe called demonstrators that picks, for each new agent in the next generation, three demonstrators at random from the current population of agents. It therefore needs three columns/variables, one for each of the demonstrators, and \\(N\\) rows, one for each new agent. We fill each column with randomly chosen traits from the agent dataframe. We can view this with head. # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- data.frame(dem1 = sample(agent$trait, N, replace = TRUE), dem2 = sample(agent$trait, N, replace = TRUE), dem3 = sample(agent$trait, N, replace = TRUE), stringsAsFactors = F) head(demonstrators) ## dem1 dem2 dem3 ## 1 B B A ## 2 A A B ## 3 A B A ## 4 A A A ## 5 A A B ## 6 B A A Think of each row here as containing the traits of three randomly-chosen demonstrators chosen by each new next-generation agent. Now we want to calculate the probability of adoption of \\(A\\) for each of these three-trait demonstrator combinations. First we need to get the number of \\(A\\)s in each combination. Then we can replace the traits in agent based on the probabilities in Table 1. When all demonstrators have \\(A\\), we set to \\(A\\). When no demonstrators have \\(A\\), we set to \\(B\\). When two out of three demonstrators have \\(A\\), we set to \\(A\\) with probability \\(2/3 + D/3\\) and \\(B\\) otherwise. When one out of three demonstrators have \\(A\\), we set to \\(A\\) with probability \\(1/3 - D/3\\) and \\(B\\) otherwise. To check it works, we can add the new agent dataframe as a column to demonstrators and view the latter with head. This will let us see the three demonstrators and the resulting new trait side by side. # get the number of As in each 3-dem combo numAs &lt;- rowSums(demonstrators == &quot;A&quot;) agent$trait[numAs == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A agent$trait[numAs == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob &lt;- runif(N) # when A is a majority, 2/3 agent$trait[numAs == 2 &amp; prob &lt; (2/3 + D/3)] &lt;- &quot;A&quot; agent$trait[numAs == 2 &amp; prob &gt;= (2/3 + D/3)] &lt;- &quot;B&quot; # when A is a minority, 1/3 agent$trait[numAs == 1 &amp; prob &lt; (1/3 - D/3)] &lt;- &quot;A&quot; agent$trait[numAs == 1 &amp; prob &gt;= (1/3 - D/3)] &lt;- &quot;B&quot; # for testing only, add the new traits to the demonstrator dataframe and show it demonstrators$newtrait &lt;- agent$trait head(demonstrators, 20) ## dem1 dem2 dem3 newtrait ## 1 B B A B ## 2 A A B A ## 3 A B A A ## 4 A A A A ## 5 A A B A ## 6 B A A A ## 7 A A A A ## 8 A B A A ## 9 A B B B ## 10 A A B A ## 11 B B A B ## 12 A A A A ## 13 A A B A ## 14 A A A A ## 15 A A A A ## 16 B A A A ## 17 A B A A ## 18 A A B A ## 19 A B B B ## 20 A A B A Because we set \\(D=1\\) above, we should see above that the new trait is always the majority trait amongst the three demonstrators. This is perfect conformity. We can weaken conformity by reducing \\(D\\), in the code below. N &lt;- 100 p_0 &lt;- 0.5 D &lt;- 0.1 agent &lt;- data.frame(trait = sample(c(&quot;A&quot;,&quot;B&quot;), N, replace = TRUE, prob = c(p_0,1-p_0)), stringsAsFactors = FALSE) # create first generation # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- data.frame(dem1 = sample(agent$trait, N, replace = TRUE), dem2 = sample(agent$trait, N, replace = TRUE), dem3 = sample(agent$trait, N, replace = TRUE), stringsAsFactors = F) # get the number of As in each 3-dem combo numAs &lt;- rowSums(demonstrators == &quot;A&quot;) agent$trait[numAs == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A agent$trait[numAs == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob &lt;- runif(N) # when A is a majority, 2/3 agent$trait[numAs == 2 &amp; prob &lt; (2/3 + D/3)] &lt;- &quot;A&quot; agent$trait[numAs == 2 &amp; prob &gt;= (2/3 + D/3)] &lt;- &quot;B&quot; # when A is a minority, 1/3 agent$trait[numAs == 1 &amp; prob &lt; (1/3 - D/3)] &lt;- &quot;A&quot; agent$trait[numAs == 1 &amp; prob &gt;= (1/3 - D/3)] &lt;- &quot;B&quot; # for testing only, add the new traits to the demonstrator dataframe and show it demonstrators$newtrait &lt;- agent$trait head(demonstrators, 20) ## dem1 dem2 dem3 newtrait ## 1 A B A A ## 2 A B B A ## 3 B B B B ## 4 B B B B ## 5 A A B B ## 6 B B A B ## 7 A B A A ## 8 B A A A ## 9 A B B B ## 10 A A A A ## 11 A B B B ## 12 B A A A ## 13 A A B B ## 14 A B B A ## 15 B A B B ## 16 B B B B ## 17 A A A A ## 18 A B A A ## 19 B A B A ## 20 A B B B Now that conformity is weaker, sometimes the new trait is not the majority amongst the three demonstrators. With the small sample shown above, it’s perhaps not possible to notice it. Hopefully when we put it all together now into a function and run it over multiple generations, we will notice an effect. The code below is a combination of Model 1 (unbiased transmission) and the code above for conformity. ConformistTransmission &lt;- function (N, p_0, D, t_max, r_max) { output &lt;- as.data.frame(matrix(NA,t_max,r_max)) # create a matrix with t_max rows and r_max columns, filled with NAs, then convert to data.frame names(output) &lt;- paste(&quot;run&quot;, 1:r_max, sep=&quot;&quot;) # purely cosmetic: rename the columns with run1, run2 etc. for (r in 1:r_max) { agent &lt;- data.frame(trait = sample(c(&quot;A&quot;,&quot;B&quot;), N, replace = TRUE, prob = c(p_0,1-p_0)), stringsAsFactors = FALSE) # create first generation output[1,r] &lt;- sum(agent$trait == &quot;A&quot;) / N # add first generation&#39;s p to first row of column r for (t in 2:t_max) { # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- data.frame(dem1 = sample(agent$trait, N, replace = TRUE), dem2 = sample(agent$trait, N, replace = TRUE), dem3 = sample(agent$trait, N, replace = TRUE), stringsAsFactors = F) # get the number of As in each 3-dem combo numAs &lt;- rowSums(demonstrators == &quot;A&quot;) agent$trait[numAs == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A agent$trait[numAs == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob &lt;- runif(N) # when A is a majority, 2/3 agent$trait[numAs == 2 &amp; prob &lt; (2/3 + D/3)] &lt;- &quot;A&quot; agent$trait[numAs == 2 &amp; prob &gt;= (2/3 + D/3)] &lt;- &quot;B&quot; # when A is a minority, 1/3 agent$trait[numAs == 1 &amp; prob &lt; (1/3 - D/3)] &lt;- &quot;A&quot; agent$trait[numAs == 1 &amp; prob &gt;= (1/3 - D/3)] &lt;- &quot;B&quot; output[t,r] &lt;- sum(agent$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } # first plot a thick line for the mean p plot(rowMeans(output), type = &#39;l&#39;, ylab = &quot;p, proportion of agents with trait A&quot;, xlab = &quot;generation&quot;, ylim = c(0,1), lwd = 3, main = paste(&quot;N =&quot;, N, &quot;, D =&quot;, D, &quot;, p_0 =&quot;, p_0, sep = &quot;&quot;)) for (r in 1:r_max) { lines(output[,r], type = &#39;l&#39;) # add lines for each run, up to r_max } output # export data from function } Note that we omit the testing code above (we’ve tested it and it works!), and there’s no need to put agent into previous_agent because we have the demonstrator dataframe doing that job. Let’s run the function. data_model4 &lt;- ConformistTransmission(N = 1000, p_0 = 0.5, D = 1, t_max = 50, r_max = 10) Here we should see some lines going to \\(p = 1\\), and some lines going to \\(p = 0\\). Conformity acts to favour the majority trait. This will depend on the initial frequency of \\(A\\) in the population. In different runs with \\(p_0 = 0.5\\), sometimes there will be slightly more \\(A\\)s, sometimes slightly more \\(B\\)s (remember, in our model this is probabilistic, like flipping coins, so initial frequencies will rarely be precisely 0.5). Let’s compare conformity to unbiased transmission, by setting \\(D = 0\\). data_model4 &lt;- ConformistTransmission(N = 1000, p_0 = 0.5, D = 0, t_max = 50, r_max = 10) As in Model 1 with a sufficiently large \\(N\\), we should see frequencies fluctuating around \\(p = 0.5\\). This underlines the effect of conformity: it drives traits to fixation as they become more and more common. As an aside, note that the last two graphs have roughly the same thick black mean frequency line, which hovers around \\(p = 0.5\\). This highlights the dangers of looking at means alone. If we hadn’t plotted the individual runs and relied solely on mean frequencies, we might think that \\(D = 0\\) and \\(D = 1\\) gave identical results. But in fact, they are very different. Always look at the underlying distribution that generates means. Now let’s explore the effect of changing the initial frequencies by changing \\(p_0\\), and adding conformity back in. data_model4 &lt;- ConformistTransmission(N = 1000, p_0 = 0.55, D = 1, t_max = 50, r_max = 10) When \\(A\\) starts off in a slight majority (\\(p_0 = 0.55\\)), most if not all of the runs should result in \\(A\\) going to fixation. Now let’s try the reverse. data_model4 &lt;- ConformistTransmission(N = 1000, p_0 = 0.45, D = 1, t_max = 50, r_max = 10) When \\(A\\) starts off in a minority (\\(p_0 = 0.45\\)), most if not all runs should result in \\(A\\) disappearing. These last two graphs show how initial conditions affect conformity. Whichever trait is more common is favoured by conformist transmission. 4.1 Summary of Model 4 Model 4 explored conformist biased cultural transmission. This is where individuals are disproportionately more likely to adopt the most common trait among a set of demonstrators. We can contrast this with the direct or content biased transmission from Model 4, where one trait is intrinsically more likely to be copied. With conformity, the traits have no intrinsic attractiveness and are preferentially copied simply because they are common. We saw how conformity increases the frequency of whichever trait is more common. Initial trait frequencies are important here: traits that are initially more common typically go to fixation. This in turn makes stochasticity important, which in small populations can affect initial frequencies. The major programming innovation in Model 4 was the use of an intermediate dataframe to hold the demonstrators. We then created the next generation using a table of probabilities (Table 1), which specified for each combination of demonstrators the probability of adopting each trait. 4.2 Analytic appendix An alternative way of doing all the above is with deterministic recursions, as Boyd &amp; Richerson (1985) originally did. Let’s revise Table 1 to add the probabilities of each combination of three demonstrators coming together, assuming they are picked at random. These probabilities can be expressed in terms of \\(p\\), the frequency of \\(A\\), and \\((1 - p)\\), the frequency of \\(B\\). Table 2 adds this column. Table 2: Full adoption probability table for trait \\(A\\) under conformist transmission Dem 1 Dem 2 Dem 3 Prob of adopting \\(A\\) Prob of combination forming \\(A\\) \\(A\\) \\(A\\) 1 \\(p^3\\) \\(A\\) \\(A\\) \\(B\\) \\(A\\) \\(B\\) \\(A\\) \\(2/3 + D/3\\) \\(p^2(1-p)\\) \\(B\\) \\(A\\) \\(A\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(1/3 - D/3\\) \\(p(1-p)^2\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) 0 (1-p)^3 To get the frequency of \\(A\\) in the next generation, \\(p&#39;\\), we multiply, for each of the eight rows in Table 2, the probability of adopting \\(A\\) by the probability of that combination forming (i.e. the final two columns in Table 2), and add up all of these eight products. After rearranging, this gives the following recursion: \\[p&#39; = p + Dp(1-p)(2p-1) \\hspace{30 mm}(4.1)\\] Now we can create a function for this recursion: ConformityRecursion &lt;- function(D, t_max, p_0) { p &lt;- rep(0,t_max) p[1] &lt;- p_0 for (i in 2:t_max) { p[i] &lt;- p[i-1] + D*p[i-1]*(1-p[i-1])*(2*p[i-1] - 1) } plot(p, type = &quot;l&quot;, ylim = c(0,1), ylab = &quot;frequency of p&quot;, xlab = &quot;generation&quot;, main = paste(&quot;D = &quot;, D, &quot;, p_0 = &quot;, p_0, sep = &quot;&quot;)) } Here, we use a for loop to cycle through each generation, each time updating \\(p\\) according to the recursion equation above. Remember, there is no \\(N\\) here because the recursion is deterministic and assumes an infinite population size; hence there is no stochasticity due to finite population sizes. There is also no need to have multiple runs as each run is identical, hence no \\(r_{max}\\). The following code runs the ConformityRecursion function with weak conformity (\\(D = 0.1\\)) and slightly more \\(A\\) in the initial generation (\\(p_0 = 0.51\\)). ConformityRecursion(D = 0.1, t_max = 150, p_0 = 0.51) As in the agent-based model, the initially most-frequent trait, here \\(A\\), goes to fixation. Let’s compare to the agent-based model with the same parameters, and a large enough \\(N\\) to make stochasticity unimportant. data_model4 &lt;- ConformistTransmission(N = 100000, p_0 = 0.51, D = 0.1, t_max = 150, r_max = 1) It should be a pretty good match. Try playing around with smaller \\(N\\) to show that stochastic agent-based models are most likely to match deterministic recursion models when \\(N\\) is large. Let’s modify the ConformityRecursion function to accept multiple values of \\(p_0\\), so we can plot different starting frequencies on the same graph. ConformityRecursion &lt;- function(D, t_max, p_0) { numSims &lt;- length(p_0) p &lt;- as.data.frame(matrix(NA, nrow = t_max, ncol = numSims)) p[1,] &lt;- p_0 for (i in 2:t_max) { p[i,] &lt;- p[i-1,] + D*p[i-1,]*(1-p[i-1,])*(2*p[i-1,] - 1) } plot(p[,1], type = &quot;l&quot;, ylim = c(0,1), ylab = &quot;frequency of p&quot;, xlab = &quot;generation&quot;, main = paste(&quot;D =&quot;, D)) if (numSims &gt; 1) { for (i in 2:numSims) { lines(p[,i], type = &#39;l&#39;) } } } The following command plots three different values of \\(p_0\\), one less than 0.5, one equal to 0.5, and one greater than 0.5. This should confirm that conformity favours whichever trait is initially most frequent. ConformityRecursion(D = 0.1, t_max = 150, p_0 = c(0.49,0.5,0.51)) Again, this matches the simulations above where some runs are randomly initially above 0.5 and others below 0.5. Finally, we can use the recursion equation to generate a plot that has become a signature for conformity in the cultural evolution literature. The following code plots, for all possible values of \\(p\\), the probability of adopting \\(p\\) in the next generation. p &lt;- seq(0,1,length.out = 101) D &lt;- 1 p_next &lt;- p + D*p*(1-p)*(2*p-1) plot(p, p_next, type = &#39;l&#39;, ylab = &quot;probability of adopting A (p&#39;)&quot;, xlab = &quot;frequency of A (p)&quot;, main = paste(&quot;D =&quot;, D)) abline(a = 0, b = 1, lty = 3) This encapsulates the process of conformity. The dotted line shows unbiased transmission: the probability of adopting \\(A\\) is exactly equal to the frequency of \\(A\\) in the population. The s-shaped solid curve shows conformist transmission. When \\(A\\) is common (\\(p &gt; 0.5\\)), then the curve is higher than the dotted line: there is a disproportionately higher probability of adopting \\(A\\). When \\(A\\) is uncommon (\\(p &lt; 0.5\\)), then the curve is lower than the dotted line: there is a disproportionately lower probability of adopting \\(A\\). "]
]
