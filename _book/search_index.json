[
["biased-transmission-indirect-bias-demonstrator.html", "4 Biased transmission (indirect bias: demonstrator) 4.1 A simple demonstrator bias 4.2 Predicting the ‘winning’ trait 4.3 Summary of the model 4.4 Analytical appendix 4.5 Further readings", " 4 Biased transmission (indirect bias: demonstrator) In the two previous chapters, we started to examine biased transmission, both based on the characteristics of the traits (or direct bias) and on the characteristics of the population. The latter can result from taking into account the frequency of a trait (as we did for conformity) or from taking into account specific features of the demonstrators, which we will look at in this chapter (demonstrator bias is also called ‘model bias’ in cultural evolution). Whereas the simulations we realised previously are fairly standard, indirect demonstrator-based biased transmission can be implemented in several different ways. Demonstrator biases can result when individuals decide whether to copy or not by taking into account any features of the demonstrators, as long as it is not directly tied to the traits. The most studied demonstrator bias is prestige bias, where individuals are more likely to copy from demonstrators that are considered more ‘prestigious’, for example because other individuals show deference to them. Alternatively, individuals can copy the demonstrators that are more successful, independently from how others judge them, or they can copy individuals that are more similar to them, or older (or younger), and so on. The crux is that the decision is not directly linked to the cultural trait itself. 4.1 A simple demonstrator bias To implement a simple version of demonstrator-biased cultural transmission, we first need to assume that there are some intrinsic differences in the population. Up until now, our populations were described only by the traits they possessed. We now want that individuals have a feature by which some of them can be distinguished from others, and, as a consequence, being more liked to be copied. We can call this feature ‘status’. An individual’s status is a binary variable that could stand for whether they are prestigious or not, old (young) or not (assuming that the time frame of the simulations is sufficiently short) or any other features that do not change, and that other individuals can use to decide whether to copy from them or not. We can have a parameter \\(p_s\\) that determines the probability an individual have an high or a low status. library(tidyverse) set.seed(111) N &lt;- 100 p_0 &lt;- 0.5 p_s &lt;- 0.05 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) We can inspect the tibble by typing its name in the R console population ## # A tibble: 100 x 2 ## trait status ## &lt;chr&gt; &lt;chr&gt; ## 1 A low ## 2 A low ## 3 B low ## 4 A low ## 5 B low ## 6 B low ## 7 B low ## 8 A low ## 9 B low ## 10 B high ## # … with 90 more rows With \\(p_s=0.05\\) around 5 individuals in a population of 100 will have high status. In this specific case, one of them is individual 10, so it will be one of the individuals that will be likely to be copied more from. How should the status used to decide whether to copy or not? Again, there are various possibilities. An intuitive way is to assume that the probabilities to pick high-status and low-status individuals as demonstrators are different. So far, when using the function sample() to select demonstrators, we did not include any specific probability, so that each individual of the previous generation had the same likelihood to be selected. However, we can pass to the function a vector of probabilities to weight the choice. We can assume that the probability to select high status individuals as demonstrators is always equal to 1, but the probability to select low-status individuals is encoded by a further parameter, \\(p_\\text{low}\\): when \\(p_\\text{low}=1\\), the simulations correspond to unbiased transmission, as everybody has the same probability to be chosen, while with \\(p_\\text{low}=0\\), there is a strict model bias, where only high-status individuals can be selected as demonstrators. To implement this, we first store in p_demonstrator the probabilities to be copied for each member of the population: p_low &lt;- 0.01 p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low After that, we sample the traits in the population using these probabilities. Notice the instruction if(sum(p_demonstrator) &gt; 0): this is necessary in case there are not high-status individuals (for example with \\(p_s\\simeq0\\)) and the probability to select demonstrators from low-status one is equal to 0. In this case, the total probability would be also equal to 0, and it would generate an error when the function is run. With this instruction, instead, no copying happens, which is what we would expect in this situation. if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } As usual, we can wrap everything in a function. biased_transmission_demonstrator &lt;- function(N, p_0, p_s, p_low, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } We can now test our simulation, assuming a very low, but not null, probability to select low-status individuals as demonstrators (remember we are using the habitual plot_multiple_runs() function to plot the results of the simulations). data_model &lt;- biased_transmission_demonstrator(N = 100, p_s = 0.05, p_low=0.0001, p_0 = 0.5, t_max = 50, r_max = 5) plot_multiple_runs(data_model) The results are similar to what we saw in the previous chapter for conformity: one of the two traits quickly reaches fixation. In the case of conformity, however, the trait reaching fixation was the one that happened to have a slightly higher frequency at the beginning, because of the random initialisation. With a demonstrator bias, this is not the case. From this perspective, an indirect, demonstrator-based, bias is more similar to unbiased transmission. If you remember from the [first chapter][Unbiased transmission], simulations with unbiased transmission also ended up with one trait reaching fixation with small populations (\\(N=100\\)), but with bigger ones (\\(N=10000\\)) the frequencies of the two traits remained around \\(p=0.5\\). What does it happen with a demonstrator bias? data_model &lt;- biased_transmission_demonstrator(N = 10000, p_s = 0.005, p_low=0.0001, p_0 = 0.5, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Even with \\(N=10000\\), if the number of high-status individuals is sufficiently low, as in this case (\\(p_s=0.005\\) means that, on average, 50 individuals are high-status in each run), traits reach fixation. By reducing the pool of demonstrators, demonstrator-based bias makes drift more important for the overall dynamics. You can experiment with different values of \\(p_s\\) and \\(p_\\text{low}\\). How big can be the pool of high-status demonstrators before the dynamics become indistinguishable from unbiased transmission? 4.2 Predicting the ‘winning’ trait With conformity, as just mentioned, the trait that reaches fixation is the one starting in majority, while with unbiased copying it can not be predicted at the beginning of the simulation. With a demonstrator bias, a reasonable guess would be that the ‘winning’ trait is the one that is, at the beginning, most common among the high-status individuals. Can we check this intuition with our model? Currently the output we obtain from the simulations is not suitable to this purpose. On the one hand, we do not have a piece of information that we need, i.e. the proportion of high-status individuals having one of the two traits when the population is initialised. On the other, we have much information that we do not need, such as the frequency of the two traits at each time step (we just need to know which traits reach fixation). we can rewrite the function and only changing the output tibble to suit our needs. biased_transmission_demonstrator_2 &lt;- function(N, p_0, p_s, p_low, t_max, r_max) { output &lt;- tibble(status_A = rep(NA, r_max), p = rep(NA, r_max)) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) output[r, ]$status_A &lt;- sum(population$status == &quot;high&quot; &amp; population$trait == &quot;A&quot;) / sum(population$status == &quot;high&quot;) for (t in 2:t_max) { p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } } output[r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N } output # export data from function } Let’s run the new function, for 50 runs (change \\(r_\\text{max}=50\\)) so to have more data points, and inspect the output. data_model &lt;- biased_transmission_demonstrator_2(N = 100, p_s = 0.05, p_low=0.0001, p_0 = 0.5, t_max = 50, r_max = 50) data_model ## # A tibble: 50 x 2 ## status_A p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.75 1 ## 2 0.333 0 ## 3 0.4 0 ## 4 0 0 ## 5 0.333 0 ## 6 0.5 0 ## 7 0.5 0 ## 8 0 0 ## 9 0.333 0 ## 10 0.286 0 ## # … with 40 more rows Each line of the output is a run of the simulation. In the first run, for example, 75% of high-status individuals had the trait \\(A\\) at the beginning, and the frequency of the trait \\(A\\) at the end of the simulation was 1, meaning it reached fixation. Generally, from a cursory inspection to the output, it seems our guess was correct. But let’s visualise all the data. We want to know how the initial proportion of high-status individuals is related to the two possible outcomes (trait \\(A\\) reaches fixation and trait \\(B\\) reaches fixation). A convenient way is to use a boxplot. In the code below, we first eliminate the runs where the traits did not reach fixation (in case they exist) using the new function filter(), and, for clarity, we assign the trait name (\\(A\\) or \\(B\\)) to each run according to which trait reached fixation. We can then plot our output (in this case, we do not write a function on purpose). The main novelties in the code are the new ggplot ‘geoms’ geom_boxplot() and geom_jitter(). Whereas boxplots are useful to detect aggregate information on our simulations, geom_jitter() plots also the single data points, so we can have a better idea on how the proportions of high-status individuals are distributed in the various runs. We could have done this with our usual geom_point(), but geom_jitter() scatters randomly (at a distance specified by the parameter width) the points in the plot. This allows to avoid the overlapping of individual data points (known as overplotting). data_model &lt;- filter(data_model, p == 1 | p == 0) data_model[data_model$p==1, ]$p &lt;- &quot;A&quot; data_model[data_model$p==0, ]$p &lt;- &quot;B&quot; ggplot(data = data_model, aes(x = p, y = status_A, fill = p)) + geom_boxplot() + geom_jitter(width = 0.05) + labs(y = &quot;proportion of high-status individuals with trait A&quot;, x = &quot;winning trait&quot;) + ylim(c(0,1)) + theme_bw() + theme(legend.position = &quot;none&quot;) ## Warning: Removed 4 rows containing missing values (geom_point). The plot shows that when the trait \\(A\\) reaches fixation there are in general more high-status individuals with trait \\(A\\) at the beginning, and vice versa for \\(B\\), confirming our intuition. However, this is far from being a safe bet. Runs with only a quarter of high-status individuals with \\(A\\) ended up with all \\(A\\)s in the population and, conversely, runs with \\(80%\\) of high-status individuals with \\(A\\) ended up with the fixation of \\(B\\). With bigger populations, it is even worst. data_model &lt;- biased_transmission_demonstrator_2(N = 10000, p_s = 0.005, p_low=0.0001, p_0 = 0.5, t_max = 200, r_max = 50) data_model &lt;- filter(data_model, p == 1 | p == 0) data_model[data_model$p==1, ]$p &lt;- &quot;A&quot; data_model[data_model$p==0, ]$p &lt;- &quot;B&quot; ggplot(data = data_model, aes(x = p, y = status_A, fill = p)) + geom_boxplot() + geom_jitter(width = 0.05) + labs(y = &quot;proportion of high-status individuals with trait A&quot;, x = &quot;winning trait&quot;) + ylim(c(0,1)) + theme_bw() + theme(legend.position = &quot;none&quot;) With \\(N=10000\\) and around 50 high-status individuals, the traits are more equally distributed among ‘influential’ demonstrators at the beginning, and there is hardly any difference in the two outcomes. 4.3 Summary of the model In this chapter we modeled an example of indirect, demonstrator-based, biased transmission. We assumed that a fraction of individuals in the population was ‘high-status’ and thus more likely to be selected as demonstrators. The results show that in this situation a trait is likely to become predominant even when populations are large. This is due to the fact that a demonstrator bias effectively works reducing the pool of demonstrators and accelerating convergence. We also saw that the possibility of predicting which trait will become predominant depends on the number of high-status demonstrators. When there are few of them is likely that the trait that they posses in majority will go to fixation, but when their number increase, it is more difficult to make prediction. We also saw how it is important to modify the output of a model depending on the question we are interested in, and we used for the first time ggplot to produce boxplot, a convenient way of displaying the distribution of data along different groups. 4.4 Analytical appendix IS THERE ANYHTIHG WE CAN DO HERE? 4.5 Further readings Test Mesoudi (2009) Henrich Joseph, Chudek Maciej, and Boyd Robert (2015) Boyd and Richerson (1985) Jiménez and Mesoudi (2019) References "]
]
