[
["index.html", "Individual-based models of cultural evolution A step-by-step guide using R Introduction", " Individual-based models of cultural evolution A step-by-step guide using R Alberto Acerbi Alex Mesoudi Marco Smolla 2019-10-21 Introduction TO DO "],
["unbiased-transmission.html", "1 Unbiased transmission 1.1 Initialising the simulation 1.2 Execute generation turn-over many times 1.3 Plotting the model results 1.4 Write a function to wrap the model code 1.5 Run several independent simulations and plot their results 1.6 Varying initial conditions 1.7 Summary of the model 1.8 Analytical appendix 1.9 Further reading", " 1 Unbiased transmission We start by simulating a simple case of unbiased cultural transmission. We will detail each step of the simulation and explain the code line-by-line. In the following chapters, we will reuse most of this initial model, building up the complexity of our simulations. 1.1 Initialising the simulation Here we will simulate a case where \\(N\\) individuals each possess one of two mutually exclusive cultural traits. These alternative traits are denoted \\(A\\) and \\(B\\). For example, \\(A\\) might be eating a vegetarian diet, and \\(B\\) might be eating a non-vegetarian diet. In reality, traits are seldom clear-cut (e.g. what about pescatarians?), but models are designed to cut away all the complexity to give tractable answers to simplified situations. Our model has non-overlapping generations. In each generation, all \\(N\\) individuals are replaced with \\(N\\) new individuals. Again, this is unlike any real biological group, but provides a simple way of simulating change over time. Generations here could correspond to biological generations, but could equally be ‘cultural generations’ (or learning episodes), which might be much shorter. Each new individual of each new generation picks a member of the previous generation at random and copies their cultural trait. This is known as unbiased oblique cultural transmission. ‘Unbiased’ refers to the fact that traits are copied entirely at random. The term ‘oblique’ means that members of one generation learn from those of the previous, non-overlapping, generation. This is different from, for example, horizontal cultural transmission, where individuals copy members of the same generation, and vertical cultural transmission, where offspring copy their biological parents. If we assume that the two cultural traits are transmitted in an unbiased way, what does that mean for the average trait frequency in the population? To answer this question, we must track the proportion of individuals who possess trait \\(A\\) over successive generations. We will call this proportion \\(p\\). We could also track the proportion who possess trait \\(B\\), but this will always be \\(1 - p\\) given that the two traits are mutually exclusive. For example, if \\(70\\%\\) of the population have trait \\(A\\) \\((p=0.7)\\), then the remaining \\(30\\%\\) must have trait \\(B\\) (i.e. \\(1-p=1-0.7=0.3\\)). The output of the model will be a plot showing \\(p\\) over all generations up to the last generation. Generations (or time steps) are denoted by \\(t\\), where generation one is \\(t=1\\), generation two is \\(t=2\\), up to the last generation \\(t=t_{\\text{max}}\\). First, we need to specify the fixed parameters of the model. These are quantities that we decide on at the start, and do not change during the simulation. In this model these are \\(N\\) (the number of individuals) and \\(t_{\\text{max}}\\) (the number of generations). Let’s start with \\(N=100\\) and \\(t_{\\text{max}}=200\\): N &lt;- 100 t_max &lt;- 200 Now we need to create our individuals. The only information we need to keep about our individuals is their cultural trait (\\(A\\) or \\(B\\)). We’ll make population the data structure containing the individuals. The type of data structure we have chosen here is a tibble. This is a more user-friendly version of a dataframe. Initially, we’ll give each individual either an \\(A\\) or \\(B\\) at random, using the sample() command. This can be seen in the code chunk below. The sample() command takes three arguments (i.e. inputs or options). The first argument lists the elements to pick at random, in our case, the traits \\(A\\) and \\(B\\). The second argument gives the number of times to pick, in our case \\(N\\) times, once for each individual. The final argument says to replace or reuse the elements specified in the first argument after they’ve been picked (otherwise there would only be one copy of \\(A\\) and one copy of \\(B\\), so we could only give two individuals traits before running out). Within the tibble command, the word \\(trait\\) denotes the name of the variable within the tibble that contains the random \\(A\\)s and \\(B\\)s, and the whole tibble is assigned the name population. There are two lines before sample() in the chunk below. First, we need to call the tidyverse library. We will use this throughout the chapter. Here, it allows us to create a tibble using the tibble command. Second, we use the function set.seed() to set the seed of the random number generator. This ensures that the results shown in the book are the same you obtain in your session. The number \\(111\\) has no special meaning. You can replace it with any other number and you will see how the traits of the individuals will be created differently. library(tidyverse) set.seed(111) population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) We can see the cultural traits of our population by simply entering its name in the R console: population ## # A tibble: 100 x 1 ## trait ## &lt;chr&gt; ## 1 B ## 2 B ## 3 A ## 4 B ## 5 A ## 6 A ## 7 A ## 8 B ## 9 A ## 10 A ## # … with 90 more rows As expected, there is a single column called \\(trait\\) containing \\(A\\)s and \\(B\\)s. The type of the column, in this case ‘chr’ (i.e. character), is reported below the name. A specific individual’s trait can be retrieved using the square bracket notation in R. For example, individual 4’s trait can be retrieved by typing: population$trait[4] ## [1] &quot;B&quot; This should match the fourth row in the table above. We also need a tibble to record the output of our simulation, that is, to track the trait frequency \\(p\\) in each generation. This will have two columns with \\(t_{\\text{max}}\\) rows, one row for each generation. The first column is simply a counter of the generations, from 1 to \\(t_{\\text{max}}\\). This will be useful to plot the output later. The other column contains the values of \\(p\\) for each generation. At this stage we don’t know what \\(p\\) will be in each generation, so for now let’s fill the output tibble with lots of NAs, which is R’s symbol for Not Available, or missing value. We can use the rep() (repeat) command to repeat NA \\(t_{\\text{max}}\\) times. We’re using NA rather than, say, zero, because zero could be misinterpreted as \\(p = 0\\), which would mean that all individuals have trait \\(B\\). This would be misleading, because at the moment we haven’t yet calculated \\(p\\), so it’s nonexistent, rather than zero. output &lt;- tibble(generation = 1:t_max, p = rep(NA, t_max)) We can, however, fill in the first value of \\(p\\) for our already-created first generation of individuals, held in population. The command below sums the number of \\(A\\)s in population and divides by \\(N\\) to get a proportion out of 1 rather than an absolute number. It then puts this proportion in the first slot of \\(p\\) in output, the one for the first generation, \\(t=1\\). We can again write the name of the tibble, output, to see that it worked. output$p[1] &lt;- sum(population$trait == &quot;A&quot;) / N output ## # A tibble: 200 x 2 ## generation p ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.51 ## 2 2 NA ## 3 3 NA ## 4 4 NA ## 5 5 NA ## 6 6 NA ## 7 7 NA ## 8 8 NA ## 9 9 NA ## 10 10 NA ## # … with 190 more rows This first value of \\(p\\) is 0.51, meaning that 51 individuals have trait \\(A\\), and 49 have trait \\(B\\). Even though sample() gives equal probability of picking each trait, this does not necessarily mean that we will get exactly 50 \\(A\\)s and 50 \\(B\\)s. This happens with simulations and finite population sizes: they are probabilistic (or stochastic), not deterministic. Analogously, flipping a coin 100 times will not always give exactly 50 heads and 50 tails. Sometimes we will get 51 heads, sometimes 49, etc. To see this in our simulation, re-run the above code with different values of the random number seed. 1.2 Execute generation turn-over many times Now that we have built the population, we can simulate what individuals do in each generation. We iterate these actions over \\(t_{\\text{max}}\\) generations. In each generation, we need to: copy the current individuals to a separate tibble called previous_population to use as demonstrators for the new individuals; this allows us to implement oblique transmission with its non-overlapping generations, rather than mixing up the generations create a new generation of individuals, each of whose trait is picked at random from the previous_population tibble calculate \\(p\\) for this new generation and store it in the appropriate slot in output To iterate, we’ll use a for-loop, using \\(t\\) to track the generation. We’ve already done generation 1 so we’ll start at generation 2. The random picking of models is done with sample() again, but this time picking from the traits held in previous_population. Note that we have added comments briefly explaining what each line does. This is perhaps superfluous when the code is this simple, but it’s always good practice. Code often gets cut-and-pasted into other places and loses its context. Explaining what each line does lets other people - and a future, forgetful you - know what’s going on. for (t in 2:t_max) { previous_population &lt;- population # copy the population tibble to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals output$p[t] &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into the output slot for this generation t } Now we should have 200 values of \\(p\\) stored in output, one for each generation. You can list them by typing output, but more effective is to plot them. 1.3 Plotting the model results We use ggplot() to plot our data. The syntax of ggplot may be slightly obscure at the beginning, but it forces us to have a clear picture of the data before plotting. In the first line in the code below, we are telling ggplot that the data we want to plot are in the tibble output. Then, with the command aes() we declare the ‘aesthetics’ of the plot, that is, how we want our data mapped in our plot. In this case, we want the values of \\(p\\) on the y-axis, and the values of \\(generation\\) on the x-axis (this is why we created earlier, in the tibble output, a column to keep the count of generations). We then use geom_line(). In ggplot, ‘geoms’ describe what kind of visual representation should be plotted: lines, bars, boxes and so on. This visual representation is independent from the mapping that we declared before with aes(). The same data, with the same mapping, can be visually represented in many different ways. In this case, we are asking ggplot to represent the data as a line. You can change geom_line() in the code below to geom_point(), and see what happens (other geoms have less obvious effects, and we will see some of them in the later chapters). The other commands are mainly to make the plot look nicer. We want the y-axis to span all the possible values of \\(p\\), from 0 to 1, and we use a particular ‘theme’ for our plot, in this case a standard theme with white background. With the command labs() we give a more informative label to the y-axis (ggplot automatically labels the axis with the name of the tibble columns that are plotted: this is good for \\(generation\\), but less so for \\(p\\)). ggplot(data = output, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) The proportion of individuals with trait \\(A\\) starts off hovering around 0.5, and after around 120 generations drops to 0. Unbiased transmission, or random copying, is by definition random, so different runs of this simulation will generate different plots. If you rerun all the code, trying different seeds of the random number generator, you will get something different. In all likelihood, \\(p\\) might go to 0 or 1 at some point. At \\(p = 0\\) there are no \\(A\\)s and every individual possesses \\(B\\). At \\(p = 1\\) there are no \\(B\\)s and every individual possesses \\(A\\). This is a typical feature of cultural drift, analogous to genetic drift: in small populations, with no selection or other directional processes operating, traits can be lost purely by chance after some generations. 1.4 Write a function to wrap the model code Ideally we would like to repeat the simulation to explore this idea in more detail, perhaps changing some of the parameters. For example, if we increase \\(N\\), are we more or less likely to lose one of the traits? As noted above, individual based models like this one are probabilistic or stochastic, thus it is essential to run simulations many times to understand what happens. With our code scattered about in chunks, it is hard to quickly repeat the simulation. Instead we can wrap it all up in a function: unbiased_transmission_1 &lt;- function(N, t_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) output &lt;- tibble(generation = 1:t_max, p = rep(NA, t_max)) output$p[1] &lt;- sum(population$trait == &quot;A&quot;) / N for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output$p[t] &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t } output } This is just all of the code snippets that we already ran above, but all within a function with parameters \\(N\\) and \\(t_{\\text{max}}\\) as arguments to the function. In addition, unbiased_transmission_1() ends with the line output. This means that this tibble will be exported from the function when it is run. This is useful for storing data from simulations wrapped in functions, otherwise that data is lost after the function is executed. Nothing will happen when you run the above code, because all you’ve done is define the function, not actually run it. The point is that we can now call the function in one go, easily changing the values of \\(N\\) and \\(t_{\\text{max}}\\). Let’s try first with the same values of \\(N\\) and \\(t_{\\text{max}}\\) as before, and save the output from the simulation into data_model, as a record of what happened. data_model &lt;- unbiased_transmission_1(N = 100, t_max = 200) We also need to create another function to plot the data, so we do not need to rewrite all the plotting instructions each time. Whereas this may seem impractical now, it is convenient to separate the function that runs the simulation and the function that plots the data for various reasons. With more complicated models, we do not want to rerun a simulation just because we want to change some detail in the plot. It also makes conceptual sense to keep separate the raw output of the model from the various ways we can visualise it, or the further analysis we want to perform on it. As above, the code is identical to what we already wrote: plot_single_run &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) } At this point, we can visualise the results: plot_single_run(data_model) As anticipated, the plot is different from the simulation we ran before, even though the code is exactly the same. This is due to the stochastic nature of the simulation. Assuming you haven’t changed the set.seed value earlier, this time trait \\(A\\) reaches a frequency of 1, meaning that all individuals have it, and trait \\(B\\) has disappeared. Now let’s try changing the parameters. We can call the simulation and the plotting functions together. The code below reruns and plots the simulation with a much larger \\(N\\). data_model &lt;- unbiased_transmission_1(N = 10000, t_max = 200) plot_single_run(data_model) You should see much less fluctuation. Rarely in a population of \\(N = 10000\\) will either trait go to fixation. Try re-running the previous code chunk to explore the effect of \\(N\\) on long-term dynamics. 1.5 Run several independent simulations and plot their results Wrapping a simulation in a function like this is good because we can easily re-run it with just a single command. However, it’s a bit laborious to manually re-run it. Say we wanted to re-run the simulation 10 times with the same parameter values to see how many times \\(A\\) goes to fixation, and how many times \\(B\\) goes to fixation. Currently, we’d have to manually run the unbiased_transmission_1() function 10 times and record somewhere else what happened in each run. It would be better to automatically re-run the simulation several times and plot each run as a separate line on the same plot. We could also add a line showing the mean value of \\(p\\) across all runs. Let’s use a new parameter \\(r_{\\text{max}}\\) to specify the number of independent runs, and use another for-loop to cycle over the \\(r_{\\text{max}}\\) runs. Let’s rewrite the unbiased_transmission_1() function to handle multiple runs. We will call the new function unbiased_transmission_2(). unbiased_transmission_2 &lt;- function(N, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { # for each run population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } There are a few changes here. First, we need a different output tibble, because we need to store data for all the runs. For that, we initialise the same \\(generation\\) and \\(p\\) columns as before, but with space for all the runs. \\(generation\\) is now built by repeating the count of each generation \\(r_{\\text{max}}\\) times, and \\(p\\) is NA repeated for all generations, for all runs. We also need a new column called \\(run\\) that keeps track of which run the data in the other two columns belongs to. Note that the definition of \\(run\\) is preceded by as.factor(). This specifies the type of data to put in the \\(run\\) column. We want \\(run\\) to be a ‘factor’ or categorical variable so that, even if runs are labeled with numbers (1, 2, 3…), this should not be misinterpreted as a continuous, real number: there is no sense in which run 2 is twice as ‘runny’ as run 1, or run 3 half as ‘runny’ as run 6. Runs could equally have been labelled using letters, or any other arbitrary scheme. While omitting as.factor() does not make any difference when running the simulation, it would create problems when plotting the data because ggplot would treat runs as continuous real numbers rather than discrete categories (you can see this yourself by modifying the definition of output in the previous code chunk). This is a good example of how it is important to have a clear understanding of your data before trying to plot or analyse them. Going back to the function, we then set up an \\(r\\) loop, which executes once for each run. The code within this loop is mostly the same as before, except that we now use the [output$generation == t &amp; output$run == r, ] notation to put \\(p\\) into the right place in output. The plotting function is also changed to handle multiple runs: plot_multiple_runs &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line(aes(colour = run)) + stat_summary(fun.y = mean, geom = &quot;line&quot;, size = 1) + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) } To understand how the above code works, we need to explain the general functioning of ggplot. As explained above, aes() specifies the ‘aesthetics’, or how the data are mapped in the plot. This is independent from the possible visual representations of this mapping, or ‘geoms’. If we declare specific aesthetics when we call ggplot(), these aesthetics will be applied to all geoms we call afterwards. Alternatively, we can specify the aesthetics in the geom itself. For example this: ggplot(data = output, aes(y = p, x = generation)) + geom_line() is equivalent to this: ggplot(data = output) + geom_line(aes(y = p, x = generation)) We can use this property to make more complex plots. The plot created in plot_multiple_runs has a first geom, geom_line(). This inherits the aesthetics specified in the initial call to ggplot() but also has a new mapping specific to geom_line(), colour = run. This tells ggplot to plot each run line with a different colour. The following command, stat_summary(), calculates the mean of all runs. However, this only inherits the mapping specified in the initial ggplot() call. If in the aesthetic of stat_summary() we had also specified colour = run, it would separate the data by run, and it would calculate the mean of each run. This, though, is just the lines we have already plotted with the geom_line() command. For this reason, we did not put colour = run in the ggplot() call, only in geom_line(). As always, there are various ways to obtain the same result. This code: ggplot(data = output) + geom_line(aes(y = p, x = generation, colour = run)) + stat_summary(aes(y = p, x = generation), fun.y = mean, geom = &quot;line&quot;, size = 1) is equivalent to the code we wrapped in the function above. However, the original code is clearer, as it distinguishes the global mapping, and the mappings specific to each visual representation. stat_summary() is a generic ggplot function which can be used to plot different statistics to summarise our data. In this case, we want to calculate the mean on the data mapped in \\(y\\), we want to plot them with a line, and we want this line to be thicker than the lines for the single runs. The default line size for geom_line is 0.5, so size = 1 doubles the thickness. Let’s now run the function and plot the results for five runs with the same parameters we used at the beginning (\\(N=100\\) and \\(t_{\\text{max}}=200\\)): data_model &lt;- unbiased_transmission_2(N = 100, t_max = 200, r_max = 5) plot_multiple_runs(data_model) You should be able to see five independent runs of our simulation shown as regular thin lines, along with a thicker line showing the mean of these lines. Some runs have probably gone to 0 or 1, and the mean should be somewhere in between. The data is stored in data_model, which we can inspect by writing its name. data_model ## # A tibble: 1,000 x 3 ## generation p run ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 0.53 1 ## 2 2 0.61 1 ## 3 3 0.55 1 ## 4 4 0.47 1 ## 5 5 0.47 1 ## 6 6 0.37 1 ## 7 7 0.34 1 ## 8 8 0.4 1 ## 9 9 0.44 1 ## 10 10 0.52 1 ## # … with 990 more rows Now let’s run the unbiased_transmission_2() model with \\(N = 10000\\), to compare with \\(N = 100\\). data_model &lt;- unbiased_transmission_2(N = 10000, t_max = 200, r_max = 5) plot_multiple_runs(data_model) The mean line should be almost exactly at \\(p = 0.5\\) now, with the five independent runs fairly close to it. 1.6 Varying initial conditions Let’s add one final modification. So far the starting frequencies of \\(A\\) and \\(B\\) have been the same, roughly 0.5 each. But what if we were to start at different initial frequencies of \\(A\\) and \\(B\\)? Say, \\(p = 0.2\\) or \\(p = 0.9\\)? Would unbiased transmission keep \\(p\\) at these initial values, or would it go to \\(p = 0.5\\) as we have found so far? To find out, we can add another parameter, \\(p_0\\), which specifies the initial probability of an individual having an \\(A\\) rather than a \\(B\\) in the first generation. Previously this was always \\(p_0 = 0.5\\), but in the new function below we add it to the sample() function to weight the initial allocation of traits in \\(t = 1\\). unbiased_transmission_3 &lt;- function(N, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } unbiased_transmission_3() is almost identical to the previous function. The only changes are the addition of \\(p_0\\) as an argument to the function, and the \\(prob\\) argument in the sample() command. The \\(prob\\) argument gives the probability of picking each option, in our case \\(A\\) and \\(B\\), in the first generation. The probability of \\(A\\) is now \\(p_0\\), and the probability of \\(B\\) is now \\(1 - p_0\\). We can use the same plotting function as before to visualise the result. Let’s see what happens with a different value of \\(p_0\\), for example \\(p_0 = 0.2\\). data_model &lt;- unbiased_transmission_3(N = 10000, p_0 = 0.2, t_max = 200, r_max = 5) plot_multiple_runs(data_model) With \\(p_0 = 0.2\\), trait frequencies stay at \\(p = 0.2\\). Unbiased transmission is truly non-directional: it maintains trait frequencies at whatever they were in the previous generation, barring random fluctuations caused by small population sizes. 1.7 Summary of the model Even this extremely simple model provides some valuable insights. First, unbiased transmission does not in itself change trait frequencies. As long as populations are large, trait frequencies remain the same. Second, the smaller the population size, the more likely traits are to be lost by chance. This is a basic insight from population genetics, known there as genetic drift, but it can also be applied to cultural evolution. Many studies have tested (and some supported) the idea that population size and other demographic factors can shape cultural diversity. Furthermore, generating expectations about cultural change under simple assumptions like random cultural drift can be useful for detecting non-random patterns like selection. If we don’t have a baseline, we won’t know selection or other directional processes when we see them. We also have introduced several programming techniques that will be useful in later simulations. We have seen how to use tibbles to hold characteristics of individuals and the outputs of simulations, how to use loops to cycle through generations and simulation runs, how to use sample() to pick randomly from sets of elements, how to wrap simulations in functions to easily re-run them with different parameter values, and how to use ggplot() to plot the results of simulations. 1.8 Analytical appendix If \\(p\\) is the frequency of \\(A\\) in one generation, we are interested in calculating \\(p&#39;\\), the frequency of \\(A\\) in the next generation under the assumption of unbiased transmission. Each new individual in the next generation picks a demonstrator at random from among the previous generation. The demonstrator will have \\(A\\) with probability \\(p\\). The frequency of \\(A\\) in the next generation, then, is simply the frequency of \\(A\\) in the previous generation: \\[p&#39; = p \\hspace{30 mm}(1.1)\\] Equation 1.1 simply says that under unbiased transmission there is no change in \\(p\\) over time. If, as we assumed above, the initial value of \\(p\\) in a particular population is \\(p_0\\), then the equilibrium value of \\(p\\), \\(p^*\\), at which there is no change in \\(p\\) over time, is just \\(p_0\\). We can plot this recursion, to recreate the final simulation plot above: p_0 &lt;- 0.2 t_max &lt;- 200 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Here, we use a for loop to cycle through each generation, each time updating \\(p\\) according to the recursion equation above. Remember, there is no \\(N\\) here because the recursion is deterministic and assumes an infinite population size; hence there is no stochasticity due to finite population sizes. There is also no need to have multiple runs as each run is identical, hence no \\(r_{max}\\). Don’t worry, it gets more complicated than this in later chapters. The key point here is that analytical (or deterministic) models assume infinite populations and no stochasticity. Simulations with very large populations should give the same results as analytical models. Basically, the closer we can get in stochastic models to the assumption of infinite populations, the closer the match to infinite-population deterministic models. Deterministic models give the ideal case; stochastic models permit more realistic dynamics based on finite populations. More generally, creating deterministic recursion-based models can be a good way of verifying simulation models, and vice versa: if the same dynamics occur in both individuals-based and recursion-based models, then we can be more confident that those dynamics are genuine and not the result of a programming error or mathematical mistake. 1.9 Further reading Cavalli-Sforza and Feldman (1981) explored how cultural drift affects cultural evolution, which was extended by Neiman (1995) in an archaeological context. Bentley, Hahn, and Shennan (2004) present models of unbiased transmission with respect to several cultural datasets. Lansing and Cox (2011) and commentaries explore the underlying assumptions of applying random drift to cultural evolution. References "],
["unbiased-and-biased-mutation.html", "2 Unbiased and biased mutation 2.1 Unbiased mutation 2.2 Biased mutation 2.3 Summary of the model 2.4 Analytical appendix 2.5 Further reading", " 2 Unbiased and biased mutation Evolution doesn’t work without a source of variation that introduces new variation upon which selection, drift and other processes can act. In genetic evolution, mutation is almost always blind with respect to function. Beneficial genetic mutations are no more likely to arise when they are needed than when they are not needed - in fact most genetic mutations are neutral or detrimental to an organism. Cultural evolution is more interesting, in that novel variation may sometimes be directed to solve specific problems, or systematically biased due to features of our cognition. In the models below we’ll simulate both unbiased and biased mutation. 2.1 Unbiased mutation First we will simulate unbiased mutation in the same basic model as used in the previous chapter. We’ll remove unbiased transmission to see the effect of unbiased mutation alone. As in the previous model, we assume \\(N\\) individuals each of whom possesses one of two non-overlapping cultural traits, denoted \\(A\\) and \\(B\\). In each generation, from \\(t = 1\\) to \\(t = t_{\\text{max}}\\), the \\(N\\) individuals are replaced with \\(N\\) new individuals. Instead of random copying, each individual now gives rise to a new individual with exactly the same cultural trait as them. (Another way of looking at this is in terms of timesteps, such as years: the same \\(N\\) individual live for \\(t_{\\text{max}}\\) years, and keep their cultural trait from one year to the next.) At each generation, however, there is a probability \\(\\mu\\) that each individual mutates from their current trait to the other trait (the Greek letter Mu is the standard notation for the mutation rate in genetic evolution, and it has an analogous function here). For example, vegetarian individuals can decide to eat animal products, and vice versa. Remember, this is not copied from other individuals, as in the previous model, but can be thought as an individual decision. Another way to see this is that the probability of changing trait applies to each individual independently; whether an individual mutates has no bearing on whether or how many other individuals have mutated. On average, this means that \\(\\mu N\\) individuals mutate each generation. Like in the previous model, we are interested in tracking the proportion \\(p\\) of agents with trait \\(A\\) over time. We’ll wrap this in a function called unbiased_mutation(), using much of the same code as unbiased_transmission_3(). As before, we need to call the tidyverse library, and set a seed for the random number genrator, so the results will be exactly the same each time we rerun the code. Of course, if you want to see the stochasticity inherent in the simulation, you can remove the set.seed command, or set it to a different number. library(tidyverse) set.seed(111) unbiased_mutation &lt;- function(N, mu, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mutate &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # determine &#39;mutant&#39; individuals if (nrow(population[mutate &amp; previous_population$trait == &quot;A&quot;, ]) &gt; 0) { # if there are &#39;mutants&#39; from A to B population[mutate &amp; previous_population$trait == &quot;A&quot;, ]$trait &lt;- &quot;B&quot; # then flip them to B } if (nrow(population[mutate &amp; previous_population$trait == &quot;B&quot;, ]) &gt; 0) { # if there are &#39;mutants&#39; from B to A population[mutate &amp; previous_population$trait == &quot;B&quot;, ]$trait &lt;- &quot;A&quot; # then flip them to A } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } The only changes from the previous model are the addition of \\(\\mu\\), the parameter that specifies the probability of mutation, in the function definition and new lines of code within the for loop on \\(t\\) which replace the random copying command with unbiased mutation. Let’s examine these lines to see how they work. The most obvious way of implementing unbiased mutation - which is not done above - would have been to set up another for loop. We would cycle through each individual one by one, each time calculating whether it should mutate or not based on \\(mu\\). This would certainly work, but R is notoriously slow at loops. It’s always preferable in R, where possible, to use ‘vectorised’ code. That’s what is done above in our three added lines, starting from mutate &lt;- sample(). First, we pre-specify the probability of mutating for each individual. For this, we again use the function sample(), picking \\(TRUE\\) (corresponding to being a mutant) or \\(FALSE\\) (not mutating, i.e. keeping the same cultural trait) for \\(N\\) times. The draw, however, is not random: the probability of drawing \\(TRUE\\) is equal to \\(\\mu\\), and the probability of drawing \\(FALSE\\) is \\(1-\\mu\\). You can think about the procedure in this way: each individual in the population flips a biased coin that has \\(\\mu\\) probability to land on, say, heads, and \\(1-\\mu\\) to land on tails. If it lands on heads they change their cultural trait. After that, in the following lines, we actually change the traits for the ‘mutant’ individuals. We need to check whether there are individuals that change their trait, both from \\(A\\) to \\(B\\) and vice versa, using the two if conditionals. If there are no such individuals, then assigning a new value to an empty tibble returns an error. To check, we make sure that the number of rows is greater than 0 (using nrow()&gt;0 within the if). To plot the results, we can use the same function plot_multiple_runs() we wrote in the previous chapter, reproduced here for convenience. Let’s now run and plot the model: data_model &lt;- unbiased_mutation(N = 100, mu = 0.05, p_0 = 0.5, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Unbiased mutation produces random fluctuations over time, and does not alter the overall frequency of \\(A\\), which stays around \\(p = 0.5\\). Because mutations from \\(A\\) to \\(B\\) are as equally likely as \\(B\\) to \\(A\\), there is no overall directional trend. If you remember from the previous chapter, with unbiased transmission, instead, when populations were small (e.g. \\(N=100\\)) generally one of the traits disappeared after a few generations. Here, though, with \\(N=100\\), both traits remain until the end of the simulation. Why this difference? You can think of it in this way: when one trait becomes popular, say the frequency of \\(A\\) is equal to \\(0.8\\), with unbiased transmission it is more likely that individuals of the new generation will pick up \\(A\\) randomly when copying. The few individuals with trait \\(B\\) will have 80% of probability of copying \\(A\\). With unbiased mutation, on the other hand, since \\(\\mu\\) is applied independently to each individual, when \\(A\\) is common then there will be more individuals that will flip to \\(B\\) (specifically, \\(\\mu p N\\) individuals, which in our case is 4) than individuals that will flip to \\(A\\) (equal to \\(\\mu (1-p) N\\) individuals, in our case 1) keeping the traits at similar freuqencies. But what if we were to start at different initial frequencies of \\(A\\) and \\(B\\)? Say, \\(p=0.1\\) and \\(p=0.9\\)? Would \\(A\\) disappear? Would unbiased mutation keep \\(p\\) at these initial values, like we saw unbiased transmission does in Model 1? To find out, let’s change \\(p_0\\), which specifies the initial probability of drawing an \\(A\\) rather than a \\(B\\) in the first generation. data_model &lt;- unbiased_mutation(N = 100, mu = 0.05, p_0 = 0.1, t_max = 200, r_max = 5) plot_multiple_runs(data_model) You should see \\(p\\) go from 0.1 up to 0.5. In fact, whatever the initial starting frequencies of \\(A\\) and \\(B\\), unbiased mutation always leads to \\(p = 0.5\\), for the reason explained above: unbiased mutation always tends to balance the proportion of \\(A\\)s and \\(B\\)s. 2.2 Biased mutation A more interesting case is biased mutation. Let’s assume now that there is a probability \\(\\mu_b\\) that an individual with trait \\(B\\) mutates into \\(A\\), but there is no possibility of trait \\(A\\) mutating into trait \\(B\\). Perhaps trait \\(A\\) is a particularly catchy or memorable version of a story, or an intuitive explanation of a phenomenon, and \\(B\\) is difficult to remember or unintuitive to understand. The function biased_mutation() captures this unidirectional mutation. biased_mutation &lt;- function(N, mu_b, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mutate &lt;- sample(c(TRUE, FALSE), N, prob = c(mu_b, 1 - mu_b), replace = TRUE) # find &#39;mutant&#39; individuals if (nrow(population[mutate &amp; previous_population$trait == &quot;B&quot;, ]) &gt; 0) { population[mutate &amp; previous_population$trait == &quot;B&quot;, ]$trait &lt;- &quot;A&quot; # if individual was B and mutates, flip to A } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } There are just two changes in this code compared to unbiased_mutation(). First, we’ve replaced \\(\\mu\\) with \\(\\mu_b\\) to keep the two parameters distinct and avoid confusion. Second, the line in unbiased_mutation() which caused individuals with \\(A\\) to mutate to \\(B\\) has been deleted. Let’s see what effect this has by running biased_mutation(). We’ll start with the population entirely composed of individuals with \\(B\\), i.e. \\(p_0 = 0\\), to see how quickly and in what manner \\(A\\) spreads via biased mutation. data_model &lt;- biased_mutation(N = 100, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) The plot shows a steep increase that slows and plateaus at \\(p = 1\\) by around generation \\(t = 100\\). There should be a bit of fluctuation in the different runs, but not much. Now let’s try a larger sample size. data_model &lt;- biased_mutation(N = 10000, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) With \\(N = 10000\\) the line should be smooth with little (if any) fluctuation across the runs. But notice that it plateaus at about the same generation, around \\(t = 100\\). Population size has little effect on the rate at which a novel trait spreads via biased mutation. \\(\\mu_b\\), on the other hand, does affect this speed. Let’s double the biased mutation rate to 0.1. data_model &lt;- biased_mutation(N = 10000, mu_b = 0.1, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Now trait \\(A\\) reaches fixation around generation \\(t = 50\\). Play around with \\(N\\) and \\(\\mu_b\\) to confirm that the latter determines the rate of diffusion of trait \\(A\\), and that it takes the same form each time - roughly an ‘r’ shape with an initial steep increase followed by a plateauing at \\(p = 1\\). 2.3 Summary of the model With this simple model we can draw the following insights. Unbiased mutation, which resembles genetic mutation in being non-directional, always leads to an equal mix of the two traits. It introduces and maintains cultural variation in the population. It is interesting to compare unbiased mutation to unbiased transmission from Model 1. While unbiased transmission did not change \\(p\\) over time, unbiased mutation always converges on \\(p^* = 0.5\\), irrespective of the starting frequency. (NB \\(p^* = 0.5\\) assuming there are two traits; more generally, \\(p^* = 1/v\\), where \\(v\\) is the number of traits.) Biased mutation, which is far more common - perhaps even typical - in cultural evolution, shows different dynamics. Novel traits favoured by biased mutation spread in a characteristic fashion - an r-shaped diffusion curve - with a speed characterised by the mutation rate \\(\\mu_b\\). Population size has little effect, whether \\(N = 100\\) or \\(N = 10000\\). Whenever biased mutation is present (\\(\\mu_b &gt; 0\\)), the favoured trait goes to fixation, even if it is not initially present. In terms of programming techniques, the major novelty in Model 2 is the use of sample() to determine which individuals should undergo whatever the fixed probability specifies (in our case, mutation). This could be done with a loop, but vectorising code in the way we did here is much faster in R than loops. 2.4 Analytical appendix If \\(p\\) is the frequency of \\(A\\) in one generation, we are interested in calculating \\(p&#39;\\), the frequency of \\(A\\) in the next generation under the assumption of unbiased mutation. The next generation retains the cultural traits of the previous generation, except that \\(\\mu\\) of them switch to the other trait. There are therefore two sources of \\(A\\) in the next generation: members of the previous generation who had \\(A\\) and didn’t mutate, therefore staying \\(A\\), and members of the previous generation who had \\(B\\) and did mutate, therefore switching to \\(A\\). The frequency of \\(A\\) in the next generation is therefore: \\[p&#39; = p(1-\\mu) + (1-p)\\mu \\hspace{30 mm}(2.1)\\] The first term on the right-hand side of Equation 2.1 represents the first group, the \\((1 - \\mu)\\) proportion of the \\(p\\) \\(A\\)-carriers who didn’t mutate. The second term represents the second group, the \\(\\mu\\) proportion of the \\(1 - p\\) \\(B\\)-carriers who did mutate. To calculate the equilibrium value of \\(p\\), \\(p^*\\), we want to know when \\(p&#39; = p\\), or when the frequency of \\(A\\) in one generation is identical to the frequency of \\(A\\) in the next generation. This can be found by setting \\(p&#39; = p\\) in Equation 2.1, which gives: \\[p = p(1-\\mu) + (1-p)\\mu \\hspace{30 mm}(2.2)\\] Rearranging Equation 2.2 gives: \\[\\mu(1 - 2p) = 0 \\hspace{30 mm}(2.3)\\] The left-hand side of Equation 2.3 equals zero when either \\(\\mu = 0\\), which given our assumption that \\(\\mu &gt; 0\\) cannot be the case, or when \\(1 - 2p = 0\\), which after rearranging gives the single equilibrium \\(p^* = 0.5\\). This matches our simulation results above. As we found in the simulations, this does not depend on \\(\\mu\\) or the starting frequency of \\(p\\). We can also plot the recursion in Equation 2.1 like so: p_0 &lt;- 0 t_max &lt;- 200 mu &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] * (1 - mu) + (1 - pop_analytical$p[i - 1]) * mu } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Again, this should resemble the figure generated by the simulations above, and confirm that \\(p^* = 0.5\\). For biased mutation, assume that only \\(B\\)s are switching to \\(A\\), and with probability \\(\\mu_b\\) instead of \\(\\mu\\). The first term on the right hand side becomes simply \\(p\\), because \\(A\\)s do not switch. The second term remains the same, but with \\(\\mu_b\\). Thus, \\[p&#39; = p + (1-p)\\mu_b \\hspace{30 mm}(2.4)\\] The equilibrium value \\(p^*\\) can be found by again setting \\(p&#39; = p\\) and solving for \\(p\\). Assuming \\(\\mu_b &gt; 0\\), this gives the single equilibrium \\(p^* = 1\\), which again matches the simulation results. We can plot the above recursion like so: p_0 &lt;- 0 t_max &lt;- 200 mu_b &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] + (1 - pop_analytical$p[i - 1]) * mu_b } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Hopefully, this looks identical to the final simulation plot with the same value of \\(\\mu_b\\). Furthermore, we can specify an equation for the change in \\(p\\) from one generation to the next, or \\(\\Delta p\\). We do this by subtracting \\(p\\) from both sides of Equation 2.4, giving: \\[\\Delta p = p&#39; - p = (1-p)\\mu_b \\hspace{30 mm}(2.5)\\] Seeing this helps explain two things. First, the \\(1 - p\\) part explains the r-shape of the curve. It says that the smaller is \\(p\\), the larger \\(\\Delta p\\) will be. This explains why \\(p\\) increases in frequency very quickly at first, when \\(p\\) is near zero, and the increase slows when \\(p\\) gets larger. We have already determined that the increase stops altogether (i.e. \\(\\Delta p\\) = 0) when \\(p = p^* = 1\\). Second, it says that the rate of increase is proportional to \\(\\mu_b\\). This explains our observation in the simulations that larger values of \\(\\mu_b\\) cause \\(p\\) to reach its maximum value faster. 2.5 Further reading Boyd and Richerson (1985) model what they call ‘guided variation’, which is equivalent to biased mutation as modelled in this chapter. Henrich (2001) shows how biased mutation / guided variation generates r-shaped curves similar to thoese generated here. References "],
["biased-transmission-direct-bias.html", "3 Biased transmission: direct bias 3.1 Strentgh of selection 3.2 Summary of the model 3.3 Analytical appendix 3.4 Further reading", " 3 Biased transmission: direct bias So far we have looked at unbiased transmission (Chapter 1) and mutation, both unbiased and biased (Chapter 2). Let’s complete the set by looking at biased transmission. This occurs when one trait is more likely to be copied than another trait. When the choice depends on features of the trait, it is often called ‘direct’ or ‘content’ bias. When the choice depends on features of the demonstrators (the individuals from whom one is copying), it is often called ‘indirect’ or ‘context’ bias. Both are sometimes also called ‘cultural selection’ because one trait is selected to be copied over another trait. In this chapter, we will look at trait-based (direct, content) bias. (As an aside, there is a confusing array of terminology in the field of cultural evolution, as illustrated by the preceding paragraph. That’s why models are so useful. Words and verbal descriptions can be ambiguous. Often the writer doesn’t realise that there are hidden assumptions or unrecognised ambiguities in their descriptions. They may not realise that what they mean by ‘cultural selection’ is entirely different to how someone else uses it. Models are great because they force us to precisely specify exactly what we mean by a particular term or process. We can use the words in the paragraph above to describe biased transmission, but it’s only really clear when we model it, making all our assumptions explicit.) To simulate biased transmission, following the simulations in Chapter 1, we assume there are two traits \\(A\\) and \\(B\\), and that each individual chooses another individual from the previous generation at random. This time, however, we give the traits two different probabilities of being copied: we can call them, \\(s_a\\) and \\(s_b\\) respectively. When an individual encounters another individual with trait \\(A\\), they will copy them with probability \\(s_a\\). When they encounter an individual with trait \\(B\\), they will copy them with probability \\(s_b\\). With \\(s_a=s_b\\), copying is unbiased, and individuals switch to the encountered alternative with the same probability. This reproduces the results of the simulations when transmission is unbiased. If \\(s_a=s_b=1\\), the model is exactly the same as in Chapter 1. The relevant situation in this chapter is when \\(s_a&gt;s_b\\) (or vice versa), so that we have biased transmission. Perhaps \\(A\\) (or \\(B\\)) is a more effective tool, a more memorable story, or a more easily pronounced word. Let’s first write the function, and then explore what happens in this case. Below is a function biased_transmission_direct() that implements all of these ideas. library(tidyverse) set.seed(111) biased_transmission_direct &lt;- function (N, s_a, s_b, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble demonstrator_trait &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # for each individual, pick a random individual from the previous generation to act as demonstrator and store their trait # biased probabilities to copy: copy_a &lt;- sample(c(TRUE, FALSE), N, prob = c(s_a, 1 - s_a), replace = TRUE) copy_b &lt;- sample(c(TRUE, FALSE), N, prob = c(s_b, 1 - s_b), replace = TRUE) if (nrow(population[copy_a &amp; demonstrator_trait == &quot;A&quot;, ]) &gt; 0) { population[copy_a &amp; demonstrator_trait == &quot;A&quot;, ]$trait &lt;- &quot;A&quot; } if (nrow(population[copy_b &amp; demonstrator_trait == &quot;B&quot;, ]) &gt; 0) { population[copy_b &amp; demonstrator_trait == &quot;B&quot;, ]$trait &lt;- &quot;B&quot; } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } Most of biased_transmission_direct() is recycled from the previous models. As before, we initalise the data structure output from multiple runs, and in generation \\(t = 1\\), we create a population tibble to hold the trait of each individual. The major change is that we now include biased transmission. We first select at random the demonstrators from the previous generation (using the same code we used in unbiased_transmission()) and we store their trait in demonstrator_trait. Then we get the probabilities to copy \\(A\\) and to copy \\(B\\) for the entire population, using the same code used in biased_mutation(), only that this time it produces a probability to copy. Again using the same code as in biased mutation(), we have the individuals copy the trait at hand with the desired probability. Let’s run our function biased_transmission_direct(). As before, to plot the results, we can use the same function plot_multiple_runs() we wrote in Chapter 1. As noted above, the interesting case is when one trait is favored over the other. We can assume, for example, \\(s_a=0.1\\) and \\(s_b=0\\). This means that when individuals encounter another individual with trait \\(A\\) they copy them 1 in every 10 times, but, when individuals encounter another individual with trait \\(B\\), they never switch. We can also assume that the favoured trait, \\(A\\), is initially rare in the population (\\(p_0=0.01\\)) to see how selection favours this initially-rare trait (Note that \\(p_0\\) needs to be higher than 0; since there is no mutation in this model, we need to include at least some \\(A\\)s at the beginning of the simulation, otherwise it would never appear). data_model &lt;- biased_transmission_direct(N = 10000, s_a = 0.1, s_b = 0 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) With a moderate selection strength we can see that \\(A\\) gradually replaces \\(B\\) and goes to fixation. It does this in a characteristic manner: the increase is slow at first, then picks up speed, then plateaus. Note the difference to biased mutation. Where biased mutation was r-shaped, with a steep initial increase, biased transmission is s-shaped, with an initial slow uptake. This is because the strength of biased transmission (like selection in general) is proportional to the variation in the population. When \\(A\\) is rare initially, there is only a small chance of picking another individual with \\(A\\). As \\(A\\) spreads, the chances of picking an \\(A\\) individual increases. As \\(A\\) becomes very common, there are few \\(B\\) individuals left to switch. In the case of biased mutation, instead, the probability to switch is independent from the variation in the population. 3.1 Strentgh of selection On what does the strength of selection depend? First, the strength is independent from the specific values of \\(s_a\\) and \\(s_b\\). What counts is their relative difference, in this case \\(s_a-s_b = 0.1\\). If we run a simulation with, say, \\(s_a=0.6\\) and \\(s_b=0.5\\), we see the same pattern, albeit with slightly more noise. That is, the single runs are more different from one another compared to the previous simulation. This is due to the fact that switches from \\(A\\) to \\(B\\) are now also possible. data_model &lt;- biased_transmission_direct(N = 10000, s_a = 0.6, s_b = 0.5 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) To change the selection strength, we need to modify the difference between \\(s_a\\) and \\(s_b\\). We can double the strength by setting \\(s_a = 0.2\\), and keeping \\(s_b=0\\). data_model &lt;- biased_transmission_direct(N = 10000, s_a = 0.2, s_b = 0 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) As we might expect, increasing the strength of selection increases the speed with which \\(A\\) goes to fixation. Note, though, that it retains the s-shape. 3.2 Summary of the model We have seen how biased transmission causes a trait favoured by cultural selection to spread and go to fixation in a population, even when it is initially very rare. Biased transmission differs in its dynamics from biased mutation. Its action is proportional to the variation in the population at the time at which it acts. It is strongest when there is lots of variation (in our model, when there are equal numbers of \\(A\\) and \\(B\\) at \\(p = 0.5\\)), and weakest when there is little variation (when \\(p\\) is close to 0 or 1). 3.3 Analytical appendix As before, we have \\(p\\) individuals with trait \\(A\\), and \\(1 - p\\) individuals with trait \\(B\\). As we saw that what is important is the relative difference between the two probabilities of being copied associated to the two traits and not their absolute value, we consider always \\(s_b=0\\), and vary \\(s_a\\), which we can call simply \\(s\\). Thus, the \\(p\\) individuals with trait \\(A\\) always keep their \\(A\\)s. The \\(1 - p\\) individuals with trait \\(B\\) pick another individual at random, hence with probability \\(p\\), and with probability \\(s\\) they switch to trait \\(A\\). We can therefore write the recursion for \\(p\\) under biased transmission as: \\[p&#39; = p + p(1-p)s \\hspace{30 mm}(3.1)\\] The first term on the right-hand side is the unchanged \\(A\\) bearers, and the second term is the \\(1-p\\) \\(B\\)-bearers who find one of the \\(p\\) \\(A\\)-bearers and switch with probability \\(s\\). Here is some code to plot this biased transmission recursion: t_max &lt;- 150 s &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- 0.01 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] + pop_analytical$p[i - 1] * (1 - pop_analytical$p[i - 1]) * s } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) The curve above should be identical to the simulation curve, given that the simulation had the same biased transmission strength \\(s\\) and a large enough \\(N\\) to minimise stochasticity. From the equation above, we can see how the strength of biased transmission depends on variation in the population, given that \\(p(1 - p)\\) is the formula for variation. This determines the shape of the curve, while \\(s\\) determines the speed with which the equilibrium \\(p^*\\) is reached. But what is the equilibrium \\(p^*\\) here? In fact there are two. As before, the equilibrium can be found by setting the change in \\(p\\) to zero, or when: \\[p(1-p)s = 0 \\hspace{30 mm}(3.2)\\] There are three ways in which the left-hand side can equal zero: when \\(p = 0\\), when \\(p = 1\\) and when \\(s = 0\\). The last case is uninteresting: it would mean that biased transmission is not occurring. The first two cases simply say that if either trait reaches fixation, then it will stay at fixation. This is to be expected, given that we have no mutation in our model. It contrasts with unbiased and biased mutation, where there is only one equilibrium value of \\(p\\). We can also say that \\(p = 0\\) is an unstable equilibrium, meaning that any slight perturbation away from \\(p = 0\\) moves \\(p\\) away from that value. This is essentially what we simulated above: a slight perturbation up to \\(p = 0.01\\) went all the way up to \\(p = 1\\). In contrast, \\(p = 1\\) is a stable equilibrium: any slight perturbation from \\(p = 1\\) immediately goes back to \\(p = 1\\). 3.4 Further reading Boyd and Richerson (1985) modelled direct bias, while Henrich (2001) added directly biased transmission to his guided variation / biased mutation model, showing that this generates s-shaped curves similar to those generated here. Note though that subsequent work has shown that s-shaped curves can be generated via other processes (e.g. Reader (2004)), and should not be considered definite evidence for biased transmission. References "],
["biased-transmission-frequency-dependent-indirect-bias.html", "4 Biased transmission: frequency-dependent indirect bias 4.1 The logic of conformity 4.2 Testing conformist transmission 4.3 Summary of the model 4.4 Analytical appendix 4.5 Further readings", " 4 Biased transmission: frequency-dependent indirect bias 4.1 The logic of conformity In Chapter 3 we looked at the case where one cultural trait is intrinsically more likely to be copied than another trait. Here we will start looking at the other kind of biased transmission, when traits are equivalent, but individuals are more likely to adopt a trait according to the characteristics of the population, and in particular which other individuals already have it. (As we mentioned previously, these are often called ‘indirect’ or ‘context’ biases). A first possibility is that we may be influenced by the frequency of the trait in the population, i.e. how many other individuals already have the trait. Conformity (or ‘positive frequency dependent bias’) has been most studied. Here, individuals are disproportionately more likely to adopt the most common trait in the population, irrespective of its intrinsic characteristics. (The opposite case, anti-conformity or negative frequency dependent bias is also possible, where the least common trait is more likely to be copied. This is probably less common in real life.) For example, imagine trait \\(A\\) has a frequency of 0.7 in the population, with the rest possessing trait \\(B\\). An unbiased learner would adopt trait \\(A\\) with a probability exactly equal to 0.7. This is unbiased transmission, and is what happens the model described in (Chapter 1: by picking a member of the previous generation at random, the probability of adoption is equal to the frequency of that trait among the previous generation. A conformist learner, on the other hand, would adopt trait \\(A\\) with a probability greater than 0.7. In other words, common traits get an ‘adoption boost’ relative to unbiased transmission. Uncommon traits get an equivalent ‘adoption penalty’. The magnitude of this boost or penalty can be controlled by a parameter, which we will call \\(D\\). Let’s keep things simple in our model. Rather than assuming that individuals sample across the entire population, which in any case might be implausible in large populations, let’s assume they pick only three demonstrators at random. Why three? This is the minimum number of demonstrators that can yield a majority (i.e. 2 vs 1), which we need in order to implement conformity. When two demonstrators have one trait and the other demonstrator has a different trait, we want to boost the probability of adoption for the majority trait, and reduce it for the minority trait. We can specify the probability of adoption as follows: Table 1: Probability of adopting trait \\(A\\) for each possible combination of traits amongst three demonstrators Demonstrator 1 Demonstrator 2 Demonstrator 3 Probability of adopting trait \\(A\\) \\(A\\) \\(A\\) \\(A\\) 1 \\(A\\) \\(A\\) \\(B\\) \\(A\\) \\(B\\) \\(A\\) \\(2/3 + D/3\\) \\(B\\) \\(A\\) \\(A\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(1/3 - D/3\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) 0 The first row says that when all demonstrators have trait \\(A\\), then trait \\(A\\) is definitely adopted. Similarly, the bottom row says that when all demonstrators have trait \\(B\\), then trait \\(A\\) is never adopted, and by implication trait \\(B\\) is always adopted. For the three combinations where there are two \\(A\\)s and one \\(B\\), the probability of adopting trait \\(A\\) is \\(2/3\\), which it would be under unbiased transmission (because two out of three demonstrators have \\(A\\)), plus the conformist adoption boost specified by \\(D\\). As we want \\(D\\) to vary from 0 to 1, it is divided by three, so that the maximum probability of adoption is equal to 1 (when \\(D=1\\)). Similarly, for the three combinations where there are two \\(B\\)s and one \\(A\\), the probability of adopting \\(A\\) is 1/3 minus the conformist adoption penalty specified by \\(D\\). Let’s implement these assumptions in the kind of individual based model we’ve been building so far. As before, assume \\(N\\) individuals each of whom possess one of two traits \\(A\\) or \\(B\\). The frequency of \\(A\\) is denoted by \\(p\\). The initial frequency of \\(A\\) in generation \\(t = 1\\) is \\(p_0\\). Rather than going straight to a function, let’s go step by step. First we’ll specify our parameters, \\(N\\) and \\(p_0\\) as before, plus the new conformity parameter \\(D\\). We also create the usual population tibble and fill it with \\(A\\)s and \\(B\\)s in the proportion specified by \\(p_0\\), again exactly as before. library(tidyverse) set.seed(111) N &lt;- 100 p_0 &lt;- 0.5 D &lt;- 1 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation Now we create another tibble, called demonstrators that picks, for each new individual in the next generation, three demonstrators at random from the current population of individuals. It therefore needs three columns/variables, one for each of the demonstrators, and \\(N\\) rows, one for each individual. We fill each column with randomly chosen traits from the population tibble. We can have a look at demonstrators by entering its name in the R console. # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- tibble(dem1 = sample(population$trait, N, replace = TRUE), dem2 = sample(population$trait, N, replace = TRUE), dem3 = sample(population$trait, N, replace = TRUE)) demonstrators ## # A tibble: 100 x 3 ## dem1 dem2 dem3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B B B ## 2 B A A ## 3 A B A ## 4 A B B ## 5 A B B ## 6 A B B ## 7 A B B ## 8 A B A ## 9 A A A ## 10 A B B ## # … with 90 more rows Think of each row here as containing the traits of three randomly-chosen demonstrators chosen by each new next-generation individual. Now we want to calculate the probability of adoption of \\(A\\) for each of these three-trait demonstrator combinations. First we need to get the number of \\(A\\)s in each combination. Then we can replace the traits in population based on the probabilities in Table 1. When all demonstrators have \\(A\\), we set to \\(A\\). When no demonstrators have \\(A\\), we set to \\(B\\). When two out of three demonstrators have \\(A\\), we set to \\(A\\) with probability \\(2/3 + D/3\\) and \\(B\\) otherwise. When one out of three demonstrators have \\(A\\), we set to \\(A\\) with probability \\(1/3 - D/3\\) and \\(B\\) otherwise. # get the number of As in each 3-dem combo num_As &lt;- rowSums(demonstrators == &quot;A&quot;) population$trait[num_As == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A population$trait[num_As == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob_majority &lt;- sample(c(TRUE, FALSE), prob = c((2/3 + D/3), 1 - (2/3 + D/3)), N, replace = TRUE) prob_minority &lt;- sample(c(TRUE, FALSE), prob = c((1/3 - D/3), 1 - (1/3 - D/3)), N, replace = TRUE) # when A is a majority, 2/3 if (nrow(population[prob_majority &amp; num_As == 2, ]) &gt; 0) { population[prob_majority &amp; num_As == 2, ] &lt;- &quot;A&quot; } if (nrow(population[prob_majority == FALSE &amp; num_As == 2, ]) &gt; 0) { population[prob_majority == FALSE &amp; num_As == 2, ] &lt;- &quot;B&quot; } # when A is a minority, 1/3 if (nrow(population[prob_minority &amp; num_As == 1, ]) &gt; 0) { population[prob_minority &amp; num_As == 1, ] &lt;- &quot;A&quot; } if (nrow(population[prob_minority == FALSE &amp; num_As == 1, ]) &gt; 0) { population[prob_minority == FALSE &amp; num_As == 1, ] &lt;- &quot;B&quot; } To check it works, we can add the new population tibble as a column to demonstrators and have a look at it. This will let us see the three demonstrators and the resulting new trait side by side. # for testing only, add the new traits to the demonstrator dataframe and show it demonstrators &lt;- add_column(demonstrators, new_trait = population$trait) demonstrators ## # A tibble: 100 x 4 ## dem1 dem2 dem3 new_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B B B B ## 2 B A A A ## 3 A B A A ## 4 A B B B ## 5 A B B B ## 6 A B B B ## 7 A B B B ## 8 A B A A ## 9 A A A A ## 10 A B B B ## # … with 90 more rows Because we set \\(D=1\\) above, the new trait is always the majority trait among the three demonstrators. This is perfect conformity. We can weaken conformity by reducing \\(D\\). Here an example with \\(D=0.5\\). All the code is the same of what we already discussed above. N &lt;- 100 p_0 &lt;- 0.5 D &lt;- 0.1 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- tibble(dem1 = sample(population$trait, N, replace = TRUE), dem2 = sample(population$trait, N, replace = TRUE), dem3 = sample(population$trait, N, replace = TRUE)) # get the number of As in each 3-dem combo num_As &lt;- rowSums(demonstrators == &quot;A&quot;) population$trait[num_As == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A population$trait[num_As == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob_majority &lt;- sample(c(TRUE, FALSE), prob = c((2/3 + D/3), 1 - (2/3 + D/3)), N, replace = TRUE) prob_minority &lt;- sample(c(TRUE, FALSE), prob = c((1/3 - D/3), 1 - (1/3 - D/3)), N, replace = TRUE) # when A is a majority, 2/3 if (nrow(population[prob_majority &amp; num_As == 2, ]) &gt; 0) { population[prob_majority &amp; num_As == 2, ] &lt;- &quot;A&quot; } if (nrow(population[prob_majority == FALSE &amp; num_As == 2, ]) &gt; 0) { population[prob_majority == FALSE &amp; num_As == 2, ] &lt;- &quot;B&quot; } # when A is a minority, 1/3 if (nrow(population[prob_minority &amp; num_As == 1, ]) &gt; 0) { population[prob_minority &amp; num_As == 1, ] &lt;- &quot;A&quot; } if (nrow(population[prob_minority == FALSE &amp; num_As == 1, ]) &gt; 0) { population[prob_minority == FALSE &amp; num_As == 1, ] &lt;- &quot;B&quot; } # for testing only, add the new traits to the demonstrator dataframe and show it demonstrators &lt;- add_column(demonstrators, new_trait = population$trait) demonstrators ## # A tibble: 100 x 4 ## dem1 dem2 dem3 new_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B A A B ## 2 A B B B ## 3 A A A A ## 4 B B B B ## 5 A B A B ## 6 A A B A ## 7 B B A A ## 8 A B B B ## 9 B A A A ## 10 B B B B ## # … with 90 more rows Now that conformity is weaker, sometimes the new trait is not the majority amongst the three demonstrators. 4.2 Testing conformist transmission As in the previous chapters, we can put all this code together into a function to see what happens over multiple generations and in multiple runs. There is nothing new in the code below, which is a combination of the code we already wrote in (Chapter 1) and the new bits of code for conformity introduced above. conformist_transmission &lt;- function (N, p_0, D, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- tibble(dem1 = sample(population$trait, N, replace = TRUE), dem2 = sample(population$trait, N, replace = TRUE), dem3 = sample(population$trait, N, replace = TRUE)) # get the number of As in each 3-dem combo num_As &lt;- rowSums(demonstrators == &quot;A&quot;) population$trait[num_As == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A population$trait[num_As == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob_majority &lt;- sample(c(TRUE, FALSE), prob = c((2/3 + D/3), 1 - (2/3 + D/3)), N, replace = TRUE) prob_minority &lt;- sample(c(TRUE, FALSE), prob = c((1/3 - D/3), 1 - (1/3 - D/3)), N, replace = TRUE) # when A is a majority, 2/3 if (nrow(population[prob_majority &amp; num_As == 2, ]) &gt; 0) { population[prob_majority &amp; num_As == 2, ] &lt;- &quot;A&quot; } if (nrow(population[prob_majority == FALSE &amp; num_As == 2, ]) &gt; 0) { population[prob_majority == FALSE &amp; num_As == 2, ] &lt;- &quot;B&quot; } # when A is a minority, 1/3 if (nrow(population[prob_minority &amp; num_As == 1, ]) &gt; 0) { population[prob_minority &amp; num_As == 1, ] &lt;- &quot;A&quot; } if (nrow(population[prob_minority == FALSE &amp; num_As == 1, ]) &gt; 0) { population[prob_minority == FALSE &amp; num_As == 1, ] &lt;- &quot;B&quot; } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } We can test the function with perfect conformity (\\(D=1\\)) and plot it (again we use the function plot_multiple_runs() we wrote in Chapter 1). data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.5, D = 1, t_max = 50, r_max = 10) plot_multiple_runs(data_model) Here we should see some lines going to \\(p = 1\\), and some lines going to \\(p = 0\\). Conformity acts to favour the majority trait. This will depend on the initial frequency of \\(A\\) in the population. In different runs with \\(p_0 = 0.5\\), sometimes there will be slightly more \\(A\\)s, sometimes slightly more \\(B\\)s (remember, in our model this is probabilistic, like flipping coins, so initial frequencies will rarely be precisely 0.5). What happens if we set \\(D = 0\\)? data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.5, D = 0, t_max = 50, r_max = 10) plot_multiple_runs(data_model) This model is equivalent to unbiased transmission. As for the simulations described in Chapter 1, with a sufficiently large \\(N\\), the frequencies fluctuate around \\(p = 0.5\\). This underlines the effect of conformity. With unbiased transmission, majority traits are favoured because they are copied in proportion to their frequency (incidentally, it is for this reason that ‘copying the majority’ is not a good description of conformity in the technical sense of cultural evolution: even with unbiased copying the majority trait is copied more than the minority one). However, they reach fixation only in small populations. With conformity, instead, the majority trait is copied with a probability higher than its frequency, so that conformity drives traits to fixation as they become more and more common. As an aside, note that the last two graphs have roughly the same thick black mean frequency line, which hovers around \\(p = 0.5\\). This highlights the dangers of looking at means alone. If we hadn’t plotted the individual runs and relied solely on mean frequencies, we might think that \\(D = 0\\) and \\(D = 1\\) gave identical results. But in fact, they are very different. Always look at the underlying distribution that generates means. Now let’s explore the effect of changing the initial frequencies by changing \\(p_0\\), and adding conformity back in. data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.55, D = 1, t_max = 50, r_max = 10) plot_multiple_runs(data_model) When \\(A\\) starts off in a slight majority (\\(p_0 = 0.55\\)), all of the runs result in \\(A\\) going to fixation (notice this partly depends on the random initialisation: you can change the number in set.seed() to see what happens. However, most if not all runs should result in \\(A\\) going to fixation). Now let’s try the reverse. data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.45, D = 1, t_max = 50, r_max = 10) plot_multiple_runs(data_model) When \\(A\\) starts off in a minority (\\(p_0 = 0.45\\)), all runs result in \\(A\\) disappearing. These last two graphs show how initial conditions affect conformity. Whichever trait is more common is favoured by conformist transmission. 4.3 Summary of the model In this chapter, we explored conformist biased cultural transmission. This is where individuals are disproportionately more likely to adopt the most common trait among a set of demonstrators. We can contrast this indirect bias with the direct (or content) biased transmission from [Chapter 3][Biased transmission (direct bias)], where one trait is intrinsically more likely to be copied. With conformity, the traits have no intrinsic attractiveness and are preferentially copied simply because they are common. We saw how conformity increases the frequency of whichever trait is more common. Initial trait frequencies are important here: traits that are initially more common typically go to fixation. This in turn makes stochasticity important, which in small populations can affect initial frequencies. We also discussed the subtle, but fundamental, difference between unbiased copying and conformity. In both, majority traits are favoured, but it is only with conformity that they are disproportionally favoured. In large populations, unbiased transmission rarely leads to trait fixation, whereas conformist transmission often does. Furthermore, as we will see later, conformity also makes majority traits resistant to external disturbances, such as the introduction of other traits via innovation or migration. 4.4 Analytical appendix Let’s revise Table 1 to add the probabilities of each combination of three demonstrators coming together, assuming they are picked at random. These probabilities can be expressed in terms of \\(p\\), the frequency of \\(A\\), and \\((1 - p)\\), the frequency of \\(B\\). Table 2 adds this column. Table 2: Full adoption probability table for trait \\(A\\) under conformist transmission Dem 1 Dem 2 Dem 3 Prob of adopting \\(A\\) Prob of combination forming \\(A\\) \\(A\\) \\(A\\) 1 \\(p^3\\) \\(A\\) \\(A\\) \\(B\\) \\(A\\) \\(B\\) \\(A\\) \\(2/3 + D/3\\) \\(p^2(1-p)\\) \\(B\\) \\(A\\) \\(A\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(1/3 - D/3\\) \\(p(1-p)^2\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) 0 \\((1-p)^3\\) To get the frequency of \\(A\\) in the next generation, \\(p&#39;\\), we multiply, for each of the eight rows in Table 2, the probability of adopting \\(A\\) by the probability of that combination forming (i.e. the final two columns in Table 2), and add up all of these eight products. After rearranging, this gives the following recursion: \\[p&#39; = p + Dp(1-p)(2p-1) \\hspace{30 mm}(4.1)\\] We can plot the recursion, with weak conformity (\\(D = 0.1\\)) and slightly more \\(A\\) in the initial generation (\\(p_0 = 0.55\\)) as we did previously in the simulation: t_max &lt;- 150 p_0 &lt;- 0.51 D &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;-pop_analytical$p[i - 1] + D * pop_analytical$p[i - 1] * (1 - pop_analytical$p[i - 1]) * (2 * pop_analytical$p[i - 1] - 1) } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) You can change the values of \\(p_0\\) in the code above, (for example less than 0.5, and equal to 0.5) and reproduce the results of the other simulations above. Finally, we can use the recursion equation to generate a plot that has become a signature for conformity in the cultural evolution literature. The following code plots, for all possible values of \\(p\\), the probability of adopting \\(p\\) in the next generation. Note first two new R commands. We use the function seq() to generate a sequence of 101, equally spaced, numbers from 0 to 1, and we use a new ggplot ‘geom’. geom_abline() draws a custom line for which we can pass slop and intercept, as well as other aesthetic properties (such as here linetype = &quot;dashed&quot;). D &lt;- 1 conformity_p_adopt &lt;- tibble( p = seq(from = 0, to = 1, length.out = 101), p_next = p + D * p * (1 - p) * (2 * p - 1)) ggplot(data = conformity_p_adopt, aes(y = p_next, x = p)) + geom_line() + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + ylim(c(0, 1)) + theme_bw() + labs(x = &quot;frequency of A (p)&quot;, y = &quot;probability of adopting A (p&#39;)&quot;) This plot encapsulates the process of conformity. The dotted line shows unbiased transmission: the probability of adopting \\(A\\) is exactly equal to the frequency of \\(A\\) in the population. The s-shaped solid curve shows conformist transmission. When \\(A\\) is common (\\(p &gt; 0.5\\)), then the curve is higher than the dotted line: there is a disproportionately higher probability of adopting \\(A\\). When \\(A\\) is uncommon (\\(p &lt; 0.5\\)), then the curve is lower than the dotted line: there is a disproportionately lower probability of adopting \\(A\\). 4.5 Further readings Boyd and Richerson (1985) introduced conformist or positive frequency-dependent cultural transmission as defined here, and modelled it analytically with similar methods. Henrich and Boyd (1998) modelled the evolution of conformist transmission, while Efferson et al. (2008) provided experimental evidence that at least some people conform in a simple learning task. References "],
["biased-transmission-demonstrator-based-indirect-bias.html", "5 Biased transmission: demonstrator-based indirect bias 5.1 A simple demonstrator bias 5.2 Predicting the ‘winning’ trait 5.3 Summary of the model 5.4 Analytical appendix 5.5 Further readings", " 5 Biased transmission: demonstrator-based indirect bias In the previous two chapters we examined two forms of biased transmission, one where the bias arises due to characteristics of the traits (or [direct bias][Biased transmission (direct bias)]) and another where the bias arises due to the characteristics of the population (or indirect bias). In the previous chapter we examined frequency-dependent indirect bias which takes into account the frequency of the trait (or [conformity][Biased transmission (indirect bias: frequency)]). Here we examine indirect bias that takes into account specific features of the demonstrators. This demonstrator-based bias is also called ‘model bias’ or ‘context bias’ in the cultural evolution literature. Whereas the simulations we created previously are fairly standard, indirect demonstrator-based biases can be implemented in several ways. Demonstrator biases result whenever individuals decide whether to copy or not by taking into account any features of the demonstrators, as long as it is not directly tied to the traits. The most studied demonstrator bias is prestige bias, where individuals are more likely to copy from demonstrators who are considered more ‘prestigious’ or high in subjective social status, for example because other individuals show deference to them. Alternatively, individuals can copy demonstrators who are more successful according to some objective criterion (e.g. wealth) independently from how others judge them, or they can copy individuals that are more similar to themselves, or older (or younger) than themselves, and so on. The key point is that the decision is not directly linked to the cultural trait itself, and relates to some characteristic of the demonstrator(s) from whom one is copying. 5.1 A simple demonstrator bias To implement a simple version of demonstrator-biased cultural transmission, we first need to assume that there are some intrinsic differences between individuals within the population. Up until now, our individuals have only been described by the traits they possess. We now want individuals to have some additional feature which others can use when deciding whether to copy that individual. We call this feature ‘status’. For simplicity, an individual’s status is a binary variable that could stand for whether they are prestigious or not, successful or not, and so on. We define a parameter \\(p_s\\) that determines the probability that an individual has high status, as opposed to low status. library(tidyverse) set.seed(111) N &lt;- 100 p_0 &lt;- 0.5 p_s &lt;- 0.05 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) We can inspect the tibble by typing its name in the R console population ## # A tibble: 100 x 2 ## trait status ## &lt;chr&gt; &lt;chr&gt; ## 1 A low ## 2 A low ## 3 B low ## 4 A low ## 5 B low ## 6 B low ## 7 B low ## 8 A low ## 9 B low ## 10 B high ## # … with 90 more rows With \\(p_s=0.05\\) around 5 individuals in a population of 100 will have high status. In this specific case, one of them is individual 10. We now need to make it so that these rare high status individuals are more likely to be copied. One way of doing this is to assume that the probabilities of picking high-status and low-status individuals as demonstrators are different. So far, when using the function sample() to select demonstrators, we did not include any specific probability. This meant that each individual of the previous generation had the same likelihood of being selected and copied. Instead, now we pass to the function a vector of probabilities to weight the choice. We assume that the probability of selecting high status individuals as demonstrators is always equal to 1, but the probability of selecting low-status individuals is given by a further parameter, \\(p_\\text{low}\\). When \\(p_\\text{low}=1\\), the simulations correspond to unbiased transmission, as everybody has the same probability of being chosen. When \\(p_\\text{low}=0\\), there is a strict status-based demonstrator bias, where only high-status individuals are ever selected as demonstrators. To implement this, we first store in p_demonstrator the probabilities of being copied for each member of the population: p_low &lt;- 0.01 p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low Then we sample the traits in the population using these probabilities. Notice the condition if(sum(p_demonstrator) &gt; 0). This is necessary in case there are no high-status individuals (for example when \\(p_s\\approx0\\)) and the probability of selecting a low status demonstrator to copy is 0 (\\(p_\\text{low}=0\\)). This would make the summed probability equal to 0, and without the condition generate an error. With the condition, no copying occurs, which is what we would expect in this situation. if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } As usual, we can wrap everything in a function. biased_transmission_demonstrator &lt;- function(N, p_0, p_s, p_low, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } We can now test our simulation, assuming a very low, but not zero, probability of selecting low-status individuals as demonstrators. We are using the usual plot_multiple_runs() function to plot the results of the simulations, reproduced for convenience here. data_model &lt;- biased_transmission_demonstrator(N = 100, p_s = 0.05, p_low=0.0001, p_0 = 0.5, t_max = 50, r_max = 5) plot_multiple_runs(data_model) The results are similar to what we saw in the [previous chapter][Biased transmission (indirect bias: frequency)] for conformity: one of the two traits quickly reaches fixation. In the case of conformity, however, the trait reaching fixation was the one that happened to have a slightly higher frequency at the beginning, because of the random initialisation. With a demonstrator bias, this is not the case. From this perspective, an indirect demonstrator-based bias is more similar to unbiased transmission. If you remember from the first chapter, simulations with unbiased transmission also ended up with one trait reaching fixation in small populations (\\(N=100\\)), but in bigger ones (\\(N=10000\\)) the frequencies of the two traits remained around \\(p=0.5\\). What happens with demonstrator-based bias? data_model &lt;- biased_transmission_demonstrator(N = 10000, p_s = 0.005, p_low=0.0001, p_0 = 0.5, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Even with \\(N=10000\\), if the number of high-status individuals is sufficiently low, as in this case (\\(p_s=0.005\\) means that, on average, 50 individuals are high-status in each run), traits reach fixation. By reducing the pool of demonstrators, demonstrator-based bias makes drift more important for the overall dynamics. The pool of high-status demonstrators (equal to \\(Np_s\\)) is the effective population size, which is much smaller than the actual population size (\\(N\\)). You can experiment with different values of \\(p_s\\) and \\(p_\\text{low}\\). How big can the pool of high-status demonstrators be before the dynamics become indistinguishable from unbiased transmission? 5.2 Predicting the ‘winning’ trait With conformity, as just mentioned, the trait that reaches fixation is the one starting in the majority. With unbiased transmission the trait that goes to fixation cannot be predicted at the beginning of the simulation. With a demonstrator-based bias, a reasonable guess would be that the ‘winning’ trait is the one that is, at the beginning, most common among the high-status individuals. Can we check this intuition with our model? Currently the output we obtain from the simulations is not suitable for this purpose. On the one hand, we do not have the crucial piece of information that we need: the proportion of each trait amongst the high-status individuals when the population is initialised. On the other hand, we have much information that we do not need, such as the frequency of the two traits at each time step. We just need to know which traits reach fixation. We can therefore rewrite the biased_transmission_demonstrator function and change the output tibble to suit our needs. biased_transmission_demonstrator_2 &lt;- function(N, p_0, p_s, p_low, t_max, r_max) { output &lt;- tibble(status_A = rep(NA, r_max), p = rep(NA, r_max)) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) output[r, ]$status_A &lt;- sum(population$status == &quot;high&quot; &amp; population$trait == &quot;A&quot;) / sum(population$status == &quot;high&quot;) for (t in 2:t_max) { p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } } output[r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N } output # export data from function } Here, status_A gives the starting frequency of A amongst the high status individuals. \\(p\\), as before, gives the frequency of \\(A\\) in the entire population, but we only record this value at the very end of the simluation, to see if one trait has gone to fixation. Let’s run the new function, biased_transmission_demonstrator_2, for 50 runs (setting \\(r_\\text{max}=50\\)) so that we have more independent data points, and inspect the output. We use set.seed to make sure your output is the same as ours. set.seed(111) data_model &lt;- biased_transmission_demonstrator_2(N = 100, p_s = 0.05, p_low=0.0001, p_0 = 0.5, t_max = 50, r_max = 50) data_model ## # A tibble: 50 x 2 ## status_A p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 0 ## 2 0.571 1 ## 3 0.75 0 ## 4 0.286 0 ## 5 1 1 ## 6 0.333 1 ## 7 0.8 0 ## 8 0.333 0 ## 9 0.571 0 ## 10 0.333 0 ## # … with 40 more rows Each line of the output is a run of the simulation. In the first run, for example, 20% of high-status individuals had the trait \\(A\\) at the beginning, and the frequency of 0trait \\(A\\) at the end of the simulation was 0, meaning that \\(B\\) reached fixation. In the second run, the starting frequency of \\(A\\) was 57%, and by the end \\(A\\) went to fixation. From a cursory inspection of the output, it seems our guess was correct. But let’s visualise all the data to be sure. We want to know how the initial proportion of high-status individuals is related to the two possible outcomes (trait \\(A\\) reaches fixation or trait \\(B\\) reaches fixation). A convenient way is to use a boxplot. In the code below, we first eliminate the runs where the traits did not reach fixation (if they exist) using the new function filter(), and, for clarity, we assign the trait name \\(A\\) or \\(B\\) to each run according to which trait reached fixation. We can then plot our output. The main novelties in this code are the new ggplot ‘geoms’ geom_boxplot() and geom_jitter(). Whereas boxplots are useful to detect aggregate information on our simulations, geom_jitter() plots also the single data points, so we can have a better idea on how the proportions of high-status individuals are distributed in the various runs. We could have done this with our usual geom_point(), but geom_jitter() scatters randomly (at a distance specified by the parameter width) the points in the plot. This allows to avoid the overlapping of individual data points (known as overplotting). data_model &lt;- filter(data_model, p == 1 | p == 0) data_model[data_model$p==1, ]$p &lt;- &quot;A&quot; data_model[data_model$p==0, ]$p &lt;- &quot;B&quot; ggplot(data = data_model, aes(x = p, y = status_A, fill = p)) + geom_boxplot() + geom_jitter(width = 0.05) + labs(y = &quot;proportion of high-status individuals with trait A&quot;, x = &quot;winning trait&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) The plot shows that when trait \\(A\\) reaches fixation there are more high-status individuals with trait \\(A\\) at the beginning, and vice versa for \\(B\\), confirming our intuition. However, this is far from being a safe bet. Runs with only a quarter of high-status individuals with \\(A\\) ended up with all \\(A\\)s in the population and, conversely, runs with 80% of high-status individuals with \\(A\\) ended up with the fixation of \\(B\\). With bigger populations, it is even worse. data_model &lt;- biased_transmission_demonstrator_2(N = 10000, p_s = 0.005, p_low=0.0001, p_0 = 0.5, t_max = 200, r_max = 50) data_model &lt;- filter(data_model, p == 1 | p == 0) data_model[data_model$p==1, ]$p &lt;- &quot;A&quot; data_model[data_model$p==0, ]$p &lt;- &quot;B&quot; ggplot(data = data_model, aes(x = p, y = status_A, fill = p)) + geom_boxplot() + geom_jitter(width = 0.05) + labs(y = &quot;proportion of high-status individuals with trait A&quot;, x = &quot;winning trait&quot;) + ylim(c(0,1)) + theme_bw() + theme(legend.position = &quot;none&quot;) With \\(N=10000\\) and around 50 high-status individuals, the traits are more equally distributed among ‘influential’ demonstrators at the beginning, and there is hardly any difference in the two outcomes. 5.3 Summary of the model In this chapter we modeled an example of indirect, demonstrator-based, biased transmission. We assumed that a fraction of individuals in the population was ‘high-status’ and thus more likely to be selected as demonstrators. The results show that in this situation a trait is likely to become predominant even when populations are large. This is due to the fact that a demonstrator bias effectively reduces the pool of demonstrators and accelerates convergence through a similar process as drift / unbiased transmission. We also saw that the possibility of predicting which trait will become predominant depends on the number of high-status demonstrators. When there are few high-status demonstrators, then the most common trait amongst these high-status demonstrators will likely go to fixation. When their number increases, it is more difficult to make such a prediction. We also saw how it is important to modify the output of a model depending on the question we are interested in. We used a novel ggplot aesthetic to produce a boxplot, a convenient way of displaying the distribution of data among different groups. 5.4 Analytical appendix IS THERE ANYHTIHG WE CAN DO HERE? 5.5 Further readings Examples of simulation models implementing indirect, demonstrator-based, biased transmission include Mesoudi (2009), an individual-based model that explores how prestige bias can generate clusters of recurring behaviours, applied to the case of copycat suicides. Henrich Joseph, Chudek Maciej, and Boyd Robert (2015) presents a population-level model that links prestige to the emergence of whithin-group cooperation. Henrich (2004) describes an analytical, population-level, model, where individuals copy the most succesfull demonstrator in the population. An earlier analytical treatment of demonstrator-based bias, with extensions on the evolution of symbolic traits that may be associated to demonstrators is in Chapter 8 of Boyd and Richerson (1985). Finally, Henrich and Gil-White (2001) is the classic treatment of prestige bias, and a recent review of the empirical evidence supporting it is Jiménez and Mesoudi (2019). References "],
["vertical-and-horizontal-transmission.html", "6 Vertical and horizontal transmission 6.1 Vertical cultural transmission 6.2 Horizontal cultural transmission 6.3 Summary of the model 6.4 Analytical appendix 6.5 Further reading", " 6 Vertical and horizontal transmission An important distinction in cultural evolution concerns the pathway of cultural transmission. Vertical cultural transmission occurs when individuals learn from their parents. Oblique cultural transmission occurs when individuals learn from other (non-parental) members of the older generation, such as teachers. Horizontal cultural transmission occurs when individuals learn from members of the same generation. These terms (vertical, oblique and horizontal) are borrowed from epidemiology, where they are used to describe the transmission of diseases. Cultural traits, like diseases, are interesting in that they have multiple pathways of transmission. While genes spread purely vertically (at least in species like ours; horizontal gene transfer is common in plants and bacteria), cultural traits can spread obliquely and horizontally. These latter pathways can increase the rate at which cultural traits can spread, compared to vertical transmission alone. In this chapter we will simulate and test this claim, focusing in particular on horizontal cultural transmission: when and why does horizontal transmission increase the rate of spread of a cultural trait compared to vertical cultural transmission? 6.1 Vertical cultural transmission To simulate vertical cultural transmission we need to decide how people learn from their parents, assuming those two parents possess different combinations of cultural traits. As in previous models, we assume two discrete traits, \\(A\\) and \\(B\\). There are then four combinations of traits amongst two parents: both parents have \\(A\\), both parents have \\(B\\), mother has \\(A\\) and father has \\(B\\), and mother has \\(B\\) and father has \\(A\\). For simplicity, we can assume that when both parents have the same trait, the child adopts that trait. When parents differ, the child faces a choice. To make things more interesting, let’s assume a bias for one trait over the other in such situations (otherwise we would be back to unbiased transmission, and no trait would reliably spread - remember we are interested in how quickly traits spread under vertical vs horizontal transmission). Hence we assume a probability \\(b\\) that, when parents differ in their traits such that there is some uncertainty, the child adopts \\(A\\). With probability \\(1-b\\) they adopt trait \\(B\\). When \\(b=0.5\\), transmission is unbiased. When \\(b&gt;0.5\\), \\(A\\) should be favoured; when \\(b&lt;0.5\\), \\(B\\) should be favoured. Let’s simulate this and test these predictions. The following function vertical_transmission() is very similar to previous simulation functions. The explanation follows. library(tidyverse) set.seed(111) vertical_transmission &lt;- function(N, p_0, b, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mother &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly pick mothers father &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly pick fathers population &lt;- tibble(trait = rep(NA, N)) # next generation both_A &lt;- mother$trait == &quot;A&quot; &amp; father$trait == &quot;A&quot; if (sum(both_A) &gt; 0) { population[both_A, ]$trait &lt;- &quot;A&quot; # parents both A, adopt A } both_B &lt;- mother$trait == &quot;B&quot; &amp; father$trait == &quot;B&quot; if (sum(both_B) &gt; 0) { population[both_B, ]$trait &lt;- &quot;B&quot; # parents both B, adopt B } if (anyNA(population)) { # if any empty NA slots (i.e. one A and one B parent) are present... population[is.na(population),]$trait &lt;- sample(c(&quot;A&quot;, &quot;B&quot;), sum(is.na(population)), prob = c(b, 1 - b), replace = TRUE) # ...make them A with probability b } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } First we set up an output tibble to store the frequency of \\(A\\) (\\(p\\)) over \\(t_{\\text{max}}\\) generations and across \\(r_{\\text{max}}\\) runs. As before we create a population tibble to store our \\(N\\) traits, one per individual. This time, however, in each generation we create two new tibbles, mother and father. These store the traits of two randomly chosen individuals from the previous_population, one pair for each new individual. Note that we are assuming random mating here: parents pair up entirely at random. Alternative mating rules are possible, such as assortative cultural mating, where parents preferentially assort based on their cultural trait. We will leave it to readers to create models of this. Once the mother and father tibbles are created, we can fill in the new individuals’ traits in population. both_A is used to mark with TRUE whether both mother and father have trait \\(A\\), and (assuming some such cases exist), sets all individuals in population for whom this is true to have trait \\(A\\). both_B works equivalently for parents who both possess trait \\(B\\). The remaining cases (identified as still being NA in the population tibble) must have one \\(A\\) and one \\(B\\) parent. We are not concerned with which parent has which in this simple model, so in each of these cases we set the individual’s trait to be \\(A\\) with probability \\(b\\) and \\(B\\) with probability \\(1-b\\). Again, we leave it to readers to modify the code to have separate probabilities for maternal and paternal transmission. Once all generations are finished, we export the output tibble as our data. We can use our existing function plot_multiple_runs() from previous chapters to plot the results. And now run both functions to see what happens. Remember we are interested in how fast the favoured trait spreads, so let’s start it off at a low frequency (\\(p_0=0.01\\)) so we can see it spreading from rarity. We use a small transmission bias \\(b=0.6\\) favouring \\(A\\). data_model &lt;- vertical_transmission(N = 10000, p_0 = 0.01, b = 0.6, t_max = 50, r_max = 5) plot_multiple_runs(data_model) Here we can see a gradual spread of the favoured trait \\(A\\) from \\(p=0.01\\) to \\(p=1\\). As in our directly biased transmission model, the diffusion curve is s-shaped. To obtain the same result with two different models is encouraging! We can also test our prediction that when \\(b=0.5\\), we recreate our unbiased transmission model from Chapter 1: data_model &lt;- vertical_transmission(N = 10000, p_0 = 0.1, b = 0.5, t_max = 50, r_max = 5) plot_multiple_runs(data_model) As predicted, there is no change in starting trait frequencies when \\(b=0.5\\). If you reduce the sample size, you will see much more fluctuation across the runs, with some runs losing \\(A\\) altogether. 6.2 Horizontal cultural transmission Now let’s add horizontal cultural transmission to our model. We will add it to vertical cultural transmission, rather than replace vertical with horizontal, so we can compare both in the same model. First there is vertical transmission as above, with random mating and the parental bias \\(b\\), to create a new generation. Then, the new generation learns from each other. The key difference between vertical and horizontal transmission is that horizontal cultural transmission can occur from more than two individuals. Let’s assume individuals pick \\(n\\) other individuals from their generation. We also assume a bias in favour of \\(A\\) during horizontal transmission. If the learner is \\(B\\), then for each of the \\(n\\) demonstrators who have \\(A\\), there is an independent probability \\(g\\) that the learner switches to \\(A\\). If the learner is already \\(A\\), or if the demonstrator is \\(B\\), then nothing happens. The following code implements this horizontal transmission in a new function vertical_horizontal_transmission(). vertical_horizontal_transmission &lt;- function(N, p_0, b, n, g, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r # vertical transmission for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mother &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly pick mothers father &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly pick fathers population &lt;- tibble(trait = rep(NA, N)) # next generation both_A &lt;- mother$trait == &quot;A&quot; &amp; father$trait == &quot;A&quot; if ( sum(both_A) &gt; 0 ) { population[both_A, ]$trait &lt;- &quot;A&quot; # parents both A, adopt A } both_B &lt;- mother$trait == &quot;B&quot; &amp; father$trait == &quot;B&quot; if ( sum(both_B) &gt; 0) { population[both_B, ]$trait &lt;- &quot;B&quot; # parents both B, adopt B } if (anyNA(population)) { # if any empty NA slots (i.e. one A and one B parent) are present... population[is.na(population),]$trait &lt;- sample(c(&quot;A&quot;, &quot;B&quot;), sum(is.na(population)), prob = c(b, 1 - b), replace = TRUE) # ...make them A with probability b } # horizontal transmission previous_population &lt;- population # previous_population are children before horizontal transmission, population are children after horizontal transmission N_B &lt;- length(previous_population$trait[previous_population$trait == &quot;B&quot;]) # N_B = number of Bs if (N_B &gt; 0 &amp; n &gt; 0) { # if there are B individuals to switch, and n is not zero for (i in 1:N_B) { # for each B individual... demonstrator &lt;- sample(previous_population$trait, n, replace = TRUE) # pick n demonstrators copy &lt;- sample(c(TRUE, FALSE), n, prob = c(g, 1-g), replace = TRUE) # get probability g if ( sum(demonstrator == &quot;A&quot; &amp; copy == TRUE) &gt; 0 ) { # if any demonstrators with A are to be copied... population[previous_population$trait == &quot;B&quot;,]$trait[i] &lt;- &quot;A&quot; # ...the B individual switches to A } } } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } The first part of this code is identical to vertical_transmission. Then there is horizontal transmission. We put population into previous_population again, but now population contains the individuals after horizontal transmission, and previous_population contains the individuals before. \\(N_B\\) holds the individuals in previous_population who are \\(B\\), as they are the only ones we need to concern ourselves with (\\(A\\) individuals do not change). If there are such individuals (\\(N_B&gt;0\\)), and individuals are learning from at least one individual (\\(n&gt;0\\)), then for each individual we pick \\(n\\) demonstrators, and if any of those demonstrators are \\(A\\) plus probability \\(g\\) is fulfilled, we set the individual to \\(A\\). Running horizontal transmission with \\(n=5\\) and \\(g=0.1\\) and without vertical transmission bias (\\(b=0.5\\)) causes, as expected, \\(A\\) to spread. data_model &lt;- vertical_horizontal_transmission(N = 5000, p_0 = 0.01, b = 0.5, n = 5, g = 0.1, t_max = 50, r_max = 5) plot_multiple_runs(data_model) This plot above confirms that horizontal cultural transmission, with some direct bias in the form of \\(g\\), again generates an s-shaped curve and causes the favoured trait to spread. But we haven’t yet done what we set out to do, which is compare the speed of the different pathways. The following code generates three datasets, one with only vertical transmission and \\(b=0.6\\), one with only horizontal transmission with \\(n=2\\) and \\(g=0.1\\) which is roughly equivalent to two parents and a bias of \\(b=0.6\\) (0.1 higher than unbiased), and one with only horizontal transmission with \\(n=5\\) and \\(g=0.1\\). data_model_v &lt;- vertical_horizontal_transmission(N = 5000, p_0 = 0.01, b = 0.6, n = 0, g = 0, t_max = 50, r_max = 5) data_model_hn2 &lt;- vertical_horizontal_transmission(N = 5000, p_0 = 0.01, b = 0.5, n = 2, g = 0.1, t_max = 50, r_max = 5) data_model_hn5 &lt;- vertical_horizontal_transmission(N = 5000, p_0 = 0.01, b = 0.5, n = 5, g = 0.1, t_max = 50, r_max = 5) plot_multiple_runs(data_model_v) plot_multiple_runs(data_model_hn2) plot_multiple_runs(data_model_hn5) The first two plots should be very similar. Horizontal cultural transmission from \\(n=2\\) demonstrators is equivalent to vertical cultural transmission, which of course also features two demonstrators, when both pathways have similarly strong direct biases. The third plot shows that increasing the number of demonstrators makes favoured traits spread more rapidly under horizontal transmission, without changing the strength of the biases. Of course, changing the relative strength of the vertical and horizontal biases (\\(b\\) and \\(g\\) respectively) also affects the relative speed. But all else being equal, horizontal transmission with \\(n&gt;2\\) is faster than vertical transmission. 6.3 Summary of the model This model has combined directly biased transmission with vertical and horizontal transmission pathways. The vertical transmission model recreates the patterns from our previous unbiased and directly biased transmission, but explicitly modelling parents and their offspring. Although there were no differences, our vertical transmission model could be modified easily to study different kinds of parental bias (e.g. making maternal influence stronger than paternal influence), or different types of non-random mating. Our horizontal transmission model is similar to the conformist bias simulated in Chapter 4, but slightly different - there is no disproportionate majority copying, and instead one trait is favoured when learning from \\(n\\) demonstrators. Comparing the two pathways, we can see that horizontal cultural transmission is faster than vertical cultural transmission largely because it allows individuals to learn from more than two demonstrators. 6.4 Analytical appendix Todo, or omit. Equations are in Cavalli-Sforza &amp; Feldman chapters on vertical &amp; horizontal transmission. 6.5 Further reading The above models are based on those by Cavalli-Sforza and Feldman (1981). Their vertical cultural transmission models feature bias parameters for each combination of matings (\\(b_0\\), \\(b_1\\), \\(b_2\\) and \\(b_3\\)); our \\(b\\) is their \\(b_1\\) and \\(b_2\\). Their horizontal transmission model also features \\(n\\) and \\(g\\), which have the same definitions as here. Subsequent models in that volume examine assortative cultural mating and oblique transmission, although the latter is similar to horizontal transmission. Empirically, there is a large literature trying to identify the relative roles of parents and non-parents on beliefs and behaviour. Harris (1995) argued that parents have little influence on their children beyond keeping them alive and passing on genes. Aunger (2000) and Henrich and Broesch (2011) argue for a 2-stage process, with children initially learning from parents, and then updating that knowledge in adolescence and adulthood by learning from successful others. References "],
["multiple-traits-models.html", "7 Multiple traits models 7.1 Introducing innovation 7.2 Optimising the code 7.3 The distribution of popularity 7.4 Summary of the model 7.5 Analytical appendix 7.6 Further readings", " 7 Multiple traits models In all previous models, individuals could possess one of only two cultural traits, \\(A\\) or \\(B\\). This is a useful simplification, and it represents cases in which cultural traits can be modeled as binary choices, such as voting Republican or Democrat, driving on the left or the right, or being vegetarian or meat-eating. In other cases, however, there are many options: in many countries there are multiple political parties to vote for, there may be many dietary choices (vegan, pescatarian, vegetarian, etc), and so on. What happens when we copy others’ choices given more than two alternatives? To simplify this question, we again assume unbiased copying as in the first chapter: all traits are functionally equivalent and other individuals are copied at random. The first modification we need to make in the code concerns how traits are represented. Since we have an undetermined number of possible traits we cannot use the two letters \\(A\\) and \\(B\\). Instead we will use numbers, referring to trait “1”, trait “2”, trait “3”, etc. How can we distribute the traits in the initial population? We can assume that there are \\(m\\) possible traits at the beginning, with \\(m \\leq N\\) (as usual, \\(N\\) is the population size). In all the following simulations, we will fix \\(m=N\\), and effectively initialise each individual with a trait randomly chosen between “1” and “100”. library(tidyverse) set.seed(111) N &lt;- 100 population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) You can inspect the population tibble by writing its name. population ## # A tibble: 100 x 1 ## trait ## &lt;int&gt; ## 1 60 ## 2 73 ## 3 38 ## 4 52 ## 5 38 ## 6 42 ## 7 2 ## 8 54 ## 9 44 ## 10 10 ## # … with 90 more rows The basic code of the simulation is similar to the code in the first chapter, but what should the output be? Until now, we just needed to save the frequency of one of the two traits, because the frequency of the other was always one minus the first’s frequency. Now we need the frequencies of all \\(N\\) traits. (Technically, we only need to track \\(N-1\\) frequencies, with the last inferred by substracting the other frequencies from 1. But for simplicity we’ll track all of the frequencies.) Second, how do we measure the frequency of the traits in each generation? The base R function tabulate() does this for us. tabulate() counts the number of times each element of a vector (population$trait in our case) occurs in the bins that we also pass to the function. In our case the bins are \\(1\\) to \\(N\\). Since we want the frequencies, and not the absolute number, we divide the result by \\(N\\). multiple_traits &lt;- function(N, t_max) { output &lt;- tibble(trait = as.factor(rep(1:N, each = t_max)), generation = rep(1:t_max, N), p = rep(NA, t_max * N)) population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) # create first generation output[output$generation == 1, ]$p &lt;- tabulate(population$trait, nbins = N) / N # add first generation&#39;s p for all traits for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t, ]$p &lt;- tabulate(population$trait, nbins = N) / N # get p for all traits and put it into output slot for this generation t } output # export data from function } Finally, the function to plot the output is similar to what we have already done when plotting multiple runs. The one difference is that now the colored lines do not represent different runs, but different traits, as indicated below by aes(colour = trait). The new line theme(legend.position = &quot;none&quot;) simply tells ggplot to not include the legend in the graph, as it is not informative. It would just show 100 colors, one for each trait. plot_multiple_traits &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line(aes(colour = trait)) + ylim(c(0, 1)) + theme_bw() + theme(legend.position = &quot;none&quot;) } As usual, we can call the function and see what happens: data_model &lt;- multiple_traits(N = 100, t_max = 200) plot_multiple_traits(data_model) Only one trait is still present at the end of the simulation. In general, only one or two traits are still present in the population after 200 generations, and, if we increase \\(t_\\text{max}\\) for example to 1000, virtually all runs end up with only a single trait reaching fixation: data_model &lt;- multiple_traits(N = 100, t_max = 1000) plot_multiple_traits(data_model) This is similar to what we saw with only two traits, \\(A\\) and \\(B\\): with unbiased copying and relatively small populations, drift is a powerful force and quickly erodes cultural diversity. As we already discussed, increasing \\(N\\) reduces the effect of drift. You can experiment with various values for \\(N\\) and \\(t_\\text{max}\\). However, the general point is that variation is gradually lost in all cases. How can we counterbalance the homogenizing effect that drift has in small and isolated population, such as the one we are simulating? 7.1 Introducing innovation One option is to introduce new traits via innovation. We can imagine that, at each time step, a proportion of individuals, \\(\\mu\\), introduces a new trait in the population. We use the same notation that we used for mutation in chapter 2: you can think that ‘mutation’ is when an individual change its trait for one that is already present, whereas an ‘innovation’ happens when an individual introduces a new trait never seen before. The remaining proportion of individuals, \\(1-\\mu\\), copy at random from others, as before. We can start with a small value, such as \\(\\mu=0.01\\). Since \\(N=100\\), this means that in each generation, on average, one new trait will be introduced into the population. The following code adds innovation to the multiple-trait code from above: mu &lt;- 0.01 last_trait &lt;- max(population) # record the last trait introduced in the population previous_population &lt;- population # copy the population tibble to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # identify the innovators if( sum(innovators) &gt; 0){ # if there are innovators population[innovators,]$trait &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } There are two modifications here. First, we need to select who are the innovators. For that, we use again the function sample(), biased by \\(\\mu\\), picking \\(TRUE\\) (corresponding to being an innovator) or \\(FALSE\\) (keeping the copied cultural trait) \\(N\\) times. Second, we need to actually introduce the new traits, with the correct number labels. First we record at the beginning of each generation the label of the last trait introduced (at the beginning, with \\(N=100\\), it will likely be 100 because we initialise each individual’s traits by choosing randomly between 1 and 100). When new traits are introduced, we give them consecutive number labels: the first new trait will be called 101, the second 102, and so on. The code above adds all of the new traits into the innovator slots all in one go, which is more efficient than doing it one innovator at a time. We can now, as usual, wrap everything in a function: multiple_traits_2 &lt;- function(N, t_max, mu) { max_traits &lt;- N + N * mu * t_max output &lt;- tibble(trait = as.factor(rep(1:max_traits, each = t_max)), generation = rep(1:t_max, max_traits), p = rep(NA, t_max * max_traits)) population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) # create first generation output[output$generation == 1, ]$p &lt;- tabulate(population$trait, nbins = N) / N # add first generation&#39;s p for all traits for (t in 2:t_max) { last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if ((last_trait + sum(innovators)) &lt; max_traits) { if( sum(innovators) &gt; 0){ population[innovators,]$trait &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } } output[output$generation == t, ]$p &lt;- tabulate(population$trait, nbins = max_traits) / N # get p for all traits and put it into output slot for this generation t } output # export data } You should now be familiar with more or less everything within this function, with one exception: the new quantity max_traits. This is a trick we are using to avoid making the code too slow to run. Our output tibble, as you remember, records all the frequencies of all traits. When programming, a good rule-of-thumb is to avoid dynamically modifying the size of your data structures, such as adding new rows to a pre-existing tibble during the simulation. Where possible, set the size of a data structure at the start, and then modify its values during the simulation. So rather than creating a tibble that is expanded dynamically as new traits are introduced via innovation, we create a bigger tibble from the start. How big should it be? We do not know for sure, but a good estimate is that we will need space for the initial traits (\\(N\\)), plus around \\(N\\mu\\) traits that are added each generation. To be absolutely sure we do not exceed this estimate, we wrap the innovation instruction within the if ((last_trait + sum(innovators)) &lt; max_traits) condition. This prevents innovation when the tibble has filled up. This might prevent innovation in the last few generations, but generally this should hae negligible consequences for our purposes. Let’s now run the function with an innovation rate \\(\\mu=0.01\\), a population of 100 individuals, and for 200 generations. data_model &lt;- multiple_traits_2(N = 100, t_max = 200, mu = 0.01) plot_multiple_traits(data_model) With innovation, there should now be more traits at non-zero frequency at the end of the simulation than when innovation was not possible. We can check the exact number, by inspecting how many frequencies are higher than 0 in the last row of our matrix: sum(filter(data_model, generation==200)$p &gt; 0) ## [1] 8 What happens if we increase the number of generations, or time steps, to 1000, as we did before? data_model &lt;- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01) plot_multiple_traits(data_model) As you can see in the plot, there should still be several traits that have frequencies higher than 0, even after 1000 generations. Again, we can find the exact number in the final generation: sum(filter(data_model, generation==1000)$p &gt; 0) ## [1] 8 Innovation, in sum, allows the maintenance of variation even in small populations. 7.2 Optimising the code Now for a short technical digression. You may have noticed that running the function multiple_traits_2() is quite time consuming with a population of 1000 individuals. There is a quick way to check the exact time needed, using the function Sys.time(). This returns the current time at the point of its execution. Let’s run the function again and calculate how long it takes. start_time &lt;- Sys.time() data_model &lt;- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01) end_time &lt;- Sys.time() end_time - start_time ## Time difference of 37.1768 secs On a typical laptop, it may take more than 30 or 40 seconds. To store the output, we are using a tibble with \\(1100000\\) data points, as max_traits is equal to \\(1100\\), which needs to be updated in each of the \\(1000\\) generations. One way of speeding up the simulation is to record our output in a different data structure. So far, we have been using tibbles to store our simulation output. R, as with all programming languages, can store data in different structures. Depending on what the data are and what one wants to do with them, different structures are more or less suitable. The advantage of tibbles is that they can contain heterogeneous data, depending on what we need to store: for example, in our output tibble, the \\(trait\\) column was specified as a factor, whereas the others two columns, \\(generation\\) and \\(p\\), were numeric. An alternative is to use vectors and matrices. A vector is a list of data points that are all of the same type, e.g. logical (TRUE/FALSE), integer (whole numbers), numeric (any numbers), or character (text). Matrices are just two-dimensional vectors: they must also contain all the same type of data, but they have rows and columns similar to a tibble, dataframe or Excel spreadsheet. The advantage of vectors and matrices is efficiency: they make simulations much faster than identical code running with tibbles. Let’s rewrite our multiple trait function that runs exactly the same simulation, but using matrices instead of tibbles. The output is now a matrix with \\(t_\\text{max}\\) rows and max_traits columns. This is initialised with NAs at the beginning. The population is a vector of integers, representing the trait held by each individual. multiple_traits_matrix &lt;- function(N, t_max, mu) { max_traits &lt;- N + N * mu * t_max output &lt;- matrix(data = NA, nrow = t_max, ncol = max_traits) # create first generation population &lt;- sample(1:N, N, replace = TRUE) output[1, ] &lt;- tabulate(population, nbins = N) / N # add first generation&#39;s p for all traits for (t in 2:t_max) { last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- sample(previous_population, N, replace = TRUE) # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if ((last_trait + sum(innovators)) &lt; max_traits) { population[innovators] &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } output[t, ] &lt;- tabulate(population, nbins = max_traits) / N # get p for all traits and put it into output slot for this generation t } output # export data } To plot the output, we re-convert it into a tibble so that it can be handled by ggplot(). We first create a column that explicitly indicates the number of generations, and then we use the function gather() from the tidyverse to reassemble the columns of the matrix in key-value pairs. plot_multiple_traits_matrix &lt;- function(data_model) { generation &lt;- rep(1:dim(data_model)[1], dim(data_model)[2]) data_to_plot &lt;- as_tibble(data_model) %&gt;% gather( key = &quot;trait&quot;, value = &quot;p&quot;) %&gt;% add_column(generation) ggplot(data = data_to_plot, aes(y = p, x = generation)) + geom_line(aes(colour = trait)) + ylim(c(0, 1)) + theme_bw() + theme(legend.position = &quot;none&quot;) } We can now run the new function, checking that it gives the same output as the tibble version, and again calculating the time needed. start_time &lt;- Sys.time() data_model &lt;- multiple_traits_matrix(N = 100, t_max = 1000, mu = 0.01) end_time &lt;- Sys.time() plot_multiple_traits_matrix(data_model) end_time - start_time ## Time difference of 0.05603719 secs The results are equivalent, but the simulation is almost 100 times faster! This shows that implementation details are very important when building individual based models. When one needs to run the same simulation many times, or test many different parameter values, implementation choices can make drastic differences. 7.3 The distribution of popularity An interesting aspect of these simulations is that, even if all traits are functionally equivalent and transmission is unbiased, a few traits, for random reasons, are more successful than the others. A way to visualise this is to plot their cumulative popularity, i.e. the sum of their quantities over all generations. Given our matrix, it is easy to calculate this by summing each column and multiplying by N (remember they are frequencies, whereas now we want to visualise their actual quantities). We also need to keep only the values that are higher than zero: values equal to zero are in fact the empty slots created in the initial matrix that were never filled wiht actual traits. cumulative &lt;- colSums(data_model) * N cumulative &lt;- cumulative[cumulative &gt; 0] Let’s sort them from the most to the least popular and plot the results. data_to_plot &lt;- tibble(cumulative = sort(cumulative, decreasing = TRUE)) ggplot(data = data_to_plot, aes(x = seq_along(cumulative), y = cumulative)) + geom_point() + theme_bw() + labs(x = &quot;trait label&quot;, y = &quot;cumulative popularity&quot;) This is an example of a long-tailed distribution. The great majority of traits did not spread in the population, and their cumulative popularity is very close to one. Very few of them—the ones on the left side of the plot—were instead very successful. Long-tailed distributions like the one we just produced are very common for cultural traits: a small number of movies, books, or first names are very popular, while the great majority is not. In addition, in these domains, the popular traits are much more popular than the unpopular ones. The average cumulative popularity is mean(cumulative), but the most successful trait has a popularity of max(cumulative). It is common to plot these distributions by binning the data in intervals of exponentially increasing size. In other words, we want to know how many traits have a cumulative popularity between 1 and 2, then between 2 and 4, then between 4 and 8, and so on, until we reach the maximum value of cumulative popularity. The code below does that, using a for cycle to find how many traits fall in each bin and further normalising according to bin size. The size is increased 50 times, until an arbitrary maximum bin size of \\(2^{50}\\), to be sure to include all cumulative popularities. bin &lt;- rep(NA, 50) x &lt;- rep(NA, 50) for( i in 1:50 ){ bin[i] &lt;- sum( cumulative &gt;= 2^(i-1) &amp; cumulative &lt; 2^i) bin[i] &lt;- ( bin[i] / length( cumulative ) ) / 2^(i-1); x[i] &lt;- 2^i } We can now visualise the data on a log-log plot, after filtering out the empty bins. A log-log plot is a graph that uses logarithmic scales on both axes. Using logarithmic axes is useful when, as in this case, the data are skewed towards large values. In the previous plot, we were not able to appreciate visually any difference in the great majority of data points, for example points that had cumulative popularity between 1 and 10, as they were all bunched up close to the x-axis. data_to_plot &lt;- tibble(bin = bin, x = x) data_to_plot &lt;- filter(data_to_plot, bin &gt; 0) ggplot(data = data_to_plot, aes(x = x, y = bin)) + geom_point() + labs(x = &quot;cumulative popularity&quot;, y = &quot;proportion of traits&quot;) + scale_x_log10() + scale_y_log10() + stat_smooth(method = &quot;lm&quot;) + theme_bw() On a log-log scale, the distribution of cumulative popularity produced by unbiased copying lies approximately on a straight line (this linear best-fit line is produced using the command stat_smooth(method = &quot;lm&quot;)). This straight line on a log-log plot is known as a “power law” frequency distribution. The goodness of fit and the slope of the line can be used to compare different types of cultural transmission. For example, what would happen to the above power law if we added some degree of conformity? What about demonstrator-based bias? We can also generate equivalent plots for real-world cultural datasets to test hypotheses about the processes that generated these distributions in the real world. 7.4 Summary of the model In this chapter we simulated the case where individuals can possess one of more than two traits. We explored the simplest case of unbiased transmission. We also implemented the possibility of innovation, where individuals introduce, with some probability, new traits into the cultural pool of the population. Individual innovations counterbalance the homogenizing effect of drift, and replace the traits that are gradually lost. To simulate multiple traits and innovation we also needed to deal with a few technical details such as how to keep track of an initially unknown number of new traits. We learned that it is best to create data structures of the desired size at the outset, rather than changing their size dynamically during the simulation. We also saw the importance of using appropriate data structures when simulations start to become more complex. Replacing tibbles with matrices, we were able to make our simulation 100 times faster. Our results showed that unbiased copying produces long-tailed distributions where very few traits are very popular and the great majority are not. An interesting insight from this model is that these extreme distributions do not necessarily result from extreme tendencies at the individual level. Some traits become hugely more popular than others without individuals being biased, for example, towards popular traits. Cultural transmission generates these distributions without biases, but simply because popular traits have the intrinsic advantage of being more likely to be randomly copied. We also introduced a new technique, the log-log plot of binned popularity distributions, to visualise this outcome. 7.5 Analytical appendix ANYHTING TO DO HERE? 7.6 Further readings Neiman (1995) first introduced a model of unbiased copying with multiple traits to explain popularity distributions in assemblages of Neolithic pottery. Bentley, Hahn, and Shennan (2004) elaborated on this idea, presenting a ‘random copying’ model (equivalent to the one developed in this chapter) and comparing the popularity distributions produced with real datasets, including the frequency distributions of first names in the US and the citations of patents. Mesoudi and Lycett (2009) explored how adding transmission biases (e.g. conformity) to the basic model changes the resulting power-law frequency distribution. "]
]
