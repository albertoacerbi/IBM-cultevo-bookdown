[
["multiple-traits-models.html", "6 Multiple traits models 6.1 Introducing innovations 6.2 Optimising the code 6.3 The distribution of popularity 6.4 Summary of the model 6.5 Analytical appendix 6.6 Further readings", " 6 Multiple traits models In all the scenarios we considered so far, individuals could posses one of two cultural traits, \\(A\\) or \\(B\\). This is a useful simplification, and it represents cases in which cultural traits can be modeled as binary choices, such as being in favour or against a particular policy, choosing between Beatles and Rolling Stones (and no one else!), eating meat or not, and similar. In other cases, however, there are many options: there are many books to read, movies to watch, and, despite our Beatles and Rolling Stones example, many musical bands one can choose to listen to. What does it happen when we copy others’ choices? To simplfy, we are again assuming unbiased copying as in the [first chapter][Unbiased transmission]: all traits are equivalent and we do not copy preferentially from any individual, but just pick them at random. The first modification we need to do in the code concerns how traits are represented. Since we have an undetermined number of possible traits we can not use the two letters \\(A\\) abd \\(B\\), but we will use instead numbers, so that traits will be now referred as trait “1”, trait “2”, trait “2”, etc. To start with, we can initliase each individual with a trait randomly chosen between “1” and “100”. library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() set.seed(111) N &lt;- 100 population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) As usual, you can inspect the population tibble by writing its name. population ## # A tibble: 100 x 1 ## trait ## &lt;int&gt; ## 1 60 ## 2 73 ## 3 38 ## 4 52 ## 5 38 ## 6 42 ## 7 2 ## 8 54 ## 9 44 ## 10 10 ## # … with 90 more rows The basic code of the simulation is similar to the code in the [first chapter][Unbiased transmission], but what the output should be? Until now, we just needed to save the freuquency of one of the two trait, but now we need the frequencies of all N traits to have an idea of what happens in the simulation. Another modification in the code above concerns how we measure the frequency of the traits in each generation. The function hist(), as the name suggests, is generally used to plot an histogram of the data. However, it can be used, adding the argument plot = FALSE, to only calculate what we would need for an histogram, without producing the graph. Among the outputs produced, density gives the relative frequencies of the binned data, which is what we are interested in. multiple_traits &lt;- function(N, t_max) { output &lt;- tibble(trait = as.factor(rep(1:N, each = t_max)), generation = rep(1:t_max, N), p = rep(NA, t_max * N)) population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) # create first generation output[output$generation == 1, ]$p &lt;- hist(population$trait, 0:N, plot = FALSE)$density # add first generation&#39;s p for all traits for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t, ]$p &lt;- hist(population$trait, 0:N, plot = FALSE)$density # get p for all traits and put it into output slot for this generation t } output # export data from function } Finally, the function to plot the output is similar to what we have already done when plotting multiple runs, with the difference that now the colored lines do not represent different runs, but different traits, as indicated below by aes(colour = trait). We also need to specify that traits should not be interpreted The new line theme(legend.position = &quot;none&quot;) simply tells to not include the legends in the graph, as it is not informative, and it would show 100 colors, one for each trait. plot_multiple_traits &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line(aes(colour = trait)) + ylim(c(0, 1)) + theme_bw() + theme(legend.position = &quot;none&quot;) } As usual, we can call the function and see what happens: data_model &lt;- multiple_traits(N = 100, t_max = 200) plot_multiple_traits(data_model) Only one trait is still present at the end of the simulation. In general, only one or two traits are still present in the population after 200 generations, and, if we increase \\(t_\\text{max}\\) for example to 1000, virtually all runs end up with only a single trait reaching fixation: data_model &lt;- multiple_traits(N = 100, t_max = 200) plot_multiple_traits(data_model) This is similar to what we saw when considering the analogous situation with only two traits, \\(A\\) and \\(B\\): with unbiased copying and relative small populations, drift is a powerful force, that quickly erodes cultural diversity. As we already discussed, increasing \\(N\\) limits the effect of drift. You can experiment with various values for \\(N\\) and \\(t_\\text{max}\\). However, the general point is that variation is gradually lost in all cases. How can we counterbalance the homogenizing effect that drift has in small and isolated population, such as the one we are simulating? 6.1 Introducing innovations An option is to introduce new traits with individual innovations. We can imagine that, at each time step, a proportion of individuals, \\(\\mu\\) (we use the same notation that we used for mutation in chapter 2), introduces a new trait in the population. The remaining proportion of individuals, \\(1-\\mu\\) copy at random from others, as before. We can start with a small value, such as \\(\\mu=0.01\\). Since \\(N=100\\), this means that at each generation, on average, one new trait will be introduced in the population. Let’s see how we can write what happens at each generation: mu &lt;- 0.01 last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy the population tibble to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators population[innovators,]$trait &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits There are two modifications here. First, we need to select who are the innovators. For that, we use again the function sample(), biased by \\(\\mu\\), picking \\(TRUE\\) (corresponding to be an innovator) or \\(FALSE\\) (keeping the copied cultural trait) for \\(N\\) times. Second, we need to keep track of the new introduced traits. In order to do so, we record at the beginning of each generation what is the “name” of the last trait introduced (at the beginning, with \\(N=100\\), it will be “100”, as we initialise each individual of the population with a different trait). When new traits are introduced, we call them with consecutive numbers: the first new traits will be called “101”, the second “102” and so on. We can now, as usual, wrap everything in a function. multiple_traits_2 &lt;- function(N, t_max, mu) { max_traits &lt;- N + N * mu * t_max output &lt;- tibble(trait = as.factor(rep(1:max_traits, each = t_max)), generation = rep(1:t_max, max_traits), p = rep(NA, t_max * max_traits)) population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) # create first generation output[output$generation == 1, ]$p &lt;- hist(population$trait, 0:N, plot = FALSE)$density # add first generation&#39;s p for all traits for (t in 2:t_max) { last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if ((last_trait + sum(innovators)) &lt; max_traits) { if( sum(innovators) &gt; 0){ population[innovators,]$trait &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } } output[output$generation == t, ]$p &lt;- hist(population$trait, 0:max_traits, plot = FALSE)$density # get p for all traits and put it into output slot for this generation t } output # export data } You should now be familiar with more or less everything within this function, with one exception: the introduction of the new quantity max_traits. This is a trick we are using to avoid making the code too heavy to run. Our output tibble, as you remember, record all the frequencies of all traits. When programming, a good rule-of-thumb is to avoid to modify dynamically the size of your data structures, such as, for example, adding new rows to a pre-existing tibble, as in our case. In our simulation, at every generation, there is some probability that a new trait will be introduced so that, as a consequence, we would need to add new rows in the output tibble to record its frequency. To avoid this, we are creating a bigger tibble from the beginning, with rows for many possible new traits. How many is ‘many’? We do not know, but an estimate is that we will need space for the initial traits (\\(N\\)), plus around \\(N\\mu\\) traits for each generation. To be sure to not exceed this number, we wrapped the innovation instruction in the if ((last_trait + sum(innovators)) &lt; max_traits) control. As a consequence, it is possible that in some runs, in the very last generations, innovations will not be permitted. For many purposes, this does not change the outcome of the simulation, and for the time being is better than modify dynamically our output. Let’s now run the function with an innovation rate \\(\\mu=0.01\\), again with a population of 100 individuals, and for 200 generations. data_model &lt;- multiple_traits_2(N = 100, t_max = 200, mu = 0.01) plot_multiple_traits(data_model) There should be now more traits at non-zero frequency at the end of the simulation that what happened when innovations were not possible. We can actually check the exact number, by inspecting how many frequencies higher than 0 are in the last row of our matrix: sum(filter(data_model, generation==200)$p &gt; 0) ## [1] 10 What happens if we increase the number of generations, or time steps, to 1000, as we did before? data_model &lt;- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01) plot_multiple_traits(data_model) As you can see in the plot, there should still be various traits that have frequencies higher than 0, even after 1000 generations. Again, we can check it sum(filter(data_model, generation==1000)$p &gt; 0) ## [1] 10 Innovation, in sum, allows the maintenance of variation even in small populations. 6.2 Optimising the code It is time for a short technical digression. You may have noticed that running the function multiple_traits_2() it is quite time consuming with a population of 1000 individuals. There is a quick way to check the exact time needed, using the function Sys.time(), which return your system current time. Let’s run the function again and calcualte how long it takes. start_time &lt;- Sys.time() data_model &lt;- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01) end_time &lt;- Sys.time() end_time - start_time ## Time difference of 49.01696 secs On a common laptop, it may take more than 30 seconds. To store the output, we are using a tibble with \\(1100000\\) observations, as max_traits is equal to \\(1100\\), which needs to be updated, in the right position, at each generation. A possiblity to speed up the simulation is to record our output in a different data structure. So far, we have been using tibbles to store the output. R, as all programming languages, can store data in different structures. Depending on what the data are and on what one wants to do with them, different structures can be more or less suitable. So far, we used tibbles for the important data of our simulations, such as population or output. Tibbles can have heterogeneous data, depending on what we need to store: for example, in our output tibble, the \\(trait\\) column was spacified as a factor, whereas the others two columns, \\(generation\\) and \\(p\\), were numeric. An alternative is to use vectors—and matrices, which can be thought as 2-dimensional vectors— that are basic structures in R. Differently from tibbles, in a matrix all data should be of the same type, in our case numeric (so we need to be careful to remember what numbers represent), but they may be convenient to use as the calculations performed on them are efficient, so that when we run models that are more complicated, or when we will need to produce bigger outputs, our simulations will still be relatively fast. We can rewrite a function that runs exactly the same simulation, but using matrices instead of tibbles. The output is now a matrix with \\(t_\\text{max}\\) rows and max_traits columns, intialised with NAs at the beginning, and the population is a vector of integers, representing the trait for each individual. multiple_traits_matrix &lt;- function(N, t_max, mu) { max_traits &lt;- N + N * mu * t_max output &lt;- matrix(data = NA, nrow = t_max, ncol = max_traits) # create first generation population &lt;- sample(1:N, N, replace = TRUE) output[1, ] &lt;- hist(population, 0:max_traits, plot = FALSE)$density # add first generation&#39;s p for all traits for (t in 2:t_max) { last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- sample(previous_population, N, replace = TRUE) # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if ((last_trait + sum(innovators)) &lt; max_traits) { population[innovators] &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } output[t, ] &lt;- hist(population, 0:max_traits, plot = FALSE)$density # get p for all traits and put it into output slot for this generation t } output # export data } To plot the output, we re-transform it in a tibble, so that can be handled by ggplot(). In order to do this, we first create a column that explicitly indicates the number of generations, and then we use the function gather() to reassemble the columns of the matrix in key-value pairs. plot_multiple_traits_matrix &lt;- function(data_model) { generation &lt;- rep(1:dim(data_model)[1], dim(data_model)[2]) data_to_plot &lt;- as_tibble(data_model) %&gt;% gather( key = &quot;trait&quot;, value = &quot;p&quot;) %&gt;% add_column(generation) ggplot(data = data_to_plot, aes(y = p, x = generation)) + geom_line(aes(colour = trait)) + ylim(c(0, 1)) + theme_bw() + theme(legend.position = &quot;none&quot;) } We can now run the function calculating the time needed, and then plot the results, to see if they look the same. start_time &lt;- Sys.time() data_model &lt;- multiple_traits_matrix(N = 100, t_max = 1000, mu = 0.01) end_time &lt;- Sys.time() plot_multiple_traits_matrix(data_model) ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. end_time - start_time ## Time difference of 0.872508 secs The results are equivalent, and the simulations is almost 100 times faster! This shows that implementation details are very important when building individual based models. When one needs to run several times the same simulation, or testing many different parameter values, implementation choices can make drastic differences. 6.3 The distribution of popularity An interesting aspect of these simulations is that, even if all traits are equal and individuals are not biased, few traits, for random reasons, are more successful than the majority of the others. A way to visualise this is to plot their cumulative popularity, that is the sums of their quantities over all generations. Given our matrix, it is easy to calculate them by summing each column and multiplying by N (remember they are frequencies, whereas we want now to visualise their actual quantities). cumulative &lt;- colSums(data_model) * N Let’s sort them from the more to the least popular and plot the results. data_to_plot &lt;- tibble(cumulative = sort(cumulative, decreasing = TRUE)) ggplot(data = data_to_plot, aes(x = seq_along(cumulative), y = cumulative)) + geom_point() + theme_bw() + labs(x = &quot;trait label&quot;, y = &quot;cumulative popularity&quot;) This is an example of a long-tailed distribution. The great majority of traits did not spread in the population, and their cumulative popularity is very close to one. Very few of them—the ones on the left side of the plot—were instead succesfull. Long-tailed distributions like the one we just produced are very common for cultural traits: very few movies, books, first names are very popular, while the great majority is not. In addition, in these domains, the popular traits are much more popular than the unpopular ones. The averge cumulative popularity is mean(cumulative), but the most succesfull trait has a popularity of max(cumulative). It is common to plot these distributions by binning the data in intervals of exponentially increasing size. In other words, we want to know how many traits have a cumulative popularity between 1 and 2, than between 2 and 4, than between 4 and 8, and so on, until we reach the maxmimum value of cumulative popularity. The code below does that, using a for cycle find how many traits fall in each bins and further normalising according to bin size. The size is increased 50 times, until an arbitrary maximum bin size of \\(2^{50}\\), to be sure to include all cumulative popularities. (We will get rid of the empty bins before plotting.) bin &lt;- rep(NA, 50) x &lt;- rep(NA, 50) for( i in 1:50 ){ bin[i] &lt;- sum( cumulative &gt;= 2^(i-1) &amp; cumulative &lt; 2^i) bin[i] &lt;- ( bin[i] / length( cumulative ) ) / 2^(i-1); x[i] &lt;- 2^i } We can now visualise the data on a log-log plot (after filtering out the empty bins). A log-log plot is a graph that uses logarithmic scales on both axes. Using logarithmic axes is useful when, as in this case, the data are skewed towards large values. In the previous plot, we were not able to appreciate visually any difference in the great majority of data points, for example points that had cumulative popularity between 1 and 10, as they were all tangled close to the x-axis. data_to_plot &lt;- tibble(bin = bin, x = x) data_to_plot &lt;- filter(data_to_plot, bin &gt; 0) ggplot(data = data_to_plot, aes(x = x, y = bin)) + geom_point() + labs(x = &quot;cumulative popularity&quot;, y = &quot;proportion of traits&quot;) + scale_x_log10() + scale_y_log10() + stat_smooth(method = &quot;lm&quot;) + theme_bw() In addition, on a log-log scale, the distribution of cumulative popularity produced by unbiased copying lies approximatively on a straigh line (we plotted a linear fitting of the data with the command stat_smooth(method = &quot;lm&quot;)). The goodness of fit and the slope of the line can be used to compare different models of cultural transmission (what would happen with conformity? What with a model bias? And so on…) and to match them with empirical data to develop hypotheses on the processes that generated these distributions in the real world. 6.4 Summary of the model An interesting insight we have from our simulations is that these extreme distributions do not necessarly result from 6.5 Analytical appendix 6.6 Further readings "]
]
