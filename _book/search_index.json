[
["index.html", "Individual-based models of cultural evolution A step-by-step guide using R Introduction", " Individual-based models of cultural evolution A step-by-step guide using R Alberto Acerbi Alex Mesoudi Marco Smolla 2019-09-30 Introduction TO DO "],
["unbiased-transmission.html", "1 Unbiased transmission 1.1 Initialising the simulation 1.2 Execute generation turn-over many times 1.3 Plotting the model results 1.4 Write a function to wrap the model code 1.5 Run several independent simulations and plot their results 1.6 Varying initial conditions 1.7 Summary of the model 1.8 Analytical appendix 1.9 Further reading", " 1 Unbiased transmission We start by simulating a simple case of unbiased cultural transmission. We will detail each step of the simulation and explain the code line-by-line. In the following chapters, we will reuse most of this initial model, building up the complexity of our simulations. 1.1 Initialising the simulation Here we will simulate a case where \\(N\\) individuals each possess one of two mutually exclusive cultural traits. These alternative traits are denoted \\(A\\) and \\(B\\). For example, \\(A\\) might be eating a vegetarian diet, and \\(B\\) might be eating a non-vegetarian diet. In reality, traits are seldom clear-cut (e.g. what about pescatarians?), but models are designed to cut away all the complexity to give tractable answers to simplified situations. Our model has non-overlapping generations. In each generation, all \\(N\\) individuals are replaced with \\(N\\) new individuals. Again, this is unlike any real biological group, but provides a simple way of simulating change over time. Generations here could correspond to biological generations, but could equally be ‘cultural generations’ (or learning episodes), which might be much shorter. Each new individual of each new generation picks a member of the previous generation at random and copies their cultural trait. This is known as unbiased oblique cultural transmission. ‘Unbiased’ refers to the fact that traits are copied entirely at random. The term ‘oblique’ means that members of one generation learn from those of the previous, non-overlapping, generation. This is different from, for example, horizontal cultural transmission, where individuals copy members of the same generation, and vertical cultural transmission, where offspring copy their biological parents. If we assume that the two cultural traits are transmitted in an unbiased way, what does that mean for the average trait frequency in the population? To answer this question, we must track the proportion of individuals who possess trait \\(A\\) over successive generations. We will call this proportion \\(p\\). We could also track the proportion who possess trait \\(B\\), but this will always be \\(1 - p\\) given that the two traits are mutually exclusive. For example, if \\(70\\%\\) of the population have trait \\(A\\) \\((p=0.7)\\), then the remaining \\(30\\%\\) must have trait \\(B\\) (i.e. \\(1-p=1-0.7=0.3\\)). The output of the model will be a plot showing \\(p\\) over all generations up to the last generation. Generations (or time steps) are denoted by \\(t\\), where generation one is \\(t=1\\), generation two is \\(t=2\\), up to the last generation \\(t=t_{\\text{max}}\\). First, we need to specify the fixed parameters of the model. These are quantities that we decide on at the start, and do not change during the simulation. In this model these are \\(N\\) (the number of individuals) and \\(t_{\\text{max}}\\) (the number of generations). Let’s start with \\(N=100\\) and \\(t_{\\text{max}}=200\\): N &lt;- 100 t_max &lt;- 200 Now we need to create our individuals. The only information we need to keep about our individuals is their cultural trait (\\(A\\) or \\(B\\)). We’ll make population the data structure containing the individuals. The type of data structure we have chosen here is a tibble. This is a more user-friendly version of a dataframe. Initially, we’ll give each individual either an \\(A\\) or \\(B\\) at random, using the sample() command. This can be seen in the code chunk below. The sample() command takes three arguments (i.e. inputs or options). The first argument lists the elements to pick at random, in our case, the traits \\(A\\) and \\(B\\). The second argument gives the number of times to pick, in our case \\(N\\) times, once for each individual. The final argument says to replace or reuse the elements specified in the first argument after they’ve been picked (otherwise there would only be one copy of \\(A\\) and one copy of \\(B\\), so we could only give two individuals traits before running out). Within the tibble command, the word \\(trait\\) denotes the name of the variable within the tibble that contains the random \\(A\\)s and \\(B\\)s, and the whole tibble is assigned the name population. There are two lines before sample() in the chunk below. First, we need to call the tidyverse library. We will use this throughout the chapter. Here, it allows us to create a tibble using the tibble command. Second, we use the function set.seed() to set the seed of the random number generator. This ensures that the results shown in the book are the same you obtain in your session. The number \\(111\\) has no special meaning. You can replace it with any other number and you will see how the traits of the individuals will be created differently. library(tidyverse) set.seed(111) population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) We can see the cultural traits of our population by simply entering its name in the R console: population ## # A tibble: 100 x 1 ## trait ## &lt;chr&gt; ## 1 B ## 2 B ## 3 A ## 4 B ## 5 A ## 6 A ## 7 A ## 8 B ## 9 A ## 10 A ## # … with 90 more rows As expected, there is a single column called \\(trait\\) containing \\(A\\)s and \\(B\\)s. The type of the column, in this case ‘chr’ (i.e. character), is reported below the name. A specific individual’s trait can be retrieved using the square bracket notation in R. For example, individual 4’s trait can be retrieved by typing: population$trait[4] ## [1] &quot;B&quot; This should match the fourth row in the table above. We also need a tibble to record the output of our simulation, that is, to track the trait frequency \\(p\\) in each generation. This will have two columns with \\(t_{\\text{max}}\\) rows, one row for each generation. The first column is simply a counter of the generations, from 1 to \\(t_{\\text{max}}\\). This will be useful to plot the output later. The other column contains the values of \\(p\\) for each generation. At this stage we don’t know what \\(p\\) will be in each generation, so for now let’s fill the output tibble with lots of NAs, which is R’s symbol for Not Available, or missing value. We can use the rep() (repeat) command to repeat NA \\(t_{\\text{max}}\\) times. We’re using NA rather than, say, zero, because zero could be misinterpreted as \\(p = 0\\), which would mean that all individuals have trait \\(B\\). This would be misleading, because at the moment we haven’t yet calculated \\(p\\), so it’s nonexistent, rather than zero. output &lt;- tibble(generation = 1:t_max, p = rep(NA, t_max)) We can, however, fill in the first value of \\(p\\) for our already-created first generation of individuals, held in population. The command below sums the number of \\(A\\)s in population and divides by \\(N\\) to get a proportion out of 1 rather than an absolute number. It then puts this proportion in the first slot of \\(p\\) in output, the one for the first generation, \\(t=1\\). We can again write the name of the tibble, output, to see that it worked. output$p[1] &lt;- sum(population$trait == &quot;A&quot;) / N output ## # A tibble: 200 x 2 ## generation p ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.51 ## 2 2 NA ## 3 3 NA ## 4 4 NA ## 5 5 NA ## 6 6 NA ## 7 7 NA ## 8 8 NA ## 9 9 NA ## 10 10 NA ## # … with 190 more rows This first value of \\(p\\) is 0.51, meaning that 51 individuals have trait \\(A\\), and 49 have trait \\(B\\). Even though sample() gives equal probability of picking each trait, this does not necessarily mean that we will get exactly 50 \\(A\\)s and 50 \\(B\\)s. This happens with simulations and finite population sizes: they are probabilistic (or stochastic), not deterministic. Analogously, flipping a coin 100 times will not always give exactly 50 heads and 50 tails. Sometimes we will get 51 heads, sometimes 49, etc. To see this in our simulation, re-run the above code with different values of the random number seed. 1.2 Execute generation turn-over many times Now that we have built the population, we can simulate what individuals do in each generation. We iterate these actions over \\(t_{\\text{max}}\\) generations. In each generation, we need to: copy the current individuals to a separate tibble called previous_population to use as demonstrators for the new individuals; this allows us to implement oblique transmission with its non-overlapping generations, rather than mixing up the generations create a new generation of individuals, each of whose trait is picked at random from the previous_population tibble calculate \\(p\\) for this new generation and store it in the appropriate slot in output To iterate, we’ll use a for-loop, using \\(t\\) to track the generation. We’ve already done generation 1 so we’ll start at generation 2. The random picking of models is done with sample() again, but this time picking from the traits held in previous_population. Note that we have added comments briefly explaining what each line does. This is perhaps superfluous when the code is this simple, but it’s always good practice. Code often gets cut-and-pasted into other places and loses its context. Explaining what each line does lets other people - and a future, forgetful you - know what’s going on. for (t in 2:t_max) { previous_population &lt;- population # copy the population tibble to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals output$p[t] &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into the output slot for this generation t } Now we should have 200 values of \\(p\\) stored in output, one for each generation. You can list them by typing output, but more effective is to plot them. 1.3 Plotting the model results We use ggplot() to plot our data. The syntax of ggplot may be slightly obscure at the beginning, but it forces us to have a clear picture of the data before plotting. In the first line in the code below, we are telling ggplot that the data we want to plot are in the tibble output. Then, with the command aes() we declare the ‘aesthetics’ of the plot, that is, how we want our data mapped in our plot. In this case, we want the values of \\(p\\) on the y-axis, and the values of \\(generation\\) on the x-axis (this is why we created earlier, in the tibble output, a column to keep the count of generations). We then use geom_line(). In ggplot, ‘geoms’ describe what kind of visual representation should be plotted: lines, bars, boxes and so on. This visual representation is independent from the mapping that we declared before with aes(). The same data, with the same mapping, can be visually represented in many different ways. In this case, we are asking ggplot to represent the data as a line. You can change geom_line() in the code below to geom_point(), and see what happens (other geoms have less obvious effects, and we will see some of them in the later chapters). The other commands are mainly to make the plot look nicer. We want the y-axis to span all the possible values of \\(p\\), from 0 to 1, and we use a particular ‘theme’ for our plot, in this case a standard theme with white background. With the command labs() we give a more informative label to the y-axis (ggplot automatically labels the axis with the name of the tibble columns that are plotted: this is good for \\(generation\\), but less so for \\(p\\)). ggplot(data = output, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) The proportion of individuals with trait \\(A\\) starts off hovering around 0.5, and after around 120 generations drops to 0. Unbiased transmission, or random copying, is by definition random, so different runs of this simulation will generate different plots. If you rerun all the code, trying different seeds of the random number generator, you will get something different. In all likelihood, \\(p\\) might go to 0 or 1 at some point. At \\(p = 0\\) there are no \\(A\\)s and every individual possesses \\(B\\). At \\(p = 1\\) there are no \\(B\\)s and every individual possesses \\(A\\). This is a typical feature of cultural drift, analogous to genetic drift: in small populations, with no selection or other directional processes operating, traits can be lost purely by chance after some generations. 1.4 Write a function to wrap the model code Ideally we would like to repeat the simulation to explore this idea in more detail, perhaps changing some of the parameters. For example, if we increase \\(N\\), are we more or less likely to lose one of the traits? As noted above, individual based models like this one are probabilistic or stochastic, thus it is essential to run simulations many times to understand what happens. With our code scattered about in chunks, it is hard to quickly repeat the simulation. Instead we can wrap it all up in a function: unbiased_transmission_1 &lt;- function(N, t_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) output &lt;- tibble(generation = 1:t_max, p = rep(NA, t_max)) output$p[1] &lt;- sum(population$trait == &quot;A&quot;) / N for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output$p[t] &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t } output } This is just all of the code snippets that we already ran above, but all within a function with parameters \\(N\\) and \\(t_{\\text{max}}\\) as arguments to the function. In addition, unbiased_transmission_1() ends with the line output. This means that this tibble will be exported from the function when it is run. This is useful for storing data from simulations wrapped in functions, otherwise that data is lost after the function is executed. Nothing will happen when you run the above code, because all you’ve done is define the function, not actually run it. The point is that we can now call the function in one go, easily changing the values of \\(N\\) and \\(t_{\\text{max}}\\). Let’s try first with the same values of \\(N\\) and \\(t_{\\text{max}}\\) as before, and save the output from the simulation into data_model, as a record of what happened. data_model &lt;- unbiased_transmission_1(N = 100, t_max = 200) We also need to create another function to plot the data, so we do not need to rewrite all the plotting instructions each time. Whereas this may seem impractical now, it is convenient to separate the function that runs the simulation and the function that plots the data for various reasons. With more complicated models, we do not want to rerun a simulation just because we want to change some detail in the plot. It also makes conceptual sense to keep separate the raw output of the model from the various ways we can visualise it, or the further analysis we want to perform on it. As above, the code is identical to what we already wrote: plot_single_run &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) } At this point, we can visualise the results: plot_single_run(data_model) As anticipated, the plot is different from the simulation we ran before, even though the code is exactly the same. This is due to the stochastic nature of the simulation. Assuming you haven’t changed the set.seed value earlier, this time trait \\(A\\) reaches a frequency of 1, meaning that all individuals have it, and trait \\(B\\) has disappeared. Now let’s try changing the parameters. We can call the simulation and the plotting functions together. The code below reruns and plots the simulation with a much larger \\(N\\). data_model &lt;- unbiased_transmission_1(N = 10000, t_max = 200) plot_single_run(data_model) You should see much less fluctuation. Rarely in a population of \\(N = 10000\\) will either trait go to fixation. Try re-running the previous code chunk to explore the effect of \\(N\\) on long-term dynamics. 1.5 Run several independent simulations and plot their results Wrapping a simulation in a function like this is good because we can easily re-run it with just a single command. However, it’s a bit laborious to manually re-run it. Say we wanted to re-run the simulation 10 times with the same parameter values to see how many times \\(A\\) goes to fixation, and how many times \\(B\\) goes to fixation. Currently, we’d have to manually run the unbiased_transmission_1() function 10 times and record somewhere else what happened in each run. It would be better to automatically re-run the simulation several times and plot each run as a separate line on the same plot. We could also add a line showing the mean value of \\(p\\) across all runs. Let’s use a new parameter \\(r_{\\text{max}}\\) to specify the number of independent runs, and use another for-loop to cycle over the \\(r_{\\text{max}}\\) runs. Let’s rewrite the unbiased_transmission_1() function to handle multiple runs. We will call the new function unbiased_transmission_2(). unbiased_transmission_2 &lt;- function(N, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { # for each run population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE)) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } There are a few changes here. First, we need a different output tibble, because we need to store data for all the runs. For that, we initialise the same \\(generation\\) and \\(p\\) columns as before, but with space for all the runs. \\(generation\\) is now built by repeating the count of each generation \\(r_{\\text{max}}\\) times, and \\(p\\) is NA repeated for all generations, for all runs. We also need a new column called \\(run\\) that keeps track of which run the data in the other two columns belongs to. Note that the definition of \\(run\\) is preceded by as.factor(). This specifies the type of data to put in the \\(run\\) column. We want \\(run\\) to be a ‘factor’ or categorical variable so that, even if runs are labeled with numbers (1, 2, 3…), this should not be misinterpreted as a continuous, real number: there is no sense in which run 2 is twice as ‘runny’ as run 1, or run 3 half as ‘runny’ as run 6. Runs could equally have been labelled using letters, or any other arbitrary scheme. While omitting as.factor() does not make any difference when running the simulation, it would create problems when plotting the data because ggplot would treat runs as continuous real numbers rather than discrete categories (you can see this yourself by modifying the definition of output in the previous code chunk). This is a good example of how it is important to have a clear understanding of your data before trying to plot or analyse them. Going back to the function, we then set up an \\(r\\) loop, which executes once for each run. The code within this loop is mostly the same as before, except that we now use the [output$generation == t &amp; output$run == r, ] notation to put \\(p\\) into the right place in output. The plotting function is also changed to handle multiple runs: plot_multiple_runs &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line(aes(colour = run)) + stat_summary(fun.y = mean, geom = &quot;line&quot;, size = 1) + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) } To understand how the above code works, we need to explain the general functioning of ggplot. As explained above, aes() specifies the ‘aesthetics’, or how the data are mapped in the plot. This is independent from the possible visual representations of this mapping, or ‘geoms’. If we declare specific aesthetics when we call ggplot(), these aesthetics will be applied to all geoms we call afterwards. Alternatively, we can specify the aesthetics in the geom itself. For example this: ggplot(data = output, aes(y = p, x = generation)) + geom_line() is equivalent to this: ggplot(data = output) + geom_line(aes(y = p, x = generation)) We can use this property to make more complex plots. The plot created in plot_multiple_runs has a first geom, geom_line(). This inherits the aesthetics specified in the initial call to ggplot() but also has a new mapping specific to geom_line(), colour = run. This tells ggplot to plot each run line with a different colour. The following command, stat_summary(), calculates the mean of all runs. However, this only inherits the mapping specified in the initial ggplot() call. If in the aesthetic of stat_summary() we had also specified colour = run, it would separate the data by run, and it would calculate the mean of each run. This, though, is just the lines we have already plotted with the geom_line() command. For this reason, we did not put colour = run in the ggplot() call, only in geom_line(). As always, there are various ways to obtain the same result. This code: ggplot(data = output) + geom_line(aes(y = p, x = generation, colour = run)) + stat_summary(aes(y = p, x = generation), fun.y = mean, geom = &quot;line&quot;, size = 1) is equivalent to the code we wrapped in the function above. However, the original code is clearer, as it distinguishes the global mapping, and the mappings specific to each visual representation. stat_summary() is a generic ggplot function which can be used to plot different statistics to summarise our data. In this case, we want to calculate the mean on the data mapped in \\(y\\), we want to plot them with a line, and we want this line to be thicker than the lines for the single runs. The default line size for geom_line is 0.5, so size = 1 doubles the thickness. Let’s now run the function and plot the results for five runs with the same parameters we used at the beginning (\\(N=100\\) and \\(t_{\\text{max}}=200\\)): data_model &lt;- unbiased_transmission_2(N = 100, t_max = 200, r_max = 5) plot_multiple_runs(data_model) You should be able to see five independent runs of our simulation shown as regular thin lines, along with a thicker line showing the mean of these lines. Some runs have probably gone to 0 or 1, and the mean should be somewhere in between. The data is stored in data_model, which we can inspect by writing its name. data_model ## # A tibble: 1,000 x 3 ## generation p run ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 0.53 1 ## 2 2 0.61 1 ## 3 3 0.55 1 ## 4 4 0.47 1 ## 5 5 0.47 1 ## 6 6 0.37 1 ## 7 7 0.34 1 ## 8 8 0.4 1 ## 9 9 0.44 1 ## 10 10 0.52 1 ## # … with 990 more rows Now let’s run the unbiased_transmission_2() model with \\(N = 10000\\), to compare with \\(N = 100\\). data_model &lt;- unbiased_transmission_2(N = 10000, t_max = 200, r_max = 5) plot_multiple_runs(data_model) The mean line should be almost exactly at \\(p = 0.5\\) now, with the five independent runs fairly close to it. 1.6 Varying initial conditions Let’s add one final modification. So far the starting frequencies of \\(A\\) and \\(B\\) have been the same, roughly 0.5 each. But what if we were to start at different initial frequencies of \\(A\\) and \\(B\\)? Say, \\(p = 0.2\\) or \\(p = 0.9\\)? Would unbiased transmission keep \\(p\\) at these initial values, or would it go to \\(p = 0.5\\) as we have found so far? To find out, we can add another parameter, \\(p_0\\), which specifies the initial probability of an individual having an \\(A\\) rather than a \\(B\\) in the first generation. Previously this was always \\(p_0 = 0.5\\), but in the new function below we add it to the sample() function to weight the initial allocation of traits in \\(t = 1\\). unbiased_transmission_3 &lt;- function(N, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } unbiased_transmission_3() is almost identical to the previous function. The only changes are the addition of \\(p_0\\) as an argument to the function, and the \\(prob\\) argument in the sample() command. The \\(prob\\) argument gives the probability of picking each option, in our case \\(A\\) and \\(B\\), in the first generation. The probability of \\(A\\) is now \\(p_0\\), and the probability of \\(B\\) is now \\(1 - p_0\\). We can use the same plotting function as before to visualise the result. Let’s see what happens with a different value of \\(p_0\\), for example \\(p_0 = 0.2\\). data_model &lt;- unbiased_transmission_3(N = 10000, p_0 = 0.2, t_max = 200, r_max = 5) plot_multiple_runs(data_model) With \\(p_0 = 0.2\\), trait frequencies stay at \\(p = 0.2\\). Unbiased transmission is truly non-directional: it maintains trait frequencies at whatever they were in the previous generation, barring random fluctuations caused by small population sizes. 1.7 Summary of the model Even this extremely simple model provides some valuable insights. First, unbiased transmission does not in itself change trait frequencies. As long as populations are large, trait frequencies remain the same. Second, the smaller the population size, the more likely traits are to be lost by chance. This is a basic insight from population genetics, known there as genetic drift, but it can also be applied to cultural evolution. Many studies have tested (and some supported) the idea that population size and other demographic factors can shape cultural diversity. Furthermore, generating expectations about cultural change under simple assumptions like random cultural drift can be useful for detecting non-random patterns like selection. If we don’t have a baseline, we won’t know selection or other directional processes when we see them. We also have introduced several programming techniques that will be useful in later simulations. We have seen how to use tibbles to hold characteristics of individuals and the outputs of simulations, how to use loops to cycle through generations and simulation runs, how to use sample() to pick randomly from sets of elements, how to wrap simulations in functions to easily re-run them with different parameter values, and how to use ggplot() to plot the results of simulations. 1.8 Analytical appendix If \\(p\\) is the frequency of \\(A\\) in one generation, we are interested in calculating \\(p&#39;\\), the frequency of \\(A\\) in the next generation under the assumption of unbiased transmission. Each new individual in the next generation picks a demonstrator at random from among the previous generation. The demonstrator will have \\(A\\) with probability \\(p\\). The frequency of \\(A\\) in the next generation, then, is simply the frequency of \\(A\\) in the previous generation: \\[p&#39; = p \\hspace{30 mm}(1.1)\\] Equation 1.1 simply says that under unbiased transmission there is no change in \\(p\\) over time. If, as we assumed above, the initial value of \\(p\\) in a particular population is \\(p_0\\), then the equilibrium value of \\(p\\), \\(p^*\\), at which there is no change in \\(p\\) over time, is just \\(p_0\\). We can plot this recursion, to recreate the final simulation plot above: p_0 &lt;- 0.2 t_max &lt;- 200 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Here, we use a for loop to cycle through each generation, each time updating \\(p\\) according to the recursion equation above. Remember, there is no \\(N\\) here because the recursion is deterministic and assumes an infinite population size; hence there is no stochasticity due to finite population sizes. There is also no need to have multiple runs as each run is identical, hence no \\(r_{max}\\). Don’t worry, it gets more complicated than this in later chapters. The key point here is that analytical (or deterministic) models assume infinite populations and no stochasticity. Simulations with very large populations should give the same results as analytical models. Basically, the closer we can get in stochastic models to the assumption of infinite populations, the closer the match to infinite-population deterministic models. Deterministic models give the ideal case; stochastic models permit more realistic dynamics based on finite populations. More generally, creating deterministic recursion-based models can be a good way of verifying simulation models, and vice versa: if the same dynamics occur in both individuals-based and recursion-based models, then we can be more confident that those dynamics are genuine and not the result of a programming error or mathematical mistake. 1.9 Further reading Cavalli-Sforza, L. L., &amp; Feldman, M. W. (1981). Cultural transmission and evolution. Princeton: Princeton Univ. Press. Bentley, R. A., Hahn, M. W., &amp; Shennan, S. J. (2004). Random drift and culture change. Proceedings of the Royal Society of London B, 271(1547), 1443-1450. Lansing, J. S., &amp; Cox, M. P. (2011). The domain of the replicators: selection, neutrality, and cultural evolution. Current Anthropology, 52(1), 105-125. (plus associated commentaries) "],
["unbiased-and-biased-mutation.html", "2 Unbiased and biased mutation 2.1 Unbiased mutation 2.2 Biased mutation 2.3 Summary of the model 2.4 Analytical appendix 2.5 Further readings", " 2 Unbiased and biased mutation Evolution doesn’t work without a source of variation that introduces new variation upon which selection, drift and other processes can act. In genetic evolution, mutation is almost always blind with respect to function. Beneficial genetic mutations are no more likely to arise when they are needed than when they are not needed - in fact most genetic mutations are neutral or detrimental to an organism. Cultural evolution is more interesting, in that novel variation may sometimes be directed to solve specific problems, or systematically biased due to features of our cognition. In the models below we’ll simulate both unbiased and biased mutation. 2.1 Unbiased mutation First we will simulate unbiased mutation in the same basic model as used in the previous chapter. We’ll remove unbiased transmission to see the effect of unbiased mutation alone. As in the previous model, we assume \\(N\\) individuals each of whom possesses one of two non-overlapping cultural traits, denoted \\(A\\) and \\(B\\). In each generation, from \\(t = 1\\) to \\(t = t_{\\text{max}}\\), the \\(N\\) individuals are replaced with \\(N\\) new individuals. Instead of random copying, each individual now gives rise to a new individual with exactly the same cultural trait as them. (Another way of looking at this is in terms of timesteps, such as years: the same \\(N\\) individual live for \\(t_{\\text{max}}\\) years, and keep their cultural trait from one year to the next.) At each generation, however, there is a probability \\(\\mu\\) that each individual mutates from their current trait to the other trait (the Greek letter Mu is the standard notation for the mutation rate in genetic evolution, and it has an analogous function here). Vegetarian individuals can decide to eat animal products, and vice versa. Remember, this is not copied from other individuals, as in the previous model, but can be thought as an individual decision. another way to say it, is that the probability of changing trait applies to each individual independently; whether an individual mutates has no bearing on whether or how many other individuals have mutated. On average, that means that \\(\\mu N\\) individuals mutate each generation. Like in the previous model, we are interested in tracking the proportion \\(p\\) of agents with trait \\(A\\) over time. We’ll wrap this in a function called unbiased_mutation(), using much of the same code as unbiased_transmission_3(). As before, we need to call the tidyverse library, and set a seed for the random number genrator, so the results will be exactly the same each time we rerun the code. library(tidyverse) set.seed(111) unbiased_mutation &lt;- function(N, mu, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mutate &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # find &#39;mutant&#39; individuals if (nrow(population[mutate &amp; previous_population$trait == &quot;A&quot;, ]) &gt; 0) { # if there are &#39;mutant&#39; from A to B population[mutate &amp; previous_population$trait == &quot;A&quot;, ]$trait &lt;- &quot;B&quot; # then flip them to B } if (nrow(population[mutate &amp; previous_population$trait == &quot;B&quot;, ]) &gt; 0) { # if there are &#39;mutant&#39; from B to A population[mutate &amp; previous_population$trait == &quot;B&quot;, ]$trait &lt;- &quot;A&quot; # then flip them to A } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } The only changes from the previous model are the addition of \\(\\mu\\), the parameter that specifies the probability of mutation, in the function definition and new lines of code within the for loop on \\(t\\) which replace the random copying command with unbiased mutation. Let’s examine these lines to see how they work. The most obvious way of implementing unbiased mutation - which is not done above - would have been to set up another for loop. We would cycle through each individual one by one, each time calculating whether it should mutate or not based on \\(mu\\). This would certainly work, but R is notoriously slow at loops. It’s always preferable in R, where possible, to use ‘vectorised’ code. That’s what is done above in our three added lines, starting from mutate &lt;- sample(). First, we pre-specify the probability of mutating for each individual For that, we use again the function sample(), picking \\(TRUE\\) (corresponding to be a mutant) or \\(FALSE\\) (keeping the same cultural trait) for \\(N\\) times. The draw, however, is not random: the probability of drawing \\(TRUE\\) is equal to \\(\\mu\\), and the probability to draw \\(FALSE\\) is \\(1-\\mu\\). You can think about the procedure in this way: each individual in the population flips a biased coin that has \\(\\mu\\) probability to land on, say, head, and \\(1-\\mu\\) to land on tail. If it land on head they change their cultural trait. After that, in the following lines, we actually change the traits for the ‘mutant’ individuals. We need to check whether there are individuals that change their trait, both from \\(A\\) to \\(B\\) and viceversa, using the two if conditionals. This is because if there are not individuals changing their traits, we are trying to assign a new value to an empty tibble, and we get an error. To check, we make sure that the number of rows is higher than 0 (using nrow()&gt;0 within the if). To plot the results, we can use the same function plot_multiple_runs() we wrote in the previous chapter. Let’s now run and plot the model: data_model &lt;- unbiased_mutation(N = 100, mu = 0.05, p_0 = 0.5, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Unbiased mutation produces random fluctuations over time, and does not alter the overall frequency of \\(A\\), which stays around \\(p = 0.5\\). Because mutations from \\(A\\) to \\(B\\) are as equally likely as \\(B\\) to \\(A\\), there is no overall directional trend. If you remember from the previous chapter, with unbiased transmission, instead, when populations where small generally one of the trait was disappearing after some generations. Why this difference? You can think it in this way: when one trait becomes popular, say the frequency of \\(A\\) is equal to \\(0.8\\), with unbiased transmission it is more likely that individuals of the new generation will pick up randomly \\(A\\) when copying. The few individuals will trait \\(B\\) will have 80% of probability to copy \\(A\\). With unbiased mutation, instead, since \\(\\mu\\) is applied independently to each individual, when \\(A\\) is popular there will be more individuals that will flip to \\(B\\) (namely, \\(\\mu p N\\), in our case 4) than individual that will flip to \\(A\\) (equal to \\(\\mu (1-p) N\\), in our case 1) keeping the traits at similar freuqencies. But what if we were to start at different initial frequencies of \\(A\\) and \\(B\\)? Say, \\(p=0.1\\) and \\(p=0.9\\)? Would \\(A\\) disappear? Would unbiased mutation keep \\(p\\) at these initial values, like we saw unbiased transmission does in Model 1? To find out, let’s change \\(p_0\\), which, as you may recall, specifies the initial probability of drawing an \\(A\\) rather than a \\(B\\) in the first generation. data_model &lt;- unbiased_mutation(N = 100, mu = 0.05, p_0 = 0.1, t_max = 200, r_max = 5) plot_multiple_runs(data_model) You should see \\(p\\) go from 0.1 up to 0.5. In fact, whatever the initial starting frequencies of \\(A\\) and \\(B\\), unbiased mutation always leads to \\(p = 0.5\\), for the reason explained above: unbiased mutation always tends to balance the proportion of \\(A\\)s and \\(B\\)s. 2.2 Biased mutation A more interesting case is biased mutation. Let’s assume now that there is a probability \\(\\mu_b\\) that an individual with trait \\(B\\) mutates into \\(A\\), but there is no possibility of trait \\(A\\) mutating into trait \\(B\\). Perhaps trait \\(A\\) is a particularly catchy or memorable version of a story, or an intuitive explanation of a phenomenon, and \\(B\\) is difficult to remember or unintuitive. The function biased_mutation() captures this unidirectional mutation. biased_mutation &lt;- function(N, mu_b, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) # create the output tibble for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble mutate &lt;- sample(c(TRUE, FALSE), N, prob = c(mu_b, 1 - mu_b), replace = TRUE) # find &#39;mutant&#39; individuals if (nrow(population[mutate &amp; previous_population$trait == &quot;B&quot;, ]) &gt; 0) { population[mutate &amp; previous_population$trait == &quot;B&quot;, ]$trait &lt;- &quot;A&quot; # if individual was B and mutates, flip to A } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } There are just two changes in this code compared to unbiased_mutation(). First, we’ve replaced \\(\\mu\\) with \\(\\mu_b\\) to keep the two parameters distinct and avoid confusion. Second, the line in unbiased_mutation() which caused individuals with \\(A\\) to mutate to \\(B\\) has been deleted. Let’s see what effect this has by running biased_mutation(). We’ll start with the population entirely composed of individuals with \\(B\\), i.e. \\(p_0 = 0\\), to see how quickly and in what manner \\(A\\) spreads via biased mutation. data_model &lt;- biased_mutation(N = 100, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) The plot shows a steep increase that slows and plateaus at \\(p = 1\\) by around generation \\(t = 100\\). There should be a bit of fluctuation in the different runs, but not much. Now let’s try a larger sample size. data_model &lt;- biased_mutation(N = 10000, mu_b = 0.05, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) With \\(N = 10000\\) the line should be smooth with little (if any) fluctuation across the runs. But notice that it plateaus at about the same generation, around \\(t = 100\\). Population size has little effect on the rate at which a novel trait spreads via biased mutation. \\(\\mu_b\\), on the other hand, does affect this speed. Let’s double the biased mutation rate to 0.1. data_model &lt;- biased_mutation(N = 10000, mu_b = 0.1, p_0 = 0, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Now trait \\(A\\) reaches fixation around generation \\(t = 50\\). Play around with \\(N\\) and \\(\\mu_b\\) to confirm that the latter determines the rate of diffusion of trait \\(A\\), and that it takes the same form each time - roughly an ‘r’ shape with an initial steep increase followed by a plateauing at \\(p = 1\\). 2.3 Summary of the model With this simple model we can draw the following insights. Unbiased mutation, which resembles genetic mutation in being non-directional, always leads to an equal mix of the two traits. It introduces and maintains cultural variation in the population. It is interesting to compare unbiased mutation to unbiased transmission from Model 1. While unbiased transmission did not change \\(p\\) over time, unbiased mutation always converges on \\(p^* = 0.5\\), irrespective of the starting frequency. (NB \\(p^* = 0.5\\) assuming there are two traits; more generally, \\(p^* = 1/v\\), where \\(v\\) is the number of traits.) Biased mutation, which is far more common - perhaps even typical - in cultural evolution, shows different dynamics. Novel traits favoured by biased mutation spread in a characteristic fashion - an r-shaped diffusion curve - with a speed characterised by the mutation rate \\(\\mu_b\\). Population size has little effect, whether \\(N = 100\\) or \\(N = 10000\\). Whenever biased mutation is present (\\(\\mu_b &gt; 0\\)), the favoured trait goes to fixation, even if it is not initially present. In terms of programming techniques, the major novelty in Model 2 is the use of sample() to determine which individuals should undergo whatever the fixed probability specifies (in our case, mutation). This could be done with a loop, but vectorising code in the way we did here is much faster in R than loops. 2.4 Analytical appendix If \\(p\\) is the frequency of \\(A\\) in one generation, we are interested in calculating \\(p&#39;\\), the frequency of \\(A\\) in the next generation under the assumption of unbiased mutation. The next generation retains the cultural traits of the previous generation, except that \\(\\mu\\) of them switch to the other trait. There are therefore two sources of \\(A\\) in the next generation: members of the previous generation who had \\(A\\) and didn’t mutate, therefore staying \\(A\\), and members of the previous generation who had \\(B\\) and did mutate, therefore switching to \\(A\\). The frequency of \\(A\\) in the next generation is therefore: \\[p&#39; = p(1-\\mu) + (1-p)\\mu \\hspace{30 mm}(2.1)\\] The first term on the right-hand side of Equation 2.1 represents the first group, the \\((1 - \\mu)\\) proportion of the \\(p\\) \\(A\\)-carriers who didn’t mutate. The second term represents the second group, the \\(\\mu\\) proportion of the \\(1 - p\\) \\(B\\)-carriers who did mutate. To calculate the equilibrium value of \\(p\\), \\(p^*\\), we want to know when \\(p&#39; = p\\), or when the frequency of \\(A\\) in one generation is identical to the frequency of \\(A\\) in the next generation. This can be found by setting \\(p&#39; = p\\) in Equation 2.1, which gives: \\[p = p(1-\\mu) + (1-p)\\mu \\hspace{30 mm}(2.2)\\] Rearranging Equation 2.2 gives: \\[\\mu(1 - 2p) = 0 \\hspace{30 mm}(2.3)\\] The left-hand side of Equation 2.3 equals zero when either \\(\\mu = 0\\), which given our assumption that \\(\\mu &gt; 0\\) cannot be the case, or when \\(1 - 2p = 0\\), which after rearranging gives the single equilibrium \\(p^* = 0.5\\). This matches our simulation results above. As we found in the simulations, this does not depend on \\(\\mu\\) or the starting frequency of \\(p\\). We can also plot the recursion in Equation 2.1 like so: p_0 &lt;- 0 t_max &lt;- 200 mu &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] * (1 - mu) + (1 - pop_analytical$p[i - 1]) * mu } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Again, this should resemble the figure generated by the simulations above, and confirm that \\(p^* = 0.5\\). For biased mutation, assume that only \\(B\\)s are switching to \\(A\\), and with probability \\(\\mu_b\\) instead of \\(\\mu\\). The first term on the right hand side becomes simply \\(p\\), because \\(A\\)s do not switch. The second term remains the same, but with \\(\\mu_b\\). Thus, \\[p&#39; = p + (1-p)\\mu_b \\hspace{30 mm}(2.4)\\] The equilibrium value \\(p^*\\) can be found by again setting \\(p&#39; = p\\) and solving for \\(p\\). Assuming \\(\\mu_b &gt; 0\\), this gives the single equilibrium \\(p^* = 1\\), which again matches the simulation results. We can plot the above recursion like so: p_0 &lt;- 0 t_max &lt;- 200 mu_b &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] + (1 - pop_analytical$p[i - 1]) * mu_b } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) Hopefully, this looks identical to the final simulation plot with the same value of \\(\\mu_b\\). Furthermore, we can specify an equation for the change in \\(p\\) from one generation to the next, or \\(\\Delta p\\). We do this by subtracting \\(p\\) from both sides of Equation 2.4, giving: \\[\\Delta p = p&#39; - p = (1-p)\\mu_b \\hspace{30 mm}(2.5)\\] Seeing this helps explain two things. First, the \\(1 - p\\) part explains the r-shape of the curve. It says that the smaller is \\(p\\), the larger \\(\\Delta p\\) will be. This explains why \\(p\\) increases in frequency very quickly at first, when \\(p\\) is near zero, and the increase slows when \\(p\\) gets larger. We have already determined that the increase stops altogether (i.e. \\(\\Delta p\\) = 0) when \\(p = p^* = 1\\). Second, it says that the rate of increase is proportional to \\(\\mu_b\\). This explains our observation in the simulations that larger values of \\(\\mu_b\\) cause \\(p\\) to reach its maximum value faster. 2.5 Further readings "],
["biased-transmission-direct-bias.html", "3 Biased transmission (direct bias) 3.1 Strenght of selection 3.2 Summary of the model 3.3 Analytical appendix 3.4 Further readings", " 3 Biased transmission (direct bias) So far we have looked at unbiased transmission (Chapter 1) and mutation, both unbiased and biased (Chapter 2). Let’s complete the set by looking at biased transmission. This occurs when one trait or is more likely to be copied than another trait. When the choice depends on features of the trait, it is often called ‘direct’ or ‘content’ bias, while when the choice depends on features of the demonstrators is often called ‘indirect’ or ‘context’ bias. Both are sometimes also called ‘cultural selection’ because the trait is selected to be copied over another. In this chapter, we will look at trait-based (direct, content) bias. (As an aside, there is a confusing array of terminology in the field of cultural evolution, as illustrated by the preceding paragraph. That’s why models are so useful. Words and verbal descriptions can be ambiguous. Often the writer doesn’t realise that there are hidden assumptions or unrecognised ambiguities in their descriptions. They may not realise that what they mean by ‘cultural selection’ is entirely different to how someone else uses it. Models are great because they force us to precisely specify exactly what we mean by a particular term or process. I can use the words in the paragraph above to describe biased transmission, but it’s only really clear when I model it, making all my assumptions explicit.) To simulate biased transmission, following the simulations in Chapter 1,we assume there are two traits \\(A\\) and \\(B\\), and that each individual chooses another individual from the previous generation at random. This time, however, we associate to the traits two different probabilities of being copied: we can call them, \\(s_a\\) and \\(s_b\\) respectively. When an individual encounter an individual with trait \\(A\\), they will copy them with probability \\(s_a\\), and when they encounter an individual with trait \\(B\\), they will copy them with probability \\(s_b\\). With \\(s_a=s_b\\), copying is unbiased, and individuals switch to the encountered alternative with the same probability. This reproduces the results of the simulations when transmission is unbiased. If \\(s_a=s_b=1\\), the model is exactly the same of Chapter 1. The relevant situation is when \\(s_a&gt;s_b\\) (or vice versa), so that we have biased transmission. Perhaps \\(A\\) (or \\(B\\)) is a more effective tool, a more memorable story, or a more easily pronounced word. Let’s first write the function, and then explore what happens in this case. Below is a function biased_ransmission_direct() that implements all of these ideas library(tidyverse) set.seed(111) biased_ransmission_direct &lt;- function (N, s_a, s_b, p_0, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble demonstrator_trait &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # for each individual, pick a random individual from the previous generation to act as demonstrator and store their trait # biased probabilities to copy: copy_a &lt;- sample(c(TRUE, FALSE), N, prob = c(s_a, 1 - s_a), replace = TRUE) copy_b &lt;- sample(c(TRUE, FALSE), N, prob = c(s_b, 1 - s_b), replace = TRUE) if (nrow(population[copy_a &amp; demonstrator_trait == &quot;A&quot;, ]) &gt; 0) { population[copy_a &amp; demonstrator_trait == &quot;A&quot;, ]$trait &lt;- &quot;A&quot; } if (nrow(population[copy_b &amp; demonstrator_trait == &quot;B&quot;, ]) &gt; 0) { population[copy_b &amp; demonstrator_trait == &quot;B&quot;, ]$trait &lt;- &quot;B&quot; } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } Most of biased_ransmission_direct() is recycled from the previous models. As before, we initalise the data structure output from multiple runs, and in generation \\(t = 1\\), we create a population tibble to hold the trait of each individual. The major change is that we now include biased transmission. We first select at random the demonstrators from the previous generation (using the same code we used in unbiased_transmission()) and we store their trait in demonstrator_trait. Then we get the probabilities to copy \\(A\\) and to copy \\(B\\) for all the population, using the same code used in biased_mutation(), only that this time it produces a probability to copy. Again using the same code as in biased mutation(), we have the individuals copy the trait at hand with the desired probability. Let’s run our function biased_ransmission_direct(). As before, to plot the results, we can use the same function plot_multiple_runs() we wrote in Chapter 1. As we discussed, the interesting case is when one trait is favored with respect to the other. We can assume, for example, \\(s_a=0.1\\) and \\(s_b=0\\). This means that when individuals encounter another individual with trait \\(A\\) they copy them 1 every 10 times, but, when individuals encounter another individual with trait \\(B\\), they never switch. We can also assume that the favoured trait, \\(A\\), is initially rare in the population (\\(p_0=0.01\\)) to see how selection favours this initially-rare trait (\\(p_0\\) needs to be higher than 0. Since we are re not including any mutation in the model, we need to include at least some \\(A\\)s at the beginning of the simulation, otherwise it would never appear). data_model &lt;- biased_ransmission_direct(N = 10000, s_a = 0.1, s_b = 0 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) With a moderate selection strength we can see that \\(A\\) gradually replaces \\(B\\) and goes to fixation. It does this in a characteristic manner: the increase is slow at first, then picks up speed, then plateaus. Note the difference to biased mutation. Where biased mutation was r-shaped, with a steep initial increase, biased transmission is s-shaped, with an initial slow uptake. This is because the strength of biased transmission (like selection in general) is proportional to the variation in the population. When \\(A\\) is rare initially, there is only a small chance of picking another individual with \\(A\\). As \\(A\\) spreads, the chances of picking an \\(A\\) individual increases. As \\(A\\) becomes very common, there are few \\(B\\) individuals left to switch. In the case of biased mutation, instead, the probability to switch is independent from the variation in the population. 3.1 Strenght of selection From what does the strength of the selection depend? First, the strength is independent from the specific values of \\(s_a\\) and \\(s_b\\). What counts is their relative difference, in this case \\(s_a-s_b = 0.1\\). We can try and run a simulation with, say, \\(s_a=0.6\\) and \\(s_b=0.5\\). We observe the same pattern, with slightly more noise - the single runs are more different one another with respect to the previous simulation - due to the fact that switches from \\(A\\) to \\(B\\) are also possible. data_model &lt;- biased_ransmission_direct(N = 10000, s_a = 0.6, s_b = 0.5 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) To change the selection strength, we need to modify the difference between \\(s_a\\) and \\(s_b\\). We can double the strength by setting \\(s_a = 0.2\\), and keeping \\(s_b=0\\). data_model &lt;- biased_ransmission_direct(N = 10000, s_a = 0.2, s_b = 0 , p_0 = 0.01, t_max = 150, r_max = 5) plot_multiple_runs(data_model) As we might expect, increasing the strength of selection increases the speed with which \\(A\\) goes to fixation. Note, though, that it retains the s-shape. 3.2 Summary of the model We saw how biased transmission causes a trait favoured by cultural selection to spread and go to fixation in a population, even when it is initially very rare. Biased transmission differs in its dynamics from biased mutation. Its action is proportional to the variation in the population at the time at which it acts. It is strongest when there is lots of variation (in our model, when there are equal numbers of \\(A\\) and \\(B\\) at \\(p = 0.5\\)), and weakest when there is little variation (when \\(p\\) is close to 0 or 1). 3.3 Analytical appendix As before, we have \\(p\\) individuals with trait \\(A\\), and \\(1 - p\\) individuals with trait \\(B\\). As we saw that what is important is the relative difference between the two probabilities of being copied associated to the two traits and not their absolute value, we consider always \\(s_b=0\\), and vary \\(s_a\\), which we can call simply \\(s\\). Thus, the \\(p\\) individuals with trait \\(A\\) always keep their \\(A\\)s. The \\(1 - p\\) individuals with trait \\(B\\) pick another individual at random, hence with probability \\(p\\), and with probability \\(s\\) they switch to trait \\(A\\). We can therefore write the recursion for \\(p\\) under biased transmission as: \\[p&#39; = p + p(1-p)s \\hspace{30 mm}(3.1)\\] The first term on the right-hand side is the unchanged \\(A\\) bearers, and the second term is the \\(1-p\\) \\(B\\)-bearers who find one of the \\(p\\) \\(A\\)-bearers and switch with probability \\(s\\). Here is some code to plot this biased transmission recursion: t_max &lt;- 150 s &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- 0.01 for (i in 2:t_max) { pop_analytical$p[i] &lt;- pop_analytical$p[i - 1] + pop_analytical$p[i - 1] * (1 - pop_analytical$p[i - 1]) * s } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) The curve above should be identical to the simulation curve, given that the simulation had the same biased transmission strength \\(s\\) and a large enough \\(N\\) to minimise stochasticity. From the equation above, we can see how the strength of biased transmission depends on variation in the population, given that \\(p(1 - p)\\) is the formula for variation. This determines the shape of the curve, while \\(s\\) determines the speed with which the equilibrium \\(p^*\\) is reached. But what is the equilibrium \\(p^*\\) here? In fact there are two. As before, the equilibrium can be found by setting the change in \\(p\\) to zero, or when: \\[p(1-p)s = 0 \\hspace{30 mm}(3.2)\\] There are three ways in which the left-hand side can equal zero: when \\(p = 0\\), when \\(p = 1\\) and when \\(s = 0\\). The last case is uninteresting: it would mean that biased transmission is not occurring. The first two cases simply say that if either trait reaches fixation, then it will stay at fixation. This is to be expected, given that we have no mutation in our model. It contrasts with unbiased and biased mutation, where there is only one equilibrium value of \\(p\\). We can also say that \\(p = 0\\) is an unstable equilibrium, meaning that any slight perturbation away from \\(p = 0\\) moves \\(p\\) away from that value. This is essentially what we simulated above: a slight perturbation up to \\(p = 0.01\\) went all the way up to \\(p = 1\\). In contrast, \\(p = 1\\) is a stable equilibrium: any slight perturbation from \\(p = 1\\) immediately goes back to \\(p = 1\\). 3.4 Further readings "],
["biased-transmission-indirect-bias-frequency.html", "4 Biased transmission (indirect bias: frequency) 4.1 The logic of conformity 4.2 Testing conformist transmission 4.3 Summary of the model 4.4 Analytical appendix 4.5 Further readings", " 4 Biased transmission (indirect bias: frequency) 4.1 The logic of conformity In Chapter 3 we looked at the case where one cultural trait is intrinsically more likely to be copied than another trait. Here we will start looking to the other kind of biased transmission, when traits are equivalent, but individuals are more likely to adopt a trait according to the characteristics of the population, the other individuals who already have it. (As we mentioned previously, these are often called ‘indirect’ or ‘context’ bias). A first possibility is that we may be influenced by the frequency of the trait in the population: how many other individuals already have the traits? Conformity (or ‘positive frequency dependent bias’) is the case most studied (the opposite case, anti-conformity or negative frequency dependent bias is also possible, even though probably more uncommon in real life). Here, individuals are disproportionately more likely to adopt the most common trait in the population, irrespective of its intrinsic characteristics. For example, imagine trait \\(A\\) has a frequency of 0.7 in the population, with the rest possessing trait \\(B\\). An unbiased learner would adopt trait \\(A\\) with a probability exactly equal to 0.7. This is unbiased transmission, and is what happens the model described in (Chapter 1: by picking a member of the previous generation at random, the probability of adoption is equal to the frequency of that trait among the previous generation. A conformist learner, on the other hand, would adopt trait \\(A\\) with a probability greater than 0.7. In other words, common traits get an ‘adoption boost’ relative to unbiased transmission. Uncommon traits get an equivalent ‘adoption penalty’. The magnitude of this boost or penalty can be controlled by a parameter, which we will call \\(D\\). Let’s keep things simple in our model. Rather than assuming that individuals sample across the entire population, which in any case might be implausible in large populations, let’s assume they pick only three demonstrators at random. Why three? This is the minimum number of demonstrators that can yield a majority (i.e. 2 vs 1), which we need in order to implement conformity. When two demonstrators have one trait and the other demonstrator has a different trait, we want to boost the probability of adoption for the majority trait, and reduce it for the minority trait. We can specify the probability of adoption as follows: Table 1: Probability of adopting trait \\(A\\) for each possible combination of traits amongst three demonstrators Demonstrator 1 Demonstrator 2 Demonstrator 3 Probability of adopting trait \\(A\\) \\(A\\) \\(A\\) \\(A\\) 1 \\(A\\) \\(A\\) \\(B\\) \\(A\\) \\(B\\) \\(A\\) \\(2/3 + D/3\\) \\(B\\) \\(A\\) \\(A\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(1/3 - D/3\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) 0 The first row says that when all demonstrators have trait \\(A\\), then trait \\(A\\) is definitely adopted. Similarly, the bottom row says that when all demonstrators have trait \\(B\\), then trait \\(A\\) is never adopted, and by implication trait \\(B\\) is always adopted. For the three combinations where there are two \\(A\\)s and one \\(B\\), the probability of adopting trait \\(A\\) is \\(2/3\\), which it would be under unbiased transmission (because two out of three demonstrators have \\(A\\)), plus the conformist adoption boost specified by \\(D\\). As \\(D\\) varies from 0 to 1 is divided by three, so that the maximum probability of adoption is equal to 1. Similarly, for the three combinations where there are two \\(B\\)s and one \\(A\\), the probability of adopting \\(A\\) is 1/3 minus the conformist adoption penalty specified by \\(D\\). Let’s implement these assumptions in the kind of individual based model we’ve been building so far. As before, assume \\(N\\) individuals each of whom possess one of two traits \\(A\\) or \\(B\\). The frequency of \\(A\\) is denoted by \\(p\\). The initial frequency of \\(A\\) in generation \\(t = 1\\) is \\(p_0\\). Rather than going straight to a function, let’s go step by step. First we’ll specify our parameters, \\(N\\) and \\(p_0\\) as before, plus the new conformity parameter \\(D\\). We also create the usual population tibble and fill it with \\(A\\)s and \\(B\\)s in the proportion specified by \\(p_0\\), again exactly as before. library(tidyverse) set.seed(111) N &lt;- 100 p_0 &lt;- 0.5 D &lt;- 1 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation Now we create another tibble, called demonstrators that picks, for each new individual in the next generation, three demonstrators at random from the current population of individuals. It therefore needs three columns/variables, one for each of the demonstrators, and \\(N\\) rows, one for each individual. We fill each column with randomly chosen traits from the population tibble. We can have a look at demonstrators entering its name in the R console. # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- tibble(dem1 = sample(population$trait, N, replace = TRUE), dem2 = sample(population$trait, N, replace = TRUE), dem3 = sample(population$trait, N, replace = TRUE)) demonstrators ## # A tibble: 100 x 3 ## dem1 dem2 dem3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B B B ## 2 B A A ## 3 A B A ## 4 A B B ## 5 A B B ## 6 A B B ## 7 A B B ## 8 A B A ## 9 A A A ## 10 A B B ## # … with 90 more rows Think of each row here as containing the traits of three randomly-chosen demonstrators chosen by each new next-generation individual. Now we want to calculate the probability of adoption of \\(A\\) for each of these three-trait demonstrator combinations. First we need to get the number of \\(A\\)s in each combination. Then we can replace the traits in population based on the probabilities in Table 1. When all demonstrators have \\(A\\), we set to \\(A\\). When no demonstrators have \\(A\\), we set to \\(B\\). When two out of three demonstrators have \\(A\\), we set to \\(A\\) with probability \\(2/3 + D/3\\) and \\(B\\) otherwise. When one out of three demonstrators have \\(A\\), we set to \\(A\\) with probability \\(1/3 - D/3\\) and \\(B\\) otherwise. # get the number of As in each 3-dem combo num_As &lt;- rowSums(demonstrators == &quot;A&quot;) population$trait[num_As == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A population$trait[num_As == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob_majority &lt;- sample(c(TRUE, FALSE), prob = c((2/3 + D/3), 1 - (2/3 + D/3)), N, replace = TRUE) prob_minority &lt;- sample(c(TRUE, FALSE), prob = c((1/3 - D/3), 1 - (1/3 - D/3)), N, replace = TRUE) # when A is a majority, 2/3 if (nrow(population[prob_majority &amp; num_As == 2, ]) &gt; 0) { population[prob_majority &amp; num_As == 2, ] &lt;- &quot;A&quot; } if (nrow(population[prob_majority == FALSE &amp; num_As == 2, ]) &gt; 0) { population[prob_majority == FALSE &amp; num_As == 2, ] &lt;- &quot;B&quot; } # when A is a minority, 1/3 if (nrow(population[prob_minority &amp; num_As == 1, ]) &gt; 0) { population[prob_minority &amp; num_As == 1, ] &lt;- &quot;A&quot; } if (nrow(population[prob_minority == FALSE &amp; num_As == 1, ]) &gt; 0) { population[prob_minority == FALSE &amp; num_As == 1, ] &lt;- &quot;B&quot; } To check it works, we can add the new population tibble as a column to demonstrators and have a look at it. This will let us see the three demonstrators and the resulting new trait side by side. # for testing only, add the new traits to the demonstrator dataframe and show it demonstrators &lt;- add_column(demonstrators, new_trait = population$trait) demonstrators ## # A tibble: 100 x 4 ## dem1 dem2 dem3 new_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B B B B ## 2 B A A A ## 3 A B A A ## 4 A B B B ## 5 A B B B ## 6 A B B B ## 7 A B B B ## 8 A B A A ## 9 A A A A ## 10 A B B B ## # … with 90 more rows Because we set \\(D=1\\) above, the new trait is always the majority trait among the three demonstrators. This is perfect conformity. We can weaken conformity by reducing \\(D\\). Here an example with \\(D=0.5\\). All the code is the same of what we already discussed above. N &lt;- 100 p_0 &lt;- 0.5 D &lt;- 0.1 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- tibble(dem1 = sample(population$trait, N, replace = TRUE), dem2 = sample(population$trait, N, replace = TRUE), dem3 = sample(population$trait, N, replace = TRUE)) # get the number of As in each 3-dem combo num_As &lt;- rowSums(demonstrators == &quot;A&quot;) population$trait[num_As == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A population$trait[num_As == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob_majority &lt;- sample(c(TRUE, FALSE), prob = c((2/3 + D/3), 1 - (2/3 + D/3)), N, replace = TRUE) prob_minority &lt;- sample(c(TRUE, FALSE), prob = c((1/3 - D/3), 1 - (1/3 - D/3)), N, replace = TRUE) # when A is a majority, 2/3 if (nrow(population[prob_majority &amp; num_As == 2, ]) &gt; 0) { population[prob_majority &amp; num_As == 2, ] &lt;- &quot;A&quot; } if (nrow(population[prob_majority == FALSE &amp; num_As == 2, ]) &gt; 0) { population[prob_majority == FALSE &amp; num_As == 2, ] &lt;- &quot;B&quot; } # when A is a minority, 1/3 if (nrow(population[prob_minority &amp; num_As == 1, ]) &gt; 0) { population[prob_minority &amp; num_As == 1, ] &lt;- &quot;A&quot; } if (nrow(population[prob_minority == FALSE &amp; num_As == 1, ]) &gt; 0) { population[prob_minority == FALSE &amp; num_As == 1, ] &lt;- &quot;B&quot; } # for testing only, add the new traits to the demonstrator dataframe and show it demonstrators &lt;- add_column(demonstrators, new_trait = population$trait) demonstrators ## # A tibble: 100 x 4 ## dem1 dem2 dem3 new_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 B A A B ## 2 A B B B ## 3 A A A A ## 4 B B B B ## 5 A B A B ## 6 A A B A ## 7 B B A A ## 8 A B B B ## 9 B A A A ## 10 B B B B ## # … with 90 more rows Now that conformity is weaker, sometimes the new trait is not the majority among st the three demonstrators, as it happens, for example, for the first individual. 4.2 Testing conformist transmission As we have done in the previous chapters, it is now time to put all together into a function to test what happens over multiple generations and in multiple runs. There is nothing new in the code below, which is a combination of the code we already wrote in (Chapter 1) and the bits of code for conformity we discussed above. conformist_transmission &lt;- function (N, p_0, D, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0))) # create first generation output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { # create dataframe with a set of 3 randomly-picked demonstrators for each agent demonstrators &lt;- tibble(dem1 = sample(population$trait, N, replace = TRUE), dem2 = sample(population$trait, N, replace = TRUE), dem3 = sample(population$trait, N, replace = TRUE)) # get the number of As in each 3-dem combo num_As &lt;- rowSums(demonstrators == &quot;A&quot;) population$trait[num_As == 3] &lt;- &quot;A&quot; # for dem combos with all As, set to A population$trait[num_As == 0] &lt;- &quot;B&quot; # for dem combos with no As, set to B prob_majority &lt;- sample(c(TRUE, FALSE), prob = c((2/3 + D/3), 1 - (2/3 + D/3)), N, replace = TRUE) prob_minority &lt;- sample(c(TRUE, FALSE), prob = c((1/3 - D/3), 1 - (1/3 - D/3)), N, replace = TRUE) # when A is a majority, 2/3 if (nrow(population[prob_majority &amp; num_As == 2, ]) &gt; 0) { population[prob_majority &amp; num_As == 2, ] &lt;- &quot;A&quot; } if (nrow(population[prob_majority == FALSE &amp; num_As == 2, ]) &gt; 0) { population[prob_majority == FALSE &amp; num_As == 2, ] &lt;- &quot;B&quot; } # when A is a minority, 1/3 if (nrow(population[prob_minority &amp; num_As == 1, ]) &gt; 0) { population[prob_minority &amp; num_As == 1, ] &lt;- &quot;A&quot; } if (nrow(population[prob_minority == FALSE &amp; num_As == 1, ]) &gt; 0) { population[prob_minority == FALSE &amp; num_As == 1, ] &lt;- &quot;B&quot; } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } We can now test the function with perfect conformity (\\(D=1\\)) and plot it (again we use the function plot_multiple_runs() we wrote in Chapter 1). data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.5, D = 1, t_max = 50, r_max = 10) plot_multiple_runs(data_model) Here we should see some lines going to \\(p = 1\\), and some lines going to \\(p = 0\\). Conformity acts to favour the majority trait. This will depend on the initial frequency of \\(A\\) in the population. In different runs with \\(p_0 = 0.5\\), sometimes there will be slightly more \\(A\\)s, sometimes slightly more \\(B\\)s (remember, in our model this is probabilistic, like flipping coins, so initial frequencies will rarely be precisely 0.5). What does it happen if we set \\(D = 0\\)? data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.5, D = 0, t_max = 50, r_max = 10) plot_multiple_runs(data_model) This model is equivalent to unbiased transmission. As for the simulations described in Chapter 1, with a sufficiently large \\(N\\), the frequencies fluctuate around \\(p = 0.5\\). This underlines the effect of conformity. With unbiased transmission, majority traits are favoured as they are copied in proportion to their frequency (incidentally, it is for this reason that ‘copying the majority’ is not a good description of conformity in the technical sense of cultural evolution: even with unbiased copying the majority trait is copied more than the minority one), but they reach fixation only in small population. With conformity, instead, the majority trait is copied with a probability higher than its frequency, so that conformity drives traits to fixation as they become more and more common. As an aside, note that the last two graphs have roughly the same thick black mean frequency line, which hovers around \\(p = 0.5\\). This highlights the dangers of looking at means alone. If we hadn’t plotted the individual runs and relied solely on mean frequencies, we might think that \\(D = 0\\) and \\(D = 1\\) gave identical results. But in fact, they are very different. Always look at the underlying distribution that generates means. Now let’s explore the effect of changing the initial frequencies by changing \\(p_0\\), and adding conformity back in. data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.55, D = 1, t_max = 50, r_max = 10) plot_multiple_runs(data_model) When \\(A\\) starts off in a slight majority (\\(p_0 = 0.55\\)), all of the runs result in \\(A\\) going to fixation (notice this depends on the random initialisation: you can try and change the number in set.seed() to see what happens. In any case, most if not all runs should result in \\(A\\) going to fixation). Now let’s try the reverse. data_model &lt;- conformist_transmission(N = 1000, p_0 = 0.45, D = 1, t_max = 50, r_max = 10) plot_multiple_runs(data_model) When \\(A\\) starts off in a minority (\\(p_0 = 0.45\\)), all runs result in \\(A\\) disappearing. These last two graphs show how initial conditions affect conformity. Whichever trait is more common is favoured by conformist transmission. 4.3 Summary of the model In this chapter, we explored conformist biased cultural transmission. This is where individuals are disproportionately more likely to adopt the most common trait among a set of demonstrators. We can contrast this indirect bias with the direct (or content) biased transmission from Chapter 3, where one trait is intrinsically more likely to be copied. With conformity, the traits have no intrinsic attractiveness and are preferentially copied simply because they are common. We saw how conformity increases the frequency of whichever trait is more common. Initial trait frequencies are important here: traits that are initially more common typically go to fixation. This in turn makes stochasticity important, which in small populations can affect initial frequencies. We also discussed the subtle, but fundamental, difference between unbiased copying and conformity: in both majority traits end up to be favoured, but it is only with conformity that they are disproportionally favoured. This allows for traits’ fixation also in large populations but also, as we will explore later, makes majority traits resistant to external disturbances, such as the introduction of other traits from innovation or circulation of people (migration). 4.4 Analytical appendix Let’s revise Table 1 to add the probabilities of each combination of three demonstrators coming together, assuming they are picked at random. These probabilities can be expressed in terms of \\(p\\), the frequency of \\(A\\), and \\((1 - p)\\), the frequency of \\(B\\). Table 2 adds this column. Table 2: Full adoption probability table for trait \\(A\\) under conformist transmission Dem 1 Dem 2 Dem 3 Prob of adopting \\(A\\) Prob of combination forming \\(A\\) \\(A\\) \\(A\\) 1 \\(p^3\\) \\(A\\) \\(A\\) \\(B\\) \\(A\\) \\(B\\) \\(A\\) \\(2/3 + D/3\\) \\(p^2(1-p)\\) \\(B\\) \\(A\\) \\(A\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(1/3 - D/3\\) \\(p(1-p)^2\\) \\(B\\) \\(B\\) \\(A\\) \\(B\\) \\(B\\) \\(B\\) 0 \\((1-p)^3\\) To get the frequency of \\(A\\) in the next generation, \\(p&#39;\\), we multiply, for each of the eight rows in Table 2, the probability of adopting \\(A\\) by the probability of that combination forming (i.e. the final two columns in Table 2), and add up all of these eight products. After rearranging, this gives the following recursion: \\[p&#39; = p + Dp(1-p)(2p-1) \\hspace{30 mm}(4.1)\\] We can plot the recursion, with weak conformity (\\(D = 0.1\\)) and slightly more \\(A\\) in the initial generation (\\(p_0 = 0.55\\)) as we did previously in the simulation: t_max &lt;- 150 p_0 &lt;- 0.51 D &lt;- 0.1 pop_analytical &lt;- tibble(p = rep(NA, t_max), generation = 1:t_max) pop_analytical$p[1] &lt;- p_0 for (i in 2:t_max) { pop_analytical$p[i] &lt;-pop_analytical$p[i - 1] + D * pop_analytical$p[i - 1] * (1 - pop_analytical$p[i - 1]) * (2 * pop_analytical$p[i - 1] - 1) } ggplot(data = pop_analytical, aes(y = p, x = generation)) + geom_line() + ylim(c(0, 1)) + theme_bw() + labs(y = &quot;p (proportion of individuals with trait A)&quot;) You can change the values of \\(p_0\\) in the code above, (for example less than 0.5, and equal to 0.5) and reproduce the results of the other simulations above. Finally, we can use the recursion equation to generate a plot that has become a signature for conformity in the cultural evolution literature. The following code plots, for all possible values of \\(p\\), the probability of adopting \\(p\\) in the next generation. Note first two new R commands. We use the function seq() to generate a sequence of 101, equally spaced, numbers from 0 to 1, and we use a new ggplot ‘geom’. geom_abline() draws a custom line for which we can pass slop and intercept, as well as other aesthetic properties (such as here linetype = &quot;dashed&quot;). D &lt;- 1 conformity_p_adopt &lt;- tibble( p = seq(from = 0, to = 1, length.out = 101), p_next = p + D * p * (1 - p) * (2 * p - 1)) ggplot(data = conformity_p_adopt, aes(y = p_next, x = p)) + geom_line() + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + ylim(c(0, 1)) + theme_bw() + labs(x = &quot;frequency of A (p)&quot;, y = &quot;probability of adopting A (p&#39;)&quot;) This plot encapsulates the process of conformity. The dotted line shows unbiased transmission: the probability of adopting \\(A\\) is exactly equal to the frequency of \\(A\\) in the population. The s-shaped solid curve shows conformist transmission. When \\(A\\) is common (\\(p &gt; 0.5\\)), then the curve is higher than the dotted line: there is a disproportionately higher probability of adopting \\(A\\). When \\(A\\) is uncommon (\\(p &lt; 0.5\\)), then the curve is lower than the dotted line: there is a disproportionately lower probability of adopting \\(A\\). 4.5 Further readings "],
["biased-transmission-indirect-bias-demonstrator.html", "5 Biased transmission (indirect bias: demonstrator) 5.1 A simple demonstrator bias 5.2 Predicting the ‘winning’ trait 5.3 Summary of the model 5.4 Analytical appendix 5.5 Further readings", " 5 Biased transmission (indirect bias: demonstrator) In the two previous chapters, we started to examine biased transmission, both based on the characteristics of the traits (or direct bias) and on the characteristics of the population. The latter can result from taking into account the frequency of a trait (as we did for conformity) or from taking into account specific features of the demonstrators, which we will look at in this chapter (demonstrator bias is also called ‘model bias’ in cultural evolution). Whereas the simulations we realised previously are fairly standard, indirect demonstrator-based biased transmission can be implemented in several different ways. Demonstrator biases can result when individuals decide whether to copy or not by taking into account any features of the demonstrators, as long as it is not directly tied to the traits. The most studied demonstrator bias is prestige bias, where individuals are more likely to copy from demonstrators that are considered more ‘prestigious’, for example because other individuals show deference to them. Alternatively, individuals can copy the demonstrators that are more successful, independently from how others judge them, or they can copy individuals that are more similar to them, or older (or younger), and so on. The crux is that the decision is not directly linked to the cultural trait itself. 5.1 A simple demonstrator bias To implement a simple version of demonstrator-biased cultural transmission, we first need to assume that there are some intrinsic differences in the population. Up until now, our populations were described only by the traits they possessed. We now want that individuals have a feature by which some of them can be distinguished from others, and, as a consequence, being more liked to be copied. We can call this feature ‘status’. An individual’s status is a binary variable that could stand for whether they are prestigious or not, old (young) or not (assuming that the time frame of the simulations is sufficiently short) or any other features that do not change, and that other individuals can use to decide whether to copy from them or not. We can have a parameter \\(p_s\\) that determines the probability an individual have an high or a low status. library(tidyverse) set.seed(111) N &lt;- 100 p_0 &lt;- 0.5 p_s &lt;- 0.05 population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) We can inspect the tibble by typing its name in the R console population ## # A tibble: 100 x 2 ## trait status ## &lt;chr&gt; &lt;chr&gt; ## 1 A low ## 2 A low ## 3 B low ## 4 A low ## 5 B low ## 6 B low ## 7 B low ## 8 A low ## 9 B low ## 10 B high ## # … with 90 more rows With \\(p_s=0.05\\) around 5 individuals in a population of 100 will have high status. In this specific case, one of them is individual 10, so it will be one of the individuals that will be likely to be copied more from. How should the status used to decide whether to copy or not? Again, there are various possibilities. An intuitive way is to assume that the probabilities to pick high-status and low-status individuals as demonstrators are different. So far, when using the function sample() to select demonstrators, we did not include any specific probability, so that each individual of the previous generation had the same likelihood to be selected. However, we can pass to the function a vector of probabilities to weight the choice. We can assume that the probability to select high status individuals as demonstrators is always equal to 1, but the probability to select low-status individuals is encoded by a further parameter, \\(p_\\text{low}\\): when \\(p_\\text{low}=1\\), the simulations correspond to unbiased transmission, as everybody has the same probability to be chosen, while with \\(p_\\text{low}=0\\), there is a strict model bias, where only high-status individuals can be selected as demonstrators. To implement this, we first store in p_demonstrator the probabilities to be copied for each member of the population: p_low &lt;- 0.01 p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low After that, we sample the traits in the population using these probabilities. Notice the instruction if(sum(p_demonstrator) &gt; 0): this is necessary in case there are not high-status individuals (for example with \\(p_s\\simeq0\\)) and the probability to select demonstrators from low-status one is equal to 0. In this case, the total probability would be also equal to 0, and it would generate an error when the function is run. With this instruction, instead, no copying happens, which is what we would expect in this situation. if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } As usual, we can wrap everything in a function. biased_transmission_demonstrator &lt;- function(N, p_0, p_s, p_low, t_max, r_max) { output &lt;- tibble(generation = rep(1:t_max, r_max), p = rep(NA, t_max * r_max), run = as.factor(rep(1:r_max, each = t_max))) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) output[output$generation == 1 &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # add first generation&#39;s p for run r for (t in 2:t_max) { p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } output[output$generation == t &amp; output$run == r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N # get p and put it into output slot for this generation t and run r } } output # export data from function } We can now test our simulation, assuming a very low, but not null, probability to select low-status individuals as demonstrators (remember we are using the habitual plot_multiple_runs() function to plot the results of the simulations). data_model &lt;- biased_transmission_demonstrator(N = 100, p_s = 0.05, p_low=0.0001, p_0 = 0.5, t_max = 50, r_max = 5) plot_multiple_runs(data_model) The results are similar to what we saw in the previous chapter for conformity: one of the two traits quickly reaches fixation. In the case of conformity, however, the trait reaching fixation was the one that happened to have a slightly higher frequency at the beginning, because of the random initialisation. With a demonstrator bias, this is not the case. From this perspective, an indirect, demonstrator-based, bias is more similar to unbiased transmission. If you remember from the first chapter, simulations with unbiased transmission also ended up with one trait reaching fixation with small populations (\\(N=100\\)), but with bigger ones (\\(N=10000\\)) the frequencies of the two traits remained around \\(p=0.5\\). What does it happen with a demonstrator bias? data_model &lt;- biased_transmission_demonstrator(N = 10000, p_s = 0.005, p_low=0.0001, p_0 = 0.5, t_max = 200, r_max = 5) plot_multiple_runs(data_model) Even with \\(N=10000\\), if the number of high-status individuals is sufficiently low, as in this case (\\(p_s=0.005\\) means that, on average, 50 individuals are high-status in each run), traits reach fixation. By reducing the pool of demonstrators, demonstrator-based bias makes drift more important for the overall dynamics. You can experiment with different values of \\(p_s\\) and \\(p_\\text{low}\\). How big can be the pool of high-status demonstrators before the dynamics become indistinguishable from unbiased transmission? 5.2 Predicting the ‘winning’ trait With conformity, as just mentioned, the trait that reaches fixation is the one starting in majority, while with unbiased copying it can not be predicted at the beginning of the simulation. With a demonstrator bias, a reasonable guess would be that the ‘winning’ trait is the one that is, at the beginning, most common among the high-status individuals. Can we check this intuition with our model? Currently the output we obtain from the simulations is not suitable to this purpose. On the one hand, we do not have a piece of information that we need, i.e. the proportion of high-status individuals having one of the two traits when the population is initialised. On the other, we have much information that we do not need, such as the frequency of the two traits at each time step (we just need to know which traits reach fixation). we can rewrite the function and only changing the output tibble to suit our needs. biased_transmission_demonstrator_2 &lt;- function(N, p_0, p_s, p_low, t_max, r_max) { output &lt;- tibble(status_A = rep(NA, r_max), p = rep(NA, r_max)) for (r in 1:r_max) { population &lt;- tibble(trait = sample(c(&quot;A&quot;, &quot;B&quot;), N, replace = TRUE, prob = c(p_0, 1 - p_0)), status = sample(c(&quot;high&quot;, &quot;low&quot;), N, replace = TRUE, prob = c(p_s, 1 - p_s))) output[r, ]$status_A &lt;- sum(population$status == &quot;high&quot; &amp; population$trait == &quot;A&quot;) / sum(population$status == &quot;high&quot;) for (t in 2:t_max) { p_demonstrator &lt;- rep(1,N) p_demonstrator[population$status == &quot;low&quot;] &lt;- p_low if(sum(p_demonstrator) &gt; 0){ demonstrator_index &lt;- sample (N, prob = p_demonstrator, replace = TRUE) population$trait &lt;- population$trait[demonstrator_index] } } output[r, ]$p &lt;- sum(population$trait == &quot;A&quot;) / N } output # export data from function } Let’s run the new function, for 50 runs (change \\(r_\\text{max}=50\\)) so to have more data points, and inspect the output. data_model &lt;- biased_transmission_demonstrator_2(N = 100, p_s = 0.05, p_low=0.0001, p_0 = 0.5, t_max = 50, r_max = 50) data_model ## # A tibble: 50 x 2 ## status_A p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.75 1 ## 2 0.333 0 ## 3 0.4 0 ## 4 0 0 ## 5 0.333 0 ## 6 0.5 0 ## 7 0.5 0 ## 8 0 0 ## 9 0.333 0 ## 10 0.286 0 ## # … with 40 more rows Each line of the output is a run of the simulation. In the first run, for example, 75% of high-status individuals had the trait \\(A\\) at the beginning, and the frequency of the trait \\(A\\) at the end of the simulation was 1, meaning it reached fixation. Generally, from a cursory inspection to the output, it seems our guess was correct. But let’s visualise all the data. We want to know how the initial proportion of high-status individuals is related to the two possible outcomes (trait \\(A\\) reaches fixation and trait \\(B\\) reaches fixation). A convenient way is to use a boxplot. In the code below, we first eliminate the runs where the traits did not reach fixation (in case they exist) using the new function filter(), and, for clarity, we assign the trait name (\\(A\\) or \\(B\\)) to each run according to which trait reached fixation. We can then plot our output (in this case, we do not write a function on purpose). The main novelties in the code are the new ggplot ‘geoms’ geom_boxplot() and geom_jitter(). Whereas boxplots are useful to detect aggregate information on our simulations, geom_jitter() plots also the single data points, so we can have a better idea on how the proportions of high-status individuals are distributed in the various runs. We could have done this with our usual geom_point(), but geom_jitter() scatters randomly (at a distance specified by the parameter width) the points in the plot. This allows to avoid the overlapping of individual data points (known as overplotting). data_model &lt;- filter(data_model, p == 1 | p == 0) data_model[data_model$p==1, ]$p &lt;- &quot;A&quot; data_model[data_model$p==0, ]$p &lt;- &quot;B&quot; ggplot(data = data_model, aes(x = p, y = status_A, fill = p)) + geom_boxplot() + geom_jitter(width = 0.05) + labs(y = &quot;proportion of high-status individuals with trait A&quot;, x = &quot;winning trait&quot;) + ylim(c(0,1)) + theme_bw() + theme(legend.position = &quot;none&quot;) ## Warning: Removed 4 rows containing missing values (geom_point). The plot shows that when the trait \\(A\\) reaches fixation there are in general more high-status individuals with trait \\(A\\) at the beginning, and vice versa for \\(B\\), confirming our intuition. However, this is far from being a safe bet. Runs with only a quarter of high-status individuals with \\(A\\) ended up with all \\(A\\)s in the population and, conversely, runs with \\(80%\\) of high-status individuals with \\(A\\) ended up with the fixation of \\(B\\). With bigger populations, it is even worst. data_model &lt;- biased_transmission_demonstrator_2(N = 10000, p_s = 0.005, p_low=0.0001, p_0 = 0.5, t_max = 200, r_max = 50) data_model &lt;- filter(data_model, p == 1 | p == 0) data_model[data_model$p==1, ]$p &lt;- &quot;A&quot; data_model[data_model$p==0, ]$p &lt;- &quot;B&quot; ggplot(data = data_model, aes(x = p, y = status_A, fill = p)) + geom_boxplot() + geom_jitter(width = 0.05) + labs(y = &quot;proportion of high-status individuals with trait A&quot;, x = &quot;winning trait&quot;) + ylim(c(0,1)) + theme_bw() + theme(legend.position = &quot;none&quot;) With \\(N=10000\\) and around 50 high-status individuals, the traits are more equally distributed among ‘influential’ demonstrators at the beginning, and there is hardly any difference in the two outcomes. 5.3 Summary of the model In this chapter we modeled an example of indirect, demonstrator-based, biased transmission. We assumed that a fraction of individuals in the population was ‘high-status’ and thus more likely to be selected as demonstrators. The results show that in this situation a trait is likely to become predominant even when populations are large. This is due to the fact that a demonstrator bias effectively works reducing the pool of demonstrators and accelerating convergence. We also saw that the possibility of predicting which trait will become predominant depends on the number of high-status demonstrators. When there are few of them is likely that the trait that they posses in majority will go to fixation, but when their number increase, it is more difficult to make prediction. We also saw how it is important to modify the output of a model depending on the question we are interested in, and we used for the first time ggplot to produce boxplot, a convenient way of displaying the distribution of data along different groups. 5.4 Analytical appendix IS THERE ANYHTIHG WE CAN DO HERE? 5.5 Further readings Examples of simulation models implementing indirect, demonstrator-based, biased transmission include Mesoudi (2009), an individual-based model that explores how prestige bias can generate clusters of recurring behaviours, applied to the case of copycat suicides. Henrich Joseph, Chudek Maciej, and Boyd Robert (2015) presents a population-level model that links prestige to the emergence of whithin-group cooperation. Henrich (2004) describes an analytical, population-level, model, where individuals copy the most succesfull demonstrator in the population. An earlier analytical treatment of demonstrator-based bias, with extensions on the evolution of symbolic traits that may be associated to demonstrators is in Chapter 8 of Boyd and Richerson (1985). Finally, Henrich and Gil-White (2001) is the classic treatment of prestige bias, and a recent review of the empirical evidence supporting it is Jiménez and Mesoudi (2019). References "],
["horizontal-vertical-and-oblique-transmission-.html", "6 Horizontal, vertical, and oblique transmission. 6.1 Analytical appendix 6.2 Further readings", " 6 Horizontal, vertical, and oblique transmission. TO DO 6.1 Analytical appendix 6.2 Further readings "],
["multiple-traits-models.html", "7 Multiple traits models 7.1 Introducing innovations 7.2 Optimising the code 7.3 The distribution of popularity 7.4 Summary of the model 7.5 Analytical appendix 7.6 Further readings", " 7 Multiple traits models In all the scenarios we considered so far, individuals could posses one of two cultural traits, \\(A\\) or \\(B\\). This is a useful simplification, and it represents cases in which cultural traits can be modeled as binary choices, such as being in favour or against a particular policy, choosing between Beatles and Rolling Stones (and no one else!), eating meat or not, and similar. In other cases, however, there are many options: there are many books to read, movies to watch, and, despite our Beatles and Rolling Stones example, many musical bands one can choose to listen to. What does it happen when we copy others’ choices? To simplify, we are again assuming unbiased copying as in the first chapter: all traits are equivalent and we do not copy preferentially from any individual, but just pick them at random. The first modification we need to do in the code concerns how traits are represented. Since we have an undetermined number of possible traits we can not use the two letters \\(A\\) and \\(B\\), but we will use instead numbers, so that traits will be now referred as trait “1”, trait “2”, trait “3”, etc. To start with, we can initliase each individual with a trait randomly chosen between “1” and “100”. library(tidyverse) set.seed(111) N &lt;- 100 population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) As usual, you can inspect the population tibble by writing its name. population ## # A tibble: 100 x 1 ## trait ## &lt;int&gt; ## 1 60 ## 2 73 ## 3 38 ## 4 52 ## 5 38 ## 6 42 ## 7 2 ## 8 54 ## 9 44 ## 10 10 ## # … with 90 more rows The basic code of the simulation is similar to the code in the first chapter, but what the output should be? Until now, we just needed to save the frequency of one of the two trait, but now we need the frequencies of all \\(N\\) traits to have an idea of what happens in the simulation. Second, how do we measure the frequency of the traits in each generation? The function tabulate() dose this for us. tabulate() counts the number of times each element of a vector (population$trait in our case) occurs in the bins we also pass to the function, thus from \\(1\\) to \\(N\\). Since we want the frequencies, and not the absolute number, we divide the result by \\(N\\). multiple_traits &lt;- function(N, t_max) { output &lt;- tibble(trait = as.factor(rep(1:N, each = t_max)), generation = rep(1:t_max, N), p = rep(NA, t_max * N)) population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) # create first generation output[output$generation == 1, ]$p &lt;- tabulate(population$trait, nbins = N) / N # add first generation&#39;s p for all traits for (t in 2:t_max) { previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation output[output$generation == t, ]$p &lt;- tabulate(population$trait, nbins = N) / N# get p for all traits and put it into output slot for this generation t } output # export data from function } Finally, the function to plot the output is similar to what we have already done when plotting multiple runs, with the difference that now the colored lines do not represent different runs, but different traits, as indicated below by aes(colour = trait). The new line theme(legend.position = &quot;none&quot;) simply tells to not include the legend in the graph, as it is not informative, and it would show 100 colors, one for each trait. plot_multiple_traits &lt;- function(data_model) { ggplot(data = data_model, aes(y = p, x = generation)) + geom_line(aes(colour = trait)) + ylim(c(0, 1)) + theme_bw() + theme(legend.position = &quot;none&quot;) } As usual, we can call the function and see what happens: data_model &lt;- multiple_traits(N = 100, t_max = 200) plot_multiple_traits(data_model) Only one trait is still present at the end of the simulation. In general, only one or two traits are still present in the population after 200 generations, and, if we increase \\(t_\\text{max}\\) for example to 1000, virtually all runs end up with only a single trait reaching fixation: data_model &lt;- multiple_traits(N = 100, t_max = 1000) plot_multiple_traits(data_model) This is similar to what we saw when considering the analogous situation with only two traits, \\(A\\) and \\(B\\): with unbiased copying and relative small populations, drift is a powerful force, that quickly erodes cultural diversity. As we already discussed, increasing \\(N\\) limits the effect of drift. You can experiment with various values for \\(N\\) and \\(t_\\text{max}\\). However, the general point is that variation is gradually lost in all cases. How can we counterbalance the homogenizing effect that drift has in small and isolated population, such as the one we are simulating? 7.1 Introducing innovations An option is to introduce new traits with individual innovations. We can imagine that, at each time step, a proportion of individuals, \\(\\mu\\) (we use the same notation that we used for mutation in chapter 2), introduces a new trait in the population. The remaining proportion of individuals, \\(1-\\mu\\) copy at random from others, as before. We can start with a small value, such as \\(\\mu=0.01\\). Since \\(N=100\\), this means that at each generation, on average, one new trait will be introduced in the population. Let’s see how we can write what happens at each generation: mu &lt;- 0.01 last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy the population tibble to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if( sum(innovators) &gt; 0){ # if there are innovators population[innovators,]$trait &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } There are two modifications here. First, we need to select who are the innovators. For that, we use again the function sample(), biased by \\(\\mu\\), picking \\(TRUE\\) (corresponding to be an innovator) or \\(FALSE\\) (keeping the copied cultural trait) for \\(N\\) times. Second, we need to keep track of the new introduced traits. In order to do so, we record at the beginning of each generation what is the ‘label’ of the last trait introduced (at the beginning, with \\(N=100\\), it will be likely to be “100”, as we initialise each individual chosing randomly between 1 and 100). When new traits are introduced, we call them with consecutive numbers: the first new traits will be called “101”, the second “102” and so on. We can now, as usual, wrap everything in a function. multiple_traits_2 &lt;- function(N, t_max, mu) { max_traits &lt;- N + N * mu * t_max output &lt;- tibble(trait = as.factor(rep(1:max_traits, each = t_max)), generation = rep(1:t_max, max_traits), p = rep(NA, t_max * max_traits)) population &lt;- tibble(trait = sample(1:N, N, replace = TRUE)) # create first generation output[output$generation == 1, ]$p &lt;- tabulate(population$trait, nbins = N) / N # add first generation&#39;s p for all traits for (t in 2:t_max) { last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- tibble(trait = sample(previous_population$trait, N, replace = TRUE)) # randomly copy from previous generation&#39;s individuals # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if ((last_trait + sum(innovators)) &lt; max_traits) { if( sum(innovators) &gt; 0){ population[innovators,]$trait &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } } output[output$generation == t, ]$p &lt;- tabulate(population$trait, nbins = max_traits) / N # get p for all traits and put it into output slot for this generation t } output # export data } You should now be familiar with more or less everything within this function, with one exception: the introduction of the new quantity max_traits. This is a trick we are using to avoid making the code too heavy to run. Our output tibble, as you remember, record all the frequencies of all traits. When programming, a good rule-of-thumb is to avoid to modify dynamically the size of your data structures, such as, for example, adding new rows to a pre-existing tibble, as in our case. In our simulation, at every generation, there is some probability that a new trait will be introduced so that, as a consequence, we would need to add new rows in the output tibble to record its frequency. To avoid this, we are creating a bigger tibble from the beginning, with rows for many possible new traits. How many is ‘many’? We do not know, but an estimate is that we will need space for the initial traits (\\(N\\)), plus around \\(N\\mu\\) traits for each generation. To be sure to not exceed this number, we wrapped the innovation instruction in the if ((last_trait + sum(innovators)) &lt; max_traits) control. As a consequence, it is possible that in some runs, in the very last generations, innovations will not be permitted. For many purposes, this does not change the outcome of the simulation, and for the time being is better than modify dynamically our output. Let’s now run the function with an innovation rate \\(\\mu=0.01\\), again with a population of 100 individuals, and for 200 generations. data_model &lt;- multiple_traits_2(N = 100, t_max = 200, mu = 0.01) plot_multiple_traits(data_model) There should be now more traits at non-zero frequency at the end of the simulation that what happened when innovations were not possible. We can actually check the exact number, by inspecting how many frequencies higher than 0 are in the last row of our matrix: sum(filter(data_model, generation==200)$p &gt; 0) ## [1] 8 What happens if we increase the number of generations, or time steps, to 1000, as we did before? data_model &lt;- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01) plot_multiple_traits(data_model) As you can see in the plot, there should still be various traits that have frequencies higher than 0, even after 1000 generations. Again, we can check it sum(filter(data_model, generation==1000)$p &gt; 0) ## [1] 8 Innovation, in sum, allows the maintenance of variation even in small populations. 7.2 Optimising the code It is time for a short technical digression. You may have noticed that running the function multiple_traits_2() it is quite time consuming with a population of 1000 individuals. There is a quick way to check the exact time needed, using the function Sys.time(), which return your system current time. Let’s run the function again and calculate how long it takes. start_time &lt;- Sys.time() data_model &lt;- multiple_traits_2(N = 100, t_max = 1000, mu = 0.01) end_time &lt;- Sys.time() end_time - start_time ## Time difference of 38.68808 secs On a common laptop, it may take more than 30 or 40 seconds. To store the output, we are using a tibble with \\(1100000\\) observations, as max_traits is equal to \\(1100\\), which needs to be updated, in the right position, at each generation. A possibility to speed up the simulation is to record our output in a different data structure. So far, we have been using tibbles to store the output. R, as all programming languages, can store data in different structures. Depending on what the data are and on what one wants to do with them, different structures can be more or less suitable. So far, we used tibbles for the important data of our simulations, such as population or output. Tibbles can have heterogeneous data, depending on what we need to store: for example, in our output tibble, the \\(trait\\) column was specified as a factor, whereas the others two columns, \\(generation\\) and \\(p\\), were numeric. An alternative is to use vectors—and matrices, which can be thought as 2-dimensional vectors— that are basic structures in R. Differently from tibbles, in a matrix all data should be of the same type, in our case numeric (so we need to be careful to remember what numbers represent), but they may be convenient to use as the calculations performed on them are efficient, so that when we run models that are more complicated, or when we will need to produce bigger outputs, our simulations will still be relatively fast. We can rewrite a function that runs exactly the same simulation, but using matrices instead of tibbles. The output is now a matrix with \\(t_\\text{max}\\) rows and max_traits columns, intialised with NAs at the beginning, and the population is a vector of integers, representing the trait for each individual. multiple_traits_matrix &lt;- function(N, t_max, mu) { max_traits &lt;- N + N * mu * t_max output &lt;- matrix(data = NA, nrow = t_max, ncol = max_traits) # create first generation population &lt;- sample(1:N, N, replace = TRUE) output[1, ] &lt;- tabulate(population, nbins = N) / N # add first generation&#39;s p for all traits for (t in 2:t_max) { last_trait &lt;- max(population) # record what is the last trait introduced in the population previous_population &lt;- population # copy individuals to previous_population tibble population &lt;- sample(previous_population, N, replace = TRUE) # randomly copy from previous generation innovators &lt;- sample(c(TRUE, FALSE), N, prob = c(mu, 1 - mu), replace = TRUE) # select the innovators if ((last_trait + sum(innovators)) &lt; max_traits) { population[innovators] &lt;- (last_trait + 1):(last_trait + sum(innovators)) # replace innovators&#39; traits with new traits } output[t, ] &lt;- tabulate(population, nbins = max_traits) / N # get p for all traits and put it into output slot for this generation t } output # export data } To plot the output, we re-transform it in a tibble, so that can be handled by ggplot(). In order to do this, we first create a column that explicitly indicates the number of generations, and then we use the function gather() to reassemble the columns of the matrix in key-value pairs. plot_multiple_traits_matrix &lt;- function(data_model) { generation &lt;- rep(1:dim(data_model)[1], dim(data_model)[2]) data_to_plot &lt;- as_tibble(data_model) %&gt;% gather( key = &quot;trait&quot;, value = &quot;p&quot;) %&gt;% add_column(generation) ggplot(data = data_to_plot, aes(y = p, x = generation)) + geom_line(aes(colour = trait)) + ylim(c(0, 1)) + theme_bw() + theme(legend.position = &quot;none&quot;) } We can now run the function calculating the time needed, and then plot the results, to see if they look the same. start_time &lt;- Sys.time() data_model &lt;- multiple_traits_matrix(N = 100, t_max = 1000, mu = 0.01) end_time &lt;- Sys.time() plot_multiple_traits_matrix(data_model) end_time - start_time ## Time difference of 0.1417139 secs The results are equivalent, and the simulations is almost 100 times faster! This shows that implementation details are very important when building individual based models. When one needs to run several times the same simulation, or testing many different parameter values, implementation choices can make drastic differences. 7.3 The distribution of popularity An interesting aspect of these simulations is that, even if all traits are equal and individuals are not biased, few traits, for random reasons, are more successful than the majority of the others. A way to visualise this is to plot their cumulative popularity, that is the sums of their quantities over all generations. Given our matrix, it is easy to calculate them by summing each column and multiplying by N (remember they are frequencies, whereas we want now to visualise their actual quantities). cumulative &lt;- colSums(data_model) * N Let’s sort them from the more to the least popular and plot the results. data_to_plot &lt;- tibble(cumulative = sort(cumulative, decreasing = TRUE)) ggplot(data = data_to_plot, aes(x = seq_along(cumulative), y = cumulative)) + geom_point() + theme_bw() + labs(x = &quot;trait label&quot;, y = &quot;cumulative popularity&quot;) This is an example of a long-tailed distribution. The great majority of traits did not spread in the population, and their cumulative popularity is very close to one. Very few of them—the ones on the left side of the plot—were instead successful. Long-tailed distributions like the one we just produced are very common for cultural traits: very few movies, books, first names are very popular, while the great majority is not. In addition, in these domains, the popular traits are much more popular than the unpopular ones. The average cumulative popularity is mean(cumulative), but the most successful trait has a popularity of max(cumulative). It is common to plot these distributions by binning the data in intervals of exponentially increasing size. In other words, we want to know how many traits have a cumulative popularity between 1 and 2, than between 2 and 4, than between 4 and 8, and so on, until we reach the maximum value of cumulative popularity. The code below does that, using a for cycle find how many traits fall in each bins and further normalising according to bin size. The size is increased 50 times, until an arbitrary maximum bin size of \\(2^{50}\\), to be sure to include all cumulative popularities. (We will get rid of the empty bins before plotting.) bin &lt;- rep(NA, 50) x &lt;- rep(NA, 50) for( i in 1:50 ){ bin[i] &lt;- sum( cumulative &gt;= 2^(i-1) &amp; cumulative &lt; 2^i) bin[i] &lt;- ( bin[i] / length( cumulative ) ) / 2^(i-1); x[i] &lt;- 2^i } We can now visualise the data on a log-log plot (after filtering out the empty bins). A log-log plot is a graph that uses logarithmic scales on both axes. Using logarithmic axes is useful when, as in this case, the data are skewed towards large values. In the previous plot, we were not able to appreciate visually any difference in the great majority of data points, for example points that had cumulative popularity between 1 and 10, as they were all tangled close to the x-axis. data_to_plot &lt;- tibble(bin = bin, x = x) data_to_plot &lt;- filter(data_to_plot, bin &gt; 0) ggplot(data = data_to_plot, aes(x = x, y = bin)) + geom_point() + labs(x = &quot;cumulative popularity&quot;, y = &quot;proportion of traits&quot;) + scale_x_log10() + scale_y_log10() + stat_smooth(method = &quot;lm&quot;) + theme_bw() In addition, on a log-log scale, the distribution of cumulative popularity produced by unbiased copying lies approximately on a straight line (we plotted a linear fitting of the data with the command stat_smooth(method = &quot;lm&quot;)). The goodness of fit and the slope of the line can be used to compare different models of cultural transmission (what would happen with conformity? What with a model bias? And so on…) and to match them with empirical data to develop hypotheses on the processes that generated these distributions in the real world. 7.4 Summary of the model In this chapter we simulated situations where more than two traits can be taken into account, exploring the simplest case, i.e. unbiased copying. We also implemented the possibility of innovations, where individuals introduce, with some probability, new traits in the cultural pool of the population. Individual innovations counterbalance the homogenizing effect of drift, and replace the traits that are gradually lost. To simulate multiple traits and individual innovations we also needed to deal with a few technical details such as how to keep track of an initially unknown number of new traits. Mostly, from the technical point of view, we showed how important is to use appropriate data structures when simulations start to become slightly more complex. Replacing tibbles with matrices, we were able to make our simulation 100 times faster. Our results showed that unbiased copying produces long-tailed distributions where very few traits are very popular and the great majority are not. An interesting insight from this model is that these extreme distributions do not necessarily result from extreme tendencies at individual level. Some traits become way more popular than others without individuals being biased, for example, towards popular traits. Cultural transmission generates these distributions without biases, but simply because popular traits have the intrinsic advantage of being more likely to be stumbled upon. We also introduced a new technique, the log-log plot of binned popularity distributions, to visualise this outcome. 7.5 Analytical appendix ANYHTING TO DO HERE? 7.6 Further readings Neiman (1995) first introduced a model of unbiased copying with multiple traits to explain popularity distributions in assemblages of Neolithic pottery. Bentley, Hahn, and Shennan (2004) elaborates on this idea, presenting a ‘random copying’ model (equivalent to the one developed in this chapter) and comparing the popularity distributions produced with real datasets, including first names in US and citations of patents. Mesoudi and Lycett (2009) added biases to a multiple traits model to compare the popularity distributions produced with the distribution produced by unbiased copying. "]
]
